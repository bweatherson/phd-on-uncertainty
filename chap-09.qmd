# Vague Decision Theory {#sec-chap-9}

## 9.1 Introduction {#introduction-3}

Part 1 had two central aims: the interpretation of probability sentences and the discovery of rules for reasoning under uncertainty. In the literature, the latter are usually derived by looking at rules for decision making under uncertainty. There are good reasons for thinking this gets matters the wrong way round. So here, in part 2, I move onto decision making under uncertainty as an application of the rules of reasoning already derived. This chapter looks in general at rules; the following two chapters apply my conclusions to some economic problems discussed by Keynes.

Although I call what I'm doing 'decision theory', strictly it is only a portion of a proper decision theory. The central problem with Dutch Book arguments is that they confuse use-value with exchange-value. The same problem I fear pollutes most decision theory. What I'll really be investigating here is the theory of qualitative use-valuation. But that's an ugly name, and since that theory is often called 'decision theory', I'll stick with the ordinary labelling. When I get to applications, the distinction between the two fields will need to be more carefully observed.

The reference to 'qualitative' is important here. I take as given the quantitative use-value of various gambles, and try to derive their relative value. This looks like it should be a triviality: φ is more valuable than ψ iff its quantitative value is higher[^111]. However, the values I take as given are interval‑valued, or perhaps set‑valued, depending on how we develop the theory. So there's no easy bridge available between quantitative and qualitative theory. Indeed it's unclear whether anyone has developed a bridge that meets some rather minimal coherence requirements. For that reason, the purpose of this chapter will be largely negative. I will examine many proposals for getting from quantitative (but non-numerical) values to quantitative orderings or practical proposals, and show where they fail. I'll then propose a relatively modest theory that seems at least plausible.

[^111]: I'll use Greek letters throughout to refer to gambles. I won't define precisely what I count as a gamble, because this is fairly standard in the literature. However, following Ramsey, I regard not only betting slips, but stocks, cars and dollar bills as gambles.

As is usual in the literature, I'll talk about an agent having a choice between various gambles. In order to avoid the problems which brought down the Dutch Book argument, I'll make the following assumptions. Whenever an agent has a choice between a set of gambles, unless otherwise stated, these conditions hold:

(i) All gambles are to be settled immediately after the choice is made. That is, if an agent has a choice between a *p*‑bet and a *q*‑bet, then immediately after the choice is made, the agent will discover whether their chosen bet wins, and receive the appropriate payout. As a corollary to this, there is no possibility of retrade, and no alternative trades available.

(ii) The marginal utility of money is constant and independent of the outcome of various bets.

(iii) The agent places no value on the discovery of whether a particular bet wins or not.

Some of these conditions are inapplicable in some of the circumstances in which we want to apply decision theory. For example, the failure of (i) and (ii) is important when making investment decisions in an inflationary environment. However, it simplifies exposition enormously to assume these conditions hold unless otherwise stated. Condition (i) holds even for decision making in a decision-tree. What I require there is that the choices in the tree are exhaustive, and that bets are settled whenever an 'end-node' is reached.

With these conditions assumed, I can state the central question. Given an agent's (imprecise) degree of belief in each of the relevant propositions, when should an agent trade φ for ψ? This divides into two questions: when is trade permissible, and when is it rationally required? I will also be interested in some associated questions, such as determining which choices are permissible (or mandatory) from a set of available gambles. The 'central question' contains a deliberate asymmetry between φ and ψ. I don't want to rule out theories which say it's permissible (mandatory) to not trade φ for ψ, but not permissible (mandatory) to trade ψ for φ. These theories privilege what the agent currently holds. I don't think such theories work, but this needs argument rather than assumption, and I don't think this privileging of what an agent holds is an argument against these theories.

The agent is reasonable, so their epistemic state can be represented by a set P of probability functions. Each of these functions will assign to each gamble a numerical value. Precisely how this is done will depend on how one resolves Newcomblike problems. I won't buy into this argument, but I'll assume that it has a resolution[^112]. And, since my assumptions mean I can restrict my attention to use-value, I'll assume that the value, according to *Pr*, of a *p*‑bet is *Pr*(*p*).

[^112]: For my purposes the solution to this problem Lewis gives in his (1981) would work, but any of the similar alternative theories he mentions (such as those due to Sobel, Skyrms and Gibbard and Harper) would also suffice.

For a bet φ, P will determine a range of *expected* values for φ: \[*l*~φ~, *u*~φ~\]. I'll assume Continuity, as defined in 7.1.2, holds, so the range of expected values will always be an interval. I'll always notate it as if it is a closed interval, but there is no reason to assume this. My use of closed intervals is just for simplicity[^113]. It's very important to remember that \[*l*~φ~, *u*~φ~\] is not the range of possible payouts for φ; that range will usually be considerably wider, and need not be an interval. I am not interested in what the agent thinks φ might pay, rather in, roughly, what she thinks φ can be expected to pay. If her degrees of belief are all precise then, whatever the range of payouts of φ, *l*~φ~ will equal *u*~φ~.

[^113]: As is the assumption that *l*~φ~ and *u*~φ~ are determined precisely. Because of higher-order vagueness they too will be vague, but that need not detain us.

Decision theories which allow for imprecise credences fall into two broad categories: structured and unstructured. Unstructured decision theories say we can determine the relative merits of φ and ψ by just looking at *l*~φ~, *u*~φ~, *l*~ψ~ and *u*~ψ~. Structured decision theories say we need to look at more; in particular, we need to compare the values φ and ψ according to particular members of P. The first three theories I'll look at are unstructured; it can be concluded from the way they fail that no unstructured decision theory is plausible.

The bulk of this chapter is negative; I show why a glut of solutions to our problem given in the literature fail. Often I will refer to the advice these solutions give to a contestant in the Monte Hall Problem (MHP). This is formally equivalent to the TPP described in @sec-chap-3, but is perhaps more enjoyable to think about. A contestant on a game show faces three doors, call them *a*, *b* and *c*, with a car behind one and worthless prizes behind the other two. She knows the prize has been allocated by a fair chance mechanism. She chooses a door, and then the host shows her that there is no prize behind one of the doors she hasn't chosen. She knows the host will show her a door, and she knows the host will choose a door to show her that doesn't contain the prize. She is ignorant, however, of the host's procedure for choosing which door to show should she have originally chosen correctly. For convenience, I'll use αβ (α, β ∈ {*a*, *b*, *c*}) to refer to the proposition that the car is behind door α and the host shows door β. For a small cost (either in dollars or regret) she is given the chance to change her choice to the other door which remains closed. What ought she do? In @sec-chap-3 I concluded that her degree of belief in 'I originally chose correctly' (call this *p*) should go from precisely 1/3 to a vague interval, possibly as large as \[0, 1/2\]. For now I'll assume that her degrees of belief do become that vague; I'll discuss the plausibility of this for the case as described at the end of the chapter.

There is something odd about this epistemic state. The contestant knows that whatever the host does, her attitude towards *p* will go from being precise to being vague. Fortunately it will always become vague over an interval including the original precise degree, but the interval is guaranteed to grow. As this causes problems for many of the theories which follow, it might be wondered whether such odd states can be ruled out as unreasonable.

The answer is they cannot, at least on pain of ruling out all vague states as unreasonable. Seidenfeld (1994) shows that on some simple assumptions[^114], the requirement that states be immune to what he calls *dilation* is equivalent to the requirement that states be precise. Let P be a set of probability functions, and let *min*(*h* \| *e*) and *max*(*h* \| *e*) be defined as the minimal and maximal values respectively of *Pr*(*h* \| *e*) for *Pr* ∈ P . Let Π = {*p*~1~, ..., *p*~n~} be a partition of *e*. That is, the elements of Π are pairwise disjoint, and their disjunction is *e*. Then P is dilated by Π with respect to *h* and *e* if for all i, *min*(*h* \| *p*~i~ & *e*) \< *min*(*h* \| *e*) and *max*(*h* \| *p*~i~ & *e*) \> *max*(*h* \| *e*). A set P is subject to dilation if there is a *h*, *e*, and Π such that P is dilated by Π with respect to *h* and *e*. Since requiring that P be immune to dilation amounts to insisting that P be a singleton (or satisfy some other even more implausible constraints so that Seidenfeld's assumptions fail) I don't think that requirement can be plausibly imposed. Hence we must learn to live with the decision theoretic consequences of dilation, and the MHP brings out the problem nicely.

[^114]: Basically that there are some probabilistically independent propositions.

To head off possible objections, I will be assuming something like the principle of conglomerability I attacked in @sec-chap-3. Since I will be restricting the scope of the principle to finite partitions this isn't necessarily inconsistent, but it might seem unmotivated. I adopt the following rule:

*Restricted Conglomerability*

Let φ and ψ be bets, such that it would be rationally mandatory for an agent to trade φ for ψ were she to learn *p*, or were she to learn ¬*p*. Then it is rationally mandatory for the agent to trade φ for ψ.

Now some theorists deny even Restricted Conglomerability because of Prisoners Dilemma and Newcomb Problem cases. I don't want to get into this debate, so I'll just note that I won't use the rule in a way that ought offend such theorists.[^115]

[^115]: As a rule like conglomerability is usually needed to justify the idea that we ought to value gambles by their expected utility, objections to it on the ground that it conflicts with the verdict of utility considerations in Newcomblike cases seem to me implausible. Recently, Norton (1998) has argued that we shouldn't accept the verdict of conglomerability in the two-envelope paradox because it conflicts with expected utility considerations. Well, he's right that we shouldn't accept all of conglomerability's verdicts here, but that's because they are inconsistent, not because of the clash with expected utility. Again, without some form of conglomerability there is no motivation for adopting a rule like 'maximise expected utility'.

In any case, this isn't the objection which concerns me most. What does concern me is the possibility that the arguments in @sec-chap-3 against general conglomerability apply equally here. My only 'argument' for Restricted Conglomerability is its intuitive plausibility. Given this it seems to me quite vulnerable to objections to similar principles supported by the same intuition. I really don't have a response here, except to say that decision theory doesn't seem to get off the ground without a rule like this and giving it up would mean giving up many theories we find attractive. No one has put forward an objection like this in the literature, so perhaps it is best to wait until such objections are made rather than shadow boxing. I think the possibility for objections to the theory of this chapter to work is much greater than for any other part of the dissertation, and this assumption might be one of the weak points.

## 9.2 Unstructured Decision Theories {#unstructured-decision-theories}

### 9.2.1 Global Dominance

Hájek (1998) discusses without endorsing a decision rule called global dominance. This says that it is only rationally compelling to trade φ for ψ when *l*~ψ~ \> *u*~φ~. It isn't made clear, but presumably whenever *u*~ψ~ \> *l*~φ~ it is rationally permissible to trade. There is a simple counterexample to this approach. Let ψ be the bet φ + \$ε, where ε is some small amount of money such that *l*~φ~ + ε = *l*~ψ~ \< *u*~φ~. That is, in any circumstance where φ pays \$*m*, ψ pays \$(*m* + ε). Clearly here it is rationally compelling to trade φ for ψ, however the global dominance rule does not require this.

### 9.2.2 Maximin

Gilboa and Schmeidler (1993) advocate a maximin decision rule. I have already given grounds for rejecting their updating rule[^116], but those objections didn't touch their decision rule. The rule is that it is rationally compelling to trade iff *l*~ψ~ \> *l*~φ~, and rationally permissible to trade iff *l*~ψ~ \> *l*~φ~. While this rule doesn't give any particularly counterintuitive results for static cases, it seems to do badly in dynamic settings. Of course it wasn't designed to be used with Conditionalisation, so the objection I'm running isn't directed at any particular theorist, just at its possible use with the Bayesian updating rule.

[^116]: In @sec-chap-3.

Consider again the MHP with the completely ignorant contestant. Initially *Bel*(*ab* ∨ *ac*) = 1/3. Hence according to the maximin rule the contestant will gladly buy a (*ab* ∨ *ac*)‑bet for 25 cents. Assume this trade is made. After the host shows the contestant a door, any door will do, the expected value of this bet will now be vague over \[0, 50*c*\]. Hence by the Maximin rule, she will sell the bet for 20 cents, incurring a sure loss. Hence the Maximin rule, when combined with Bayesian updating, leads to dynamic incoherence.

### 9.2.3 Maxi

This problem could be avoided by adopting a decision rule I call *Maxi*. This says that ψ is strictly preferred to φ, i.e. trade is rationally compelling, iff *l*~ψ~ \> *l*~φ~ and *u*~ψ~ \> *u*~φ~. Trade is rationally permissible iff *l*~ψ~ ≥ *l*~φ~ or *u*~ψ~ ≥ *u*~φ~. No one to my knowledge has endorsed *Maxi* in the literature, but since it is such an obvious weakening of Maximin and other such rules which have been endorsed, it is worth some discussion.

Although there are no simple examples where Maxi gives counterintuitive results, it is in conflict with conglomerability in some hoked-up examples. If one was committed to Maxi, I suppose it could be said that these were arguments against the sure-thing principle rather than Maxi; or alternatively, that in such bizarre examples we can't expect standard rules to apply. I don't think either of these replies works, but I mention them to note that my objections to Maxi are weaker than my objections to other rules.

Say an agent's degrees of belief are determined by the family of probability functions satisfying the following criteria:

\(i\) 0.2 ≤ *Pr*(*p* \| *r*) ≤ 0.6

\(ii\) 0.1 ≤ *Pr*(*q* \| *r*) ≤ 0.5

\(iii\) 0.3 ≤ *Pr*(*p* \| ¬*r*) ≤ 0.7

\(iv\) 0.2 ≤ *Pr*(*q* \| ¬*r*) ≤ 0.6

\(v\) *Pr*(*p*) = 0.35

\(vi\) *Pr*(*q*) = 0.4

It can quickly be seen that none of these conditions are redundant by considering functions like *Pr*~1~, defined as follows. *Pr*~1~(*p* \| *r*) = 0.2; *Pr*(*p* \| ¬*r*) = 0.7, *Pr*(*r*) = 0.7, *Pr*(*q* \| *r*) = *Pr*(*q* \| ¬*r*) = 0.4. Similar functions show the other six bounds given in the inequalities are non-redundant. Given this epistemic state the value of a *p*‑bet will be precisely 35 cents, and the value of a *q*‑bet precisely 40 cents. However, if the agent were to discover *r*, the value (in dollars) of a *p*‑bet would be vague over the interval \[0.2, 0.6\], and that of a *q*‑bet vague over \[0.1, 0.5\]; that is a *p*‑bet would be more valuable, according to Maxi, were the agent to discover *r*. Similarly if the agent were to discover ¬*r*, the value of a *p*‑bet would go to \[0.3, 0.7\] and of a *q*‑bet would go to \[0.2, 0.6\]. Again by Maxi, the *p*‑bet would be more valuable.

Hence in these circumstances, Maxi gives the result that a *q*‑bet is more valuable than a *p*‑bet (by 5 cents), however if either *r* or ¬*r* were found to be true, it would become the case that a *p*‑bet would be 10 cents more valuable than a *q*‑bet. That is, Maxi is in breach of the conglomerative principle I have adopted. Given that the problem with Maxi is that it is too strong, in the sense that it cannot be that all of the trades which are rationally compelling according to Maxi are really compelling we can draw a more important conclusion. There is no rule expressed purely in terms of *l*~φ~, *u*~φ~, *l*~ψ~ and *u*~ψ~ which is stronger than Global Dominance but weaker than Maxi. Yet I've shown that any acceptable rule must be stronger than Global Dominance and weaker than Maxi. Hence no acceptable rule can be expressed purely in terms of *l*~φ~, *u*~φ~, *l*~ψ~ and *u*~ψ~.

As a special case, the Horvitz-style decision rules advocated by Strat (1990) and Jaffray (1994) are incoherent. These advocate that for any bet φ we evaluate its expected worth *E*(φ) according to this rule.

*E*(φ) = ρ*l*~φ~ + (1 ‑ ρ)*u*~φ~. (ρ ∈ \[0, 1\]).

The operator ρ is an optimism / pessimism operator. The more optimistic we are the higher ρ will be. Since we now have a numerical utility for each bet, we can simply choose the bet with the higher utility. Of course this approach is stronger than Maxi, so if Maxi is too strong, so is this approach. Here the fact that the counterexamples to Maxi are so artificial becomes important, because Strat and Jaffray are not, it appears, aiming to discover the ideal decision rule, but rather trying to find a rule which can be implemented efficiently and gives results which are usually correct. Until an example is found in which the recommendations of this approach are implausible despite the example being realistic enough, their approach might be well-suited to the task they have set themselves.

## 9.3 Levi's Rule {#levis-rule}

For the subsequent rules I'll be discussing, I need to look more closely at the structure of the expectation values, not just at their upper and lower bounds. For any bet, say φ, and any element *Pr* of P , there is a numerical expectation value of φ, which we'll call *E~Pr~*(φ). In a completely general theory, utilities as well as credences would be allowed to be vague, but I'll stick to the simple case of assuming precise utilities.

All the subsequent rules I discuss have the property that if for all *Pr* in P *E~Pr~*(ψ) \> *E~Pr~*(φ), then ψ is strictly preferred to φ. That is, it is rationally compelling to trade φ for ψ. How the rules differ is in what can be done when neither bet is strictly preferred to the other in this sense. For convenience, I'll simply define strict preference to hold between two bets ψ and φ iff *E~Pr~*(ψ) \> *E~Pr~*(φ) for all *Pr* in P. This reduces the scope of discussion to bets such that neither is strictly preferred to the other. I will say in this case that the bets are *almost indifferent*. On pain of inconsistency it can't be said that almost indifference implies indifference. This is because almost indifference is intransitive whereas indifference, at least as usually defined, in transitive.

Levi's Rule is that when φ and ψ are almost indifferent we should choose the bet which has the highest minimum payout (Levi 1974, 1980, 1986). This minimum payout is referred to as the 'security level' of the bet. I'm keeping with Levi's terminology in referring to choices rather than permissible trades; the translation back into terminology I've been using is usually trivial. He doesn't mean by this that we ought choose ψ iff *l*~ψ~ \> *l*~φ~. Rather he is referring back to the actual payouts of φ and ψ and advocating choice of the bet with the highest possible minimum return, or as he puts it security level. As he notes, when applied to three-way choice this implies violation of the rule of independence of irrelevant alternatives. That is, under his rule it can be rationally required that φ be chosen in a pair-wise choice from {φ, ψ}, but also required that ψ be chosen in a choice from the set {φ, ψ, χ}. Since he regards the analysis he offers as "impeccable" (1974: 411) he concludes that the rule of independence of irrelevant alternatives must be mistaken in some way.

It's not too surprising that this rule would have to go under such an analysis. After all we can regard each of the *Pr* as a voter which voices an opinion about which choice is best, and then the overall choice becomes the well-known social choice problem of aggregating preferences. Arrow's theorem says that no aggregation rule can satisfy the following four constraints, here explained for voters who are probability functions[^117]:

[^117]: Arrow's Theorem is set out in Arrow (1963). The setting out here closely follows Hausman (1991).

\(1\) *Pareto*. If φ is strictly preferred to ψ in the above sense φ will be chosen from {φ, ψ}.

\(2\) *Collective Rationality*. The rule determines a preferred option no matter what the various *Pr* functions say about φ and ψ.

\(3\) *Non-Dictatorship*. There is no *Pr* function whose choice is followed no matter what the other functions say.

\(4\) *Independence of Irrelevant Alternatives*. The choice between φ and ψ should not depend on what other options are available.

Levi's Rule is committed to (1), (2) and (3), hence it would be inconsistent if it satisfied (4). However, there are good grounds for preserving (4). Of course, there are good grounds for keeping each of these rules, so this argument will necessarily be less than completely compelling. I suspect the strongest argument for (4) is its intuitive plausibility; any attempt to explain this plausibility will sell it short. Nevertheless, I'll try.

Assume an agent, say Lenny, does not satisfy (4). For example, he chooses φ from {φ, ψ}, but chooses ψ from {φ, ψ, χ}. Assume now he has a choice between {φ, ψ, χ}, but the choice dynamics are as follows. First, he has to specify whether he wants χ or not, and if not he has to say whether he wants φ or ψ. Lenny's preference is, *ex hypothesi*, to choose ψ, but he can't carry out this choice. Presumably he will reject χ at the first stage, then he will face a choice between φ and ψ. And here he is forced by his own preferences to choose φ. Levi (1987) in response to this argument claims that Lenny could have adopted at the start a strategy to choose ψ. Hence, at the second stage he will just have to follow his strategy rather than to make a decision about whether φ or ψ is preferable. But now the original objection can be restated in a different way. Surely it's a problem for a decision-rule if the only way to consistently implement it is to ignore its recommendations at various stages. Alternatively, it might be argued that the amendment to the rule to allow strategic choice in this way constitutes a rescission of the original rule and substitution of a new rule. The basis for this argument is simply that, according to the amended rule, at times agents times are required to act in the opposite way to how they were required to act under the old rule.

Levi tries to minimise this difficulty by saying that it is an ineliminable feature of what he calls 'unresolved conflict'. The problem is that he seems to rely here on some equivocations about what would count as a resolution of a conflict. This leads to a problem, I fear, at the core of his lexicographic approach. Levi thinks that we can have a hierachy of 'values', such that if we can't decide between two options using the most important value, we can use lower values to resolve it. That's essentially what is being applied here, with expected value being the highest value, and security levels the next. When it is allowed that each of these values might issue non-linear verdicts (they might allow us to be unresolved and not just indifferent between choices) the lexicographic approach hits problems. The problem is essentially that he seems to be commited to saying that some decision making contexts involve a conflict which is essentially unresolved, while at the same time saying that there is a resolution of these conflicts!

Here's an example he gives. Jones, an office manager, has to hire a new worker to do typing and stenographical work. There are three applicants: Jane, Dolly and Lilly. The applicants take tests in typing and stenography. On the typing test their scores are 100, 91 and 90 respectively, on the stenography test the scores are 90, 91 and 100. So Jones has a dilemma; does he hire the best typist, or the best stenographer, or perhaps someone moderately good at each?

Levi suggests that there are in fact a continuum of tests Jones could apply. For each β ∈ \[0, 1\] we can work out a candidate's β-score as β*x* + (1 ‑ β)*y*, where *x* is their typing test score and *y* their stenography score. For each β test there corresponds an argument for selecting the applicant with the highest score on that test. These arguments will often conflict, as in fact they do here. Some tests favour Jane, and some favour Lilly. Since, however, none favour Dolly she can be ruled out. Now Jones is a liberal, but to a degree: he favours using affirmative action criteria to choose a candidate when the continuum of β-tests have failed to be decisive. The affirmative action criteria support ranking the applicants as follows: Dolly, Jane and Lilly. Since Jane is the highest ranked of the candidates left (not ruled out by the β-tests), she gets the job.

But there's a twist to the tale. Just as he's about to tell Jane she has the job, he finds Lilly has withdrawn her application. Now he has to choose between just Jane and Dolly. And since on some β-tests Dolly is now the best of the applicants (where β \< 0.1) she isn't ruled out by those tests. Hence Jones has to make a decision between Jane and Dolly on affirmative action grounds, and *ex hypothesi* Dolly wins. So Lilly's withdrawl means that Dolly now gets the job over Jane.

Levi notes that most decision theorists would demur here. After all, Jones, a poster-boy for his decision-theory, has just violated what we're calling independence of irrelevant alternatives. Here's his defence:

> When Jones chooses Dolly, this does not reveal that he thinks Dolly is at least as good as Jane for the job. Jones is in conflict as to who is better, all things considered. He chooses Dolly because in the face of such conflict among the values to which he is committed, he invokes considerations which otherwise would not have counted for him. When he contemplates the three-way choice, hiring Dolly is ruled out because of his values. This does not mean that his values have changed or that he has inconsistent values. *Hiring Dolly is neither better nor worse than hiring Jane in the two-way choice*. The same remains true in the three-way choice. This example illustrates an important difference between resolving a conflict so that one can choose for the best and failing to resolve a conflict. In the latter case, some consideration which otherwise would not be taken into account is used to provide counsel as to what to do when one cannot choose for the best, all things considered. (1986: 34, my italics)

But this is inconsistent with his description of Jones's motivation. Jones has made a value commitment to hiring on the basis of affirmative action when the β-tests are inconclusive. This is why it can be deduced from his general principles (including his 'tie-breaker' principles) that he will hire Dolly. It is worse, given his principles, to hire Jane over Dolly in the two-way choice, contra what is said in the italicised sentence. Levi wants here to have it both ways; Jones's affirmative action commitment is supposed not to be a value of any kind, so that it wouldn't be against his values to hire Jane over Dolly, but it can at the same time be used 'to provide counsel as to what to do'. It is rather hard to see how this is consistent. To paraphrase Ramsey, if Jones can't say what his choice is, he can't say it, and he can't whistle it either. If two options really are incommensurable, there can't be a reason for choosing one over the other; that just would show that they weren't really incommensurable to start with.

As a footnote to all this, at (1986: 82) he says that rational agents may have a hierachy of value commitments. This seems to suggest that he favours saying Jones's commitment to affirmative action is a value, in which case the italicised sentence is simply false, so his general defence here fails.

More difficulties can be made for Levi's decision theory. Assume we have the followng test scores for the applicants.

+--------------------+:-------------------+:-------------------------+
|                    | Typing             | Stenography              |
+--------------------+--------------------+--------------------------+
| Tom                | 100                | 90                       |
+--------------------+--------------------+--------------------------+
| Dick               | 90                 | 100                      |
+--------------------+--------------------+--------------------------+
| Harry              | 89                 | 99                       |
+--------------------+--------------------+--------------------------+

We have the following affirmative action ordering: Harry, Tom, Dick. If we adopt Levi's rule, we will choose Tom for the position. Dick's scores dominate Harry, so Harry can't pass any of the β-tests. However, both Tom and Dick pass some, so the affirmative action test applies, and Tom is chosen. Now assume that instead of choosing one applicant for a position we have to choose two. We could assume that Tom will be chosen, leaving a two-way choice between Dick and Harry for the final position, which presumbly goes to Dick.

It might be thought more efficient, however, to decide whom it would be worst to give the position, and hence offer jobs to the other two. The only plausible way to do this is simply to reverse our tests. So at the first stage we'll look at who's worst on all β-tests, as this is our main criterion. If there is more than one person who is worst according to some β-test, we'll look at who does worst by the affirmative action criteria among these. If we apply this method we find that the worst person to give the job to would be Tom! The only people who are worst according to some β-test are Tom and Harry, and Tom is further down the affirmative action list than Harry. So there are two absurd results: the best person to give the job to is also the worst, and we get different results to the question of which two people we should hire depending on whether we look for the best two candidates or the worst. For the reasons indicated above, I am unimpressed with Levi's assertions that choices on the basis of 'tie-breaker' principles are not real preferences.

In summary, not only does Levi's rule give counterintuitive results, it rests on a methodology which is suspect because of this equivocation. To add to the counterintuitive results, briefly note the problems Levi's rule has with the MHP. It has just the same problem that the Maximin rule has. Initially *Bel*(*ab* ∨ *ac*) = 1/3. Hence according to Levi's rule, a contestant will gladly buy a (*ab* ∨ *ac*)‑bet for 25 cents. Assume this trade is made. After the host shows the contestant a door -- any door will do -- the expected value of this bet will now be vague over \[0, 50*c*\]. Hence if the contestant is offered 20 cents for her bet, by Levi's rule she will look to security levels. And on these, the 20 cents does best, no matter how we formulate the security rule[^118]. Hence by Levi's rule, she will sell the bet for 20 cents, incurring a sure loss. Hence Levi's rule, when combined with Bayesian updating, leads to dynamic incoherence. So there are both theoretical and practical grounds for rejecting Levi's rule.

[^118]: I.e. in terms of maximising minimal expected returns or maximising minimal actual returns.

## 9.4 Conservatism {#conservatism}

The rule I am calling Conservatism is perhaps the dominant decision-theoretic rule amongst Bayesians who allow degrees of belief to be vague. For endorsements of it, see for example Williams (1976) or Seidenfeld (1984) and the references contained therein. The rule is that it's rationally permissible to trade φ for ψ iff ψ is strictly preferred to φ. As noted above, the rule is asymmetric. There are circumstances in which it is impermissible to trade φ for ψ, and impermissible to trade ψ for φ. This is an oddity but not an inconsistency. If it was the worst that could be said for the rule it wouldn't be much of an objection. There is, however, a stronger objection.

Assume a Conservative is holding φ, and ψ is a bet which is almost indifferent to φ. Further assume that φ + \$10 is strictly preferred to ψ. The following is a simple-minded objection to Conservatism which doesn't work; I include it to distinguish it from an objection which does work. Assume the only trades which are possible are to swap φ for ψ, and, if that swap is made, to swap ψ for φ + \$10. It would clearly be in the agent's best interests to make each of these swaps, but since they are a Conservative they can't make the first swap, hence Conservatism is an irrational rule. The decision-tree is set out in Figure 1.

![](media/image11.emf)

Figure 9.1

Here's what goes wrong with this objection. When considering the first swap, the Conservative won't be comparing φ and ψ; rather they will be comparing holding φ with the possibility of having a choice between having ψ and having φ + \$10. If they had the latter choice, they would choose φ + \$10, hence the original choice is between φ and φ + \$10. That isn't much of a choice at all, they will clearly choose the φ + \$10. That is, it is consistent with the Conservative rule to accept both trades.

So this objection fails because it relied on a too simplistic Conservative rule. However, a similar objection can succeed. Alter the payout of accepting both trades to φ + \$5, and assume this is strictly preferred to φ, but almost indifferent to ψ. Now the initial choice is a choice between holding on to φ, and having the choice between holding ψ or trading it for φ + \$5. The Conservative knows if they have that choice they will hold onto ψ. So now the initial choice reduces to a choice between holding φ and trading it for ψ. Again, the Conservative here prefers to hold φ. But this is absurd. Whatever we should end up with in this circumstance, it isn't φ, as there is some other option strictly preferred to it. It might be noted that the use of decision-trees in this argument, as opposed to the flawed argument given above, is entirely standard. See Raiffa (1968).

There are two ways out of this problem for the Conservative, neither of them particularly attractive. The first is to make the move Levi makes above, to say that an agent should adopt a strategy for getting through a decision-tree and refuse to reconsider it at later stages. The above objections to that move still apply. The other move is to deny the following rule for reducing complex bets to simple bets.

*Reduction*: If C([β]{.underline}, χ) = δ for any δ ∈ {β, χ}, then C([α]{.underline}, ([β]{.underline}, χ)) = C([α]{.underline}, ).

To explain the notation, by C([α]{.underline}, β) = α I mean that in a choice between holding α and trading it for β, it is rationally compelling that α be chosen. The underlining on α indicates that α is what is currently held; this is important because by the Conservative's lights C([α]{.underline}, β) = α and C(α, [β]{.underline}) = β is consistent. C([α]{.underline}, ([β]{.underline}, χ)) =  ( ∈ {α, (β, χ)}) means that the choice between holding α and trading it for β with the knowledge that this can in turn be traded for χ. Note that I don't assume C([α]{.underline}, β) is always defined.

I don't have any particularly strong arguments for *Reduction*, but it does have a high degree of intuitive plausibility. It is hard to see what other approach could be taken. As was shown in @sec-chap-3, there is a close relationship between adopting *Reduction* as a decision-theoretic principle and adopting *Addition* as a constraint on credences. Since I have argued that *Addition* is a constraint, because it follows from the Equivalence Analysis, I have a justification for Reduction. If anyone thinks it is possible to justify avoiding *Reduction* and hence can avoid this problem I might not have much of a reply. I don't know of any such justification, and I can't see how it could be intuitively plausible, but I'm not going to try and write knock-down objections to as yet unformulated justifications.

## 9.5 Caprice {#caprice}

To set out the correct decision-rule, Caprice, I need a new piece of terminology. Say ψ is *almost preferred* to φ according to P iff for all *Pr* in P , *E~Pr~*(ψ) ≥ *E~Pr~*(φ). When no ambiguity results I omit the 'according to P '. Clearly whenever ψ is strictly preferred to φ it is almost preferred, but the converse is not true. Unlike strict preference, almost preference is not anti-symmetric. Bets ψ and φ can each be almost preferred to the other.

The core idea behind Caprice is that there should be as few restrictions on rational choice as possible apart from the rule that, whenever ψ is strictly preferred to φ it is irrational to choose φ over ψ. Unfortunately, as it is, this won't do, because it permits the following irrational course of action. Recall the earlier example where φ and ψ are almost indifferent, as are φ + \$5 and ψ. If there were no rational restrictions on trade between almost indifferent bets then there would be no grounds for criticising the trader who first swaps φ + \$5 for ψ and then swaps ψ for φ. Yet presumably it should be possible to subject this person to rational criticism.

I think the best thing to say about this case is that neither trade is itself irrational, but they are an irrational combination. In most decision-theories on the market this option is ruled out by stipulation; a set of trades is irrational iff one member of that set is irrational. There is, however, no reason to make such a restriction. Consider this analogy with belief. It seems plausible to say that it is reasonable to believe Oswald killed Kennedy and reasonable to believe he didn't, but it isn't reasonable to believe both that Oswald killed Kennedy and that he didn't. A set of beliefs, each reasonable on its own, might be unreasonable in combination. I am simply claiming we can say the same about decisions. A set of decisions, each reasonable on its own, might be unreasonable.

Because of this intuition, the Caprice rule must be expressed in terms of the reasonableness of sets of decisions. This can be applied easily to simple choices by looking at singleton sets. The notation #(α, β) =  ( ∈ {α, β}) means that is chosen (by the agent under consideration) in a pairwise choice between α and β. This is a different concept to the earlier C(α, β) notation in two respects. First, it is descriptive not normative. Given that I am usually discussing ideal agents this isn't as big a difference as it might normally seem. Secondly, #(α, β) can be defined, even for rational agents, when C(α, β) is not. If α and β are almost indifferent, but when faced with the choice between them the agent chooses α, then C(α, β) is undefined (according to Caprice), but #(α, β) = α.

*Caprice*: A set *S* of choices of the form #(α~i~, β~i~) = α~i~ (i ∈ {1, ..., *n*, ...}) is rationally permissible according to P iff there is some non-empty subset G of P such that for all i, α~i~ is almost preferred to β~i~ according to G .

Caprice is only defined in terms of pairwise choices. If α is chosen in a three-way choice between α, β and χ, we say #(α, β) = α and #(α, χ) = α. This can easily be extended to *n*-way choices. Hence a single *n*-way choice, with *n* \> 2, can be regarded as a many-element set of pairwise choices.

Note two immediate consequences of this rule. First, when we are just considering a single choice between almost indifferent bets φ and ψ, either choice is acceptable. In trading terms, it is permissible but not compelling to trade φ for ψ. This is the motivation for calling the rule 'Caprice'. Secondly, any set of choices which leaves the trader with a position such that they would strictly prefer to be back where they started is not rationally permissible according to Caprice. Hence Caprice as specified captures the two important intuitive requirements on decision-rules.

I haven't yet specified how Caprice should be applied to choices between nodes of a decision-tree, because here there isn't much to say. In cases like that set out in Figure 9.1, the Capricious decision-maker can simply decide which end-point she wants to end up with, and follow the tree to that point. Provided her original *n*-way choice is permissible, every pair-wise choice she makes will be permissible. I showed above that the only way for the Conservative to avoid absurd decisions was to be closed-minded in the sense that she had to deliberately decide *not* to reflect at various stages in the tree about whether her initial strategy should be carried through. By comparison, the Capricious agent can be completely reflective.

There is one interesting special case of Caprice, which I'm adopting from Smets (1994). It isn't Smets's preferred approach for a couple of reasons, not least being that Smets advocates using the Dempster-Shafer updating rule, but the terminology and idea is largely his. An agent whose degrees of belief are vague over the set of probability functions in P , whose 'representor' in van Fraassen's terms is P , has P as their *credal* probability function. They arbitrarily select an element *Pi* from P to use for decision-making purposes; this is their *pignistic* probability. ('Pignistic' is from the Latin *pignus*, meaning to bet.) When making a choice between gambles they choose that gamble α such that *E~Pi~*(α) is maximised. An agent who does this will never do anything wrong according to Caprice.

I noted at page [305](#arrowstheorem) that any decision-rule would have to give up one of Arrow's constraints (1) through (4). Caprice gives up (2). It says that sometimes given the composition of P we simply can't say which of two bets should be chosen. If this pignistic approach is followed, in a sense (2) is kept at the cost of (3). The pignistic probability function becomes the dictator in Arrow's sense. This might be an improvement; I leave it up to the reader to decide whether or not it is.

There is one odd result as a consequence of adopting Caprice. An agent is told (reliably) that there are red and black marbles in a box in front of them, and a marble is to be drawn from the box. They are given the choice between three bets. α pays \$1 if a red marble is drawn, nothing otherwise, β pays a certain 45 cents, and χ pays \$1 if a black marble is drawn. Is it rationally permissible for the agent to choose β, again assuming constant marginal utility of money?

Levi (1974) writes as if it is obvious that choosing β is irrational. This is a cornerstone of the 'impeccable' analysis which leads to a dismissal of (4) but receives almost no justification. Jeffrey (1983) defines Bayesian approaches to decision-making so that choosing β is not Bayesian, but of course it isn't an obvious truth that only Bayesian approaches are correct. Dempster (1988) claims that choosing β is permissible, and perhaps even compelling, though it appears he is motivated by the maximin rule, which I showed above is flawed.

I only bring this up to note that Caprice says it is not rational to choose β. To see this, assume we choose β. We will now show that G must be empty. Let *p* be the proposition that the marble to be drawn is red. Since β is almost preferred to α according to G , for every *Pr* in G it follows that *Pr*(*p*) ≤ 0.45. However, since β is almost preferred to χ according to G , for every *Pr* in G it follows that *Pr*(*p*) ≥ 0.55. There is no *Pr* satisfying each of these constraints, hence G is empty. It doesn't however, appear at all intuitively compelling that it should be irrational to choose β. A defender of Caprice has to either explain away this intuition or, like Levi, simply deny that the intuition exists. The first of these choices is possible. One approach already noted is to say a choice of β reflects an irrational commitment to Maximin. Another is to say that it reflects a failure to internalise fully the assumption that the marginal utility of money is constant. I suspect that is what explains my intuition that β is an acceptable choice. I don't think this raises a huge problem for the defender of Caprice -- some questions are always going to be spoils to the victor -- but it is a little disconcerting. If there is to be a strong attack on Caprice, I suspect it will be built around cases like this one.

## 9.6 Arguments For Caprice {#arguments-for-caprice}

Apart from the fact that it avoids the pitfalls of its more well-known rivals, there are two positive arguments for Caprice. Each of them is essentially the reverse of an argument I used against Levi. I'll call them the arguments from Arrow and Buridan.

The argument from Arrow notes that the four principles Arrow gave, (1) to (4) above, are inconsistent. Hence we must give up one of them. As there is strong intuitive support for Pareto, Non-Dictatorship and Independence of Irrelevant Alternatives, it seems the correct decision theory must give up what Arrow calls 'Collective Rationality', but what is perhaps better called Completeness in our context. There must be some choices about which our decision theory is silent. Since Caprice, unlike its popular rivals, satisfies this constraint, this is something in its favour. Of course this is not an argument against other incomplete rivals of Caprice. However, one strength of Caprice is that the class of decisions over which it is silent is quite a natural class. I doubt there could be a smaller class than this which is equally natural.

This leads to the argument from Buridan. Given the way I have set out the problem, when φ and ψ are almost indifferent, there is no reason to choose one over the other. The agent really is in the position of Buridan's ass. Of course like the ass the agent may be well advised to choose either φ or ψ over some less attractive alternatives. Unlike all its rivals, Caprice takes this conclusion seriously. If there is no reason to choose φ over ψ or *vice versa*, there really is no reason. It doesn't go and say this and then find a reason.

In particular, it must be really inexplicable why an agent chooses φ over ψ or *vice versa* in such cases. Should there be such a reason, it must be traceable to the beliefs and desires (or more generally partial beliefs and preferences) of the agent. The assumption of incomparability is just the assumption that those beliefs and desires don't determine a choice. Hence any decision theory must agree with Caprice's 'no explanation' conclusion. Given this, it is hard to see how the theory can differ from Caprice.

It might be thought that Caprice breaches this 'no explanation' rule in an important case. Say the expected value of φ is vague over \[\$30, \$40\], and that an agent has just sold a unit of φ for \$32. According to Caprice, if she now buys a unit for \$38, or indeed any price over \$32, she will have acted irrationally. Does this mean that either (i) the value of φ is now vague over merely \[\$30, \$32\] or (ii) the value is unchanged but she now has a reason for not buying φ for more than \$32? According to the objection, I have ruled out (i) and (ii), but I am committed to one of them.

The objection is in part correct, I have ruled out (i) and (ii). However, I am not committed to their disjunction. Were the agent to now buy φ for \$38, that would not of itself be an irrational act, however it would take her from having performed a set of rational acts to having performed a set of irrational ones. The only reason one would think this implies the last act is irrational is if one was wedded to the idea that a set of acts is irrational iff it includes an irrational act. By that principle, an agent can only move from a rational to an irrational set by performing an irrational act. However, that is a principle I gave reasons for rejecting in setting out Caprice.

Again the analogy with belief is instructive. If the agent believed yesterday that Oswald killed Kennedy, she can't rationally believe today that Oswald didn't kill Kennedy unless she ceases to believe that he did kill him. But, and here's the difference, yesterday's beliefs can be more easily undone than yesterday's trades. If she could cease to have sold φ for \$32 yesterday, she can rationally buy it for \$38 today. Sometimes this will be possible (if the sale has a 'cooling off' period), but usually it will be just as fixed as the rest of the past. It is because she can change her beliefs, but not her trades, that we judge an agent's trades diachronically, but her beliefs largely synchronically[^119]. When we keep all this in mind, we won't unduly focus on her last trade and judge it too harshly.

[^119]: At least when she is moving between acceptable beliefs. If precision were mandatory, then less flexibility would be permitted.

## 9.7 Monte Hall Again {#monte-hall-again}

Now that I have the correct rule for updating vague degrees of belief (conditionalisation) and for decision-making with vague degrees of belief (Caprice) I can provide some advice to our contestant on the MHP. Recall that the contestant is, *ex hypothesi*, completely ignorant about which rule the host will use for showing her a door given that she has chosen the door which hides the car. Let *p* be the proposition that she has chosen this door. Her updated degree of belief in *p* should be vague over the interval \[0, 1/2\]. The bet φ pays whatever is behind the door she has chosen, and the bet ψ pays whatever is behind the other closed door less some small amount. Hence φ and ψ are almost indifferent, so she can rationally choose either door.

Note that if she had been committed to making decisions according to Levi's Rule or Conservatism she would be committed to keeping φ, that is not switching[^120]. I have argued above that any of these options are errors, but if my arguments fail they provide an argument to the effect that switching is irrational. However, this only goes through if the representation of ignorance I have used is correct, as does the argument from conditionalisation and Caprice that it is not rationally compelling to switch.

[^120]: None of these approaches justify the rather odd conclusion of Moser and Mulder (1994) that it would be irrational to switch doors in a one-off Monty Hall game, but it would be irrational to stay if playing a repeat version of the game 100 times. To see why this is odd, consider the first of these games. By assumption it is better to not switch doors in the one-off game. So the cost of switching must be offset by gains in later games. But the games are totally independent, so what is this gain? Alternatively, and perhaps more controversially, run a backward induction argument showing that by Moser and Mulder's lights it is wrong to switch on the last game of the 100, hence it must be wrong to switch on the 99th, and so on. From their text it appears they have applied Dempster-Shafer updating rule (or something like it) to the single case. However, when considering the long-run case good common sense has overcome bad theory.

And it seems this representation is a mistake. I was assuming that it was rational for the contestant's degree of belief in *ab* to be vague over \[0, 1/3\]. ('*ab*' is the proposition that the prize is behind the door she chose, *a*, and the host will open door *b*.) This means that it would be permissible to be disposed not to pay any arbitrarily small sum for a bet which pays an arbitrarily large sum if *ab*, and nothing otherwise. This, on reflection, seems unreasonable behaviour. It might be reasonable to refuse such a bet if it were offered (the offer being good evidence against *ab*), but the disposition seems unreasonable. I don't want to argue the contestant must have degree of belief precisely 1/6 in *ab*, as is sometimes suggested. (See, for example, Horgan (1995).) Reason it seems would have our contestant be vague over an interval around 1/6, but not all the way to 0. Since the possibility of the interval going so low is central to the argument that it is permissible to not change doors, that argument fails. It is, as many have suggested, unreasonable to stay with the original door.

