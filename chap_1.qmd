# What Probability Isn't {#sec-chapter-1}

## Introduction {#sec-introduction}

Part one of this dissertation defends the view that we should analyse probability as reasonable degree of belief. So the correct analysis of the sentence *The probability that Oswald killed JFK is greater than 0.5* is that the only degrees of belief in *Oswald killed JFK* that are reasonable are greater than 0.5. This will obviously have to be relativised to some evidence; the sentence is not refuted by the existence of people who have never heard of Oswald or Kennedy and who can thus reasonably refrain from having a high degree of belief in *Oswald killed JFK*. Hence I claim that probability sentences contain an elliptical reference to evidence; in @sec-chapter-4 I'll say more about how this reference works. That probability sentences are in part elliptical is not at all controversial: virtually every theory of probability does this in some way.

This approach to probability puts my account in the tradition of Keynes (1921a) and Carnap (1950). They advocated this analysis of probability, and what I regard as one of its corollaries, that probability sentences are non-contingent. I differ from these theorists in one important respect. They thought that probability is a 'logical' concept, in a rather narrow sense. That is, they thought that the truth of probability sentences could be deduced from their syntactical structure in an ideal language. As I will argue in section 1.7, there are pragmatic and theoretical reasons for rejecting this approach. Nevertheless, there is no reason why all non-contingent truths must be true in virtue of syntactic form, so I can differ from Keynes and Carnap on this point while holding on to their more important insight.

Probability is not just used in sentences like *The probability that Oswald killed Kennedy is* α. We also use probability in a purely mathematical sense when we talk about the probability calculus. Part of my theory of probability is to explain the connection between these two facets of probability. In chapter 2 I argue that a very common way of proving a connection (the 'Dutch Book argument') is unsound, however in chapter 3 I give a new argument for this connection. The argument only works on the assumption that degrees of belief ought be precise, or what is equivalent, completely ordered, and that assumption is false. The argument is, however, illuminating as to what we ought say about situations where degrees of belief are partially ordered. The probability calculus will be so important to what follows that I give a brief introduction to it in section 2.

The rest of this chapter is to set out the common objections to all the other analyses of probability on the market. The aim is not to provide a conclusive refutation, but as Ramsey said to "show that \[they are\] not so completely satisfactory as to render futile any attempt to treat the subject from a rather different point of view" (Ramsey 1926: 166). The most important objection to some of these theories is provided by the rest of the dissertation. In particular those theories of probability which are defended by showing necessitarian analyses to be implausible are weakened not so much by my direct attacks on them as by my defence of their rival.

Sections 3 argues that probability should not be analysed as actual frequency, and section 4 shows that analysing probability as modal frequency is no better. In section 5 I argue that Popper's conception of probability as propensity cannot explain probability sentences about past events, and hence cannot be a complete theory. Section 6 argues that we cannot adopt Ayer's conventionalist approach to probability, because of the problem of unknown conventions. Section 7 looks at the problems with the syntactic theories of probability defended by Keynes and Carnap. Finally, and perhaps most importantly, in section 8 I examine the various kinds of theory of probability called 'subjective'. This includes the necessitarian theory defended here. Following Carnap I argue that this theory is not properly called subjectivist, and following Ayer I argue that most other theories called subjectivist are flawed.

A note on notation before I start. I will often talk about probability sentences; indeed the overall project here could be described as trying to analyse probability sentences. There are more types of probability sentence than I have indicated above. I also intend the term to refer to sentences like the following:

-   Oswald probably killed JFK.
-   It's more probable that Oswald killed JFK than that O. J. Simpson killed his wife.
-   The probability that Oswald killed JFK is 0.6.
-   The probability that Oswald killed JFK given the forensic evidence is 0.6.

That is, I take probability sentences to come in qualitative, comparative, quantitative forms as well as in conditional and unconditional forms. Of course I could by mixing these forms come up with even more examples, but I hope this is enough to indicate the field in which I'm interested.

## 1.2 The Probability Calculus {#the-probability-calculus}

Mathematically, probability functions have as their domain a field of sets, and as their range reals in \[0, 1\]. However, the probability sentences that I'm taking to be the explicandum of our theory seem to refer to the probability of an event or sentence. We solve this little problem by saying that probability sentences talk about the probability of propositions, and propositions are just sets of possible worlds. The proposition *p* is just the set of possible worlds in which *p*. Hence we can interpret the 'universe' in the mathematical representation as the set of all possible worlds, and the field as a set of propositions.

We take as given a possibility space *U*, and a field *F* of subsets of *U*. That *F* is a field just means it includes *U*, and is closed under complementation, union and intersection. *Pr*: *F* → \[0, 1\] is a simple probability function just in case it satisfies the following three axioms.

(Pr1) For all *A* ∈ *F*, *Pr*(*A*) ≥ 0;

(Pr2) *Pr*(*U*) = 1

(Pr3) If *A*, *B* ∈ *F* and *A* ∩ *B* = ∅ then *Pr*(*A*) + *Pr*(*B*) = *Pr*(*A* ∪ *B*)

In propositional terms, (Pr2) says that the probability of any (classical) tautology is 1, and (Pr3) says that if *p* and *q* are inconsistent then the probability of *p* ∨ *q* is the probability of *p* plus the probability of *q*. The canonical statement of all this is in Kolmogorov (1933). He makes two complications to the theory. The first is to extend it to conditional probability functions. Often the axiomatisations for conditional probability functions are given in such a way that probability could be conditional or non-conditional. I think it's neater to only allow conditional probabilities, and since I think all probability sentences make elliptical (or explicit) reference to evidence, there is a philosophical justification for this. So the axiomatisation for conditional probability functions *Pr*: *F* × *F* → *U* is as follows.

For all *A*, *B*, *C* ∈ *F*

(CP1) *Pr*(*A* \| *B*) ≥ 0

(CP2) *Pr*(*U* \| *A*) = 1

(CP3) If *A*, *B*, *C* ∈ *F* and *A* ∩ *B* = ∅ then *Pr*(*A* \| *C*) + *Pr*(*B* \| *C*) = *Pr*((*A* ∪ *B*) \| *C*)

(CP4) *Pr*(*A* \| *B* & *C*) · *Pr*(*B* \| *C*) = *Pr*(*A* & *B* \| *C*)

The notation *Pr*(*A* \| *B*) is read as 'the probability of *A* given *B*'. We can recover the 'unconditional' probability *Pr*(*A*) as *Pr*(*A* \| *U*). When the simplification is harmless and aids the exposition I will occasionally talk about simple, or unconditional, probability functions, but the main focus will be on analysing probability sentences by using of conditional probability functions. Note that we can almost recover a conditional probability function from a simple one by setting *Pr*(*A* \| *B*) =~df~ *Pr*(*A* & *B*) / *Pr*(*B*). The problem is that this definition fails when *Pr*(*B*) = 0. It seems on the whole simpler to take conditional probability functions as basic.[^1]

[^1]: It has been reported to me that Alan Hájek's as yet unpublished Ph.D. thesis contains a wide range of arguments for this conclusion, including arguments against resolving the difficulty of undefined conditional probabilities by moving to infinitesimals. However, without having seen that thesis, I am unable to comment in any detail on it.

The other complication Kolmogorov makes is to extend the additivity axiom, (Pr3) or (CP3), from a principle of 'finite additivity' to one of 'countable additivity'. This involves the adoption of a new axiom, (CP5).

(CP5) If *A*~1~, ..., *A~n~*, ... are pairwise disjoint elements of *F*, then ![](media/image1.emf)

It is hardly ever suggested that this be extended to cases where there are more than denumerably many *A*'s, for example where there is one element of the *A*'s for every real in \[0, 1\].[^2] However, there is some debate about whether even extension to the countable case is plausible. Kolmogorov merely defended it on grounds of mathematical convenience, which is hardly telling. The following example shows both how (CP5) is independent of the other axioms, and why we might not want this axiom.

[^2]: Though Lewis (1994) seems to suggest that we might need such 'strong forms of additivity' to deal with the infinitesimal-valued probabilities he posits.

Say we know *x* is a natural number, but have no idea about which natural number it is. In this case we might think it appropriate to spread the probability evenly over every element of *N*. That is, for any natural number *n*, set *Pr*(*x* = *n*) = 0. Or in conditional language, set *Pr*(*x* = *n* \| *x* ∈ *N*) = 0. Now this is clearly consistent with the axioms apart from (CP5), and it is clearly inconsistent with (CP5). To see this, set *A~i~* as *x* = *i* for all *i*. The probability of each *A~i~* is 0, but the probability of their union, *x* ∈ *N*, is 1. de Finetti thought this probability function was so obviously reasonable in the circumstances that he rejected Kolmogorov's axiom (de Finetti 1974: 121). In chapter 3 I'll look into this in more detail, but to get ahead of myself a little bit, I don't think (CP5) has been proven to be appropriate for our usage of probability. And since I think there's a burden of proof on the proponent of a new axiom, for now I take de Finetti's side of this debate. However, I'm not as convinced as de Finetti that there will never be an argument for countable additivity.

It might be worth noting one of the confusing nomenclatures in this field, if just to note that I won't be adopting it. Sometimes the term 'finitely additive' is used for only those probability functions which do not satisfy countable additivity, our (CP5). This is misleading because of course countably additive functions are also finitely additive on the most natural interpretation of that term. That is, they satisfy (CP1) to (CP4). When I use the term 'finitely additive' that is precisely what I will mean, but to minimise confusion I'll just try not to use it at all.

So all I mean by a probability function is something satisfying (CP1) to (CP4). These will be important to our eventual analysis of probability sentences, but for now we can leave mathematics and return to philosophical analysis. Or at least to refuting philosophical analyses.

## 1.3 Probability is not Frequency {#probability-is-not-frequency}

It could be the case that *The probability that Oswald killed JFK is more than 0.5*, which we'll abbreviate to *O*, is true even if Oswald did not kill JFK. Since there was only one JFK assassination, that would make Oswald's frequency of being JFK's assassin 0. Yet this wouldn't, one suspects, make us say that *O* is necessarily false now. If there is a lot of evidence for Oswald's guilt, as many people seem to believe, then *O* will be true. Hence we cannot interpret *O* as a statement about frequencies, in this simple sense.

Nor could probability be long-run frequency. If probability is long-run frequency it must be that Oswald being the assassin is one type of event, and the JFK assassination is another, and the ratio of events of the first type amongst events of the second is more than 0.5. Now on the one hand if we specify these types too closely then we will be back to the problem that there is at most one event of each type, so the ratio will be 0 or 1. On the other hand if we specify too coarsely, we lose any theoretical motivation for linking probability and frequency.

Consider, for example, some of the possible event types *E*~1~ and *E*~2~ such that Oswald being the assassin is an instance of *E*~1~ and the assassination is an instance of *E*~2~ and the probability of Oswald being the assassin is the frequency of *E*~1~ events amongst the *E*~2~. (I.e. *n*(*E*~1~ & *E*~2~) / *n*(*E*~2~) or some limit of this, where *n*(*E*) is the number of times *E* occurs). If *E*~1~ is Oswald being the assassin and *E*~2~ is there being an assassination, then *O* will be obviously false, but presumably it could be true. If *E*~1~ is the initial suspect being guilty then the probability of initial suspects being guilty at every assassination will be constant, which seems mistaken. Similar considerations preclude *E*~1~ being say, a communist sympathiser is guilty, or being that someone who killed someone else on the day of the assassination is the killer. If we start taking conjunctions of these, say *E*~1~ being the initial suspect, who is a communist sympathiser and a known killer, is guilty and *E*~2~ is that there is an assassination where the initial suspect is a communist sympathiser and a known killer we risk the classes contracting to size 1 again, and the frequencies hence being either 0 or 1.

Even when the frequency analysis gives the correct output, it seems to get the direction of explanation wrong. Moving from assassinations to casinos, let *E*~2~ be the event that a standard (i.e. 37-slot) roulette wheel is spun and *E*~1~ the event that the ball lands in 1. The probability of the ball landing 1 is 1/37, which is presumably also the frequency. I have just made a well-balanced, apparently fair 35-slot roulette wheel. The probability of the ball landing 1 on first spin is, it would seem, 1/35, even if this is the first ever spin of a 35-slot roulette wheel, and indeed even if it is the only ever spin of such a wheel. This is just the point of the previous argument, however there is a larger problem for the frequency analysis.

Say that a schmoulette wheel is a 35-slot roulette wheel made today or a 37-slot wheel made any other day. Let *E*~2~ be the event that a schmoulette wheel is spun, and *E*~1~ the event that the ball lands 1. The frequency of *E*~1~ amongst the *E*~2~ will in the long run be little different from 1/37. This doesn't alter the fact that the probability that the ball will land 1 on the first spin of my 35-slot wheel is 1/35, even though the spin is an event of type *E*~2~ and the ball landing 1 of type *E*~1~. The conclusion I draw is that the events in *E*~1~ and *E*~2~ must be homogenous in some way if the frequency analysis is to give the correct response. However, I suspect there will be no way of defining this homogeneity except by reference to probability. In other words, it seems that it must be probability that determines frequency, rather than frequency determining probability. Unless we already know the probability of particular events we can't determine appropriate event types, and without that we can't determine the frequency of a type of event[^3].

[^3]: In section 7.4 I argue against Kyburg's attempt to define homogeneity in just this way. Kyburg is not a frequency theorist; he is a logical theorist who thinks that probability refers to a metalinguistic relation between a sentence and its evidence whose value is determined by the most pertinent statement about frequencies in the evidence.

Russell (1948: 384) showed, when the cardinality of *E*~1~ and *E*~2~ is infinite, the ratio of occurrences of *E*~1~ to ¬*E*~1~ amongst the *E*~2~ can only be defined as a limit. That is, we list the occurrences of *E*~2~ in some order, and say the frequency of *E*~1~ is the limit as *n* tends to infinity of the number of events in the first *n* which are *E*~1~ to *n*. However, the limit of this ratio depends not only on the membership of *E*~2~, but on its ordering. For example, if we order the natural numbers in the standard way (i.e. 1, 2, 3, ...) then as *n* tends to infinity the ratio of the number of primes less than or equal to *n* to *n* will tend to 0. So the long-run frequency of primes in the natural numbers is 0. However, if we simply re-order the numbers, we can make this limit be 1/2, or 1, or indeed any number we care to choose in \[0, 1\]. So frequency can't be defined as a relation between classes, but only as a ratio between sequences. As Russell remarked, "This seems strange." (1948: 385).

There are also a multitude of theoretical reasons for not equating frequency and probability. One is what van Fraassen (1989) calls the *horizontal-vertical* problem. Assuming we're trying to work out the probability of an event that will (or will not) happen in 2000, say a Democrat winning the 2000 U. S. Presidential election. Consider a branching-time model of the universe, with the possible world time-slices being points on a Cartesian plane, and with actual time as the ­*y*-axis. This is drawn in the diagram below. Possible worlds are functions *x* = *f*(*y*). In the diagram below the actual world \@ (the bold line) is represented by the function *x* = *c*. The dotted horizontal lines then represent all the ways the world could be at various points in time. The large dot is the way the actual world is now. The other lines leaving this world represent worlds which have the same past as ours, but diverge between now and the year 2000. There are of course infinitely many such worlds, but only finitely many can be drawn.

![](media/image2.emf)

To work out the relative frequency of one event type given another, we only have to look at \@. In particular, we look at the vertical line *x* = *c*, and work out the ratio of points on it that are of type *E*~1~ & *E*~2~ to those that are of type *E*~2~. If this is impossible we work out the limit of this ratio as time tends to infinity. However, to work out the probability now of a certain event happening in 2000, we presumably have to look at the ratio of points on *t* = 2000 which are of that type to those that are not, or more likely some weighted average of this type, or more likely again a limit of some such weighted average. The important point is that what is important to the probability of a Democrat winning is a ratio of some kind on the horizontal line *t* = 2000, not on the vertical line \@. Frequencies measure the wrong things to be probabilities.

Finally, there is the problem Kyburg (1961: 22) noted about the applicability of the frequency analysis. Let's take a case where the frequentist should be on solid ground, the case where we try to work out the probability of a coin toss landing heads. Coin tosses happen often enough, and are homogenous enough, that at least some of the standard objections to frequentism are irrelevant. Perhaps then the frequentist can explain what we mean by 'The probability of a coin toss landing heads is 1/2'. However, as Kyburg points out by their own lights we cannot mean anything by 'The probability of the next coin toss landing heads is 1/2'. The frequentist can only talk about the probability of events which are outcomes of trials repeated very often, perhaps infinitely. However, 'the next coin toss' is not a repeated trial, hence they can't talk about the probability of it. So even in cases where they appear most comfortable, the frequentist only gets away with their story by shifting from a definite to an indefinite article.

## 1.4 Probability is not Modal Frequency {#probability-is-not-modal-frequency}

Recognising the horizontal-vertical problem, some people have argued that probability is modal frequency. That is, the probability of *p* is the frequency of *p* across the possible worlds, or the ratio of *p*-worlds to all worlds. This does solve the horizontal-vertical problem, and it solves the problem of one-off events (like the JFK assassination) having a probability. But it seems to make a fundamental mistake about the nature of the possible worlds. There are just too many of them for this to get off the ground. Provided *p* is contingent, there are infinitely many worlds in which *p*, and infinitely many in which ¬*p*. Now there are ways to get around this, indeed my theory could be considered such a way, but when we take it we seem to not have a modal frequency theory. Moreover, it is hard to see how on this analysis we could think the probability of a Democrat winning in 2000 is higher than that of a Republican winning. It's not that there are more worlds in which Democrats go on to win than in which Republicans do, just that (at present) the Democrat-winning worlds are more probable.

We might hold that probability is modal frequency among the accessible worlds, or that it is some kind of weighted modal frequency. I have no objection to such a view; indeed it is quite similar to a view I adopt. However, adopting this as an analysis seems to me to get the order of explanation wrong. The frequency of a highly probable event among accessible worlds is high simply because the event is probable. That is, the accessible worlds are accessible because they are probable, they are not probable because they are accessible. So I think such analyses may be extensionally correct, but even if they are will be flawed as analyses because their 'direction of fit' is wrong.

## 1.5 Probability is not Propensity {#probability-is-not-propensity}

Popper (1959) held that probability should be analysed as propensity. This seems to make sense when we are looking to analyse probability statements in, say, quantum mechanics. But it doesn't make sense in a number of senses in which probability is used. In particular, it doesn't seem to work when we are considering the probability of events which, if they did happen, would have happened in the past, or the probability of laws of nature.

As an example of the first type of problem, note that it makes sense to talk about the probability that Oswald is guilty. Now this doesn't mean (and nor would Popper have said it meant) that Oswald is likely to commit more crimes. Even if we are now totally convinced that Oswald's current propensity to commit crimes is low (because he's dead), the probability that he was a killer can be high. So we can at most talk about what the propensity was. Even this seems implausible, as the following is not contradictory: "It seems highly probable on the basis of the forensic evidence that Oswald did it, though it would have been completely out of character for him". Assuming Oswald's character determines his propensity to commit crimes, this means we can distinguish between probability and past propensity. So it must be current propensity that matters. But the current propensity must be either 0 or 1. The world is already either an 'Oswald‑did‑it' world or an 'Oswald‑didn't‑do‑it' world, so its propensity to become one of these is 0 or 1. Yet the probability that Oswald did it on the basis of a certain body of evidence can be between 0 and 1.

As an example of the second type of problem, note that it seems plausible, at least when doing historical reconstructions, to talk about the probability that the laws are one way rather than another. We can talk sensibly about a certain experiment making one theory more or less probable. But we can't, it would seem, make any sense of propensity statements without assuming laws as given. What is usually referred to as the propensity of, say, atoms to decay is at best a matter of natural law. If we take the law as up for question, the propensity is indeterminate. Since Popper does not think that all probabilities are indeterminate, it must be that we take laws as given when determining probabilities. Hence the probability of any actual law must be 1, and the probability of any counterlegal is 0. But this goes against our evidence that the probability of a purported law can change with experiments. So the propensity analysis must fail. These two counterexamples can be connected. The propensity theory cannot explain statements like 'On the evidence the ancient Egyptians had, it was highly probable that the earth was flat, but on the evidence we have today, this is highly improbable' both because it discusses the probability of prior events and it allows for counterlegals to have a positive probability.

## 1.6 Probability is not What Everyone Believes {#probability-is-not-what-everyone-believes}

Before getting onto orthodox subjectivist analyses in section 8, I want to address here conventionalist theories of probability, which are quite similar. The conventionalist, or intersubjectivist, argues that the probability of *p* given some evidence *q* is the degree of belief which the community holds to be appropriate in *p* given that evidence. On some tellings, the conventionalist agrees with the necessitarian position advocated here that probability should be analysed in terms of reasonable degrees of belief. They even proffer a broadly realist conception of what is reasonable. However, that conception is so different to what I am defending that it amounts to a different analysis.

The conventionalist account is historically important because it seems to be the theory of probability in Ayer's *Language, Truth and Logic*. I say 'seems' because Ayer isn't particularly explicit on this point, and the discussion amounts to no more than a couple of pages. The main evidence is the following quotes.

> To say that an observation increases the probability of a hypothesis ... is equivalent to saying that the observation increases the degree of confidence with which it is rational to entertain the hypothesis. And here we may repeat that the rationality of a belief is defined, not by reference to any absolute standard, but by reference to our own actual practice.
>
> \[W\]hen a man relates belief to observation in a way which is inconsistent with the accredited scientific method of evaluating hypotheses ... he is mistaken about the probability of the propositions which he believes (Ayer 1936: 100-101).

More recently various writers such as Gillies (1988, 1991), Runde (1994a), Davis (1994) and Bateman (1996) have held that Keynes moved to a conventionalist position when he wrote his later economics. Some of these writers, particularly Gillies and Runde, seem to endorse this shift.

It is a little surprising at first that Ayer takes this position on probability. I expected Ayer to adopt a position that was both subjectivist and non-cognitivist about probability, much as he does about ethics. The reason he does not do this is two-fold. First, he recognised some of the good objections to subjectivism, at least as it is commonly presented. Secondly, his verification principle is expressed in terms of probability. A sentence is meaningful, says Ayer, iff it is verifiable. But to make this plausible we have to adopt what Ayer calls the 'weak' conception of verifiability. "\[A proposition\] is verifiable, in the weak sense, if it is possible for experience to render it probable" (Ayer 1936: 37). Now if what was probable varied from person to person (as some subjectivists assert) it would turn out that which sentences were meaningful varied from person to person. This is much too implausible for Ayer. Alternatively, if we go fully expressivist (or non-cognitivist) about probability, and say that there is no fact of the matter as to whether or not a proposition has been rendered probable, there will be no fact of the matter as to whether some sentences are verifiable. This is again not a conclusion Ayer wants.

However, the conventionalist move has problems of its own. There is one argument against it which seems quite powerful to me, but which is obviously question-begging. On Ayer's story whether *q* renders *p* probable will depend not just upon *p* and *q*, and perhaps on background facts, but on the prevailing scientific standards. Probability sentences for Ayer presumably have an elliptical reference to these standards. Now this seems completely implausible, but since Ayer happily accepts it we can hardly urge it as an argument. I only mention it to remind the reader that their intuitions on this matter may differ from Ayer's.

The more substantial problem for Ayer is what I call the problem of unknown conventions. Since it is an empirical fact that convention *A* is operative in our society, rather than say convention *B*, this is only something we can learn by experience. That is, we learn it because we acquire evidence for it. Presumably this evidence, like all other evidence, could be misleading. So say an agent has evidence *q*, and that evidence provides strong but misleading support for the proposition that convention *B* is operative. Now assume, as again seems possible, that conventions *A* and *B* provide different directions as to the appropriate degree of belief in *p* given *q*. Say *A* says *p* ought be believed to degree 0.3, and *B* says it should be believed to degree 0.8. Now, what ought our agent do?

The conventionalist says that the agent ought believe *p* to degree 0.3. Note, however, that if the agent does the best they can do to accord with the conventions, that is, arrange their beliefs in accord with what they reasonably believe to be the conventions, they will believe *p* to degree 0.8. I don't see how the conventionalist can criticise an agent who does follow convention *B* in these circumstances. After all, that agent has done what they could to satisfy conventionalist doctrines; they have arranged their beliefs in accord with what they have reasonably taken to be the conventions of society. So I think the conventionalist is forced to say this kind of person both is and is not reasonable.

We can make the same point in a more dramatic way. The conventions in which we are interested are just rules for converting evidence to reasonable degrees of belief. If someone believes all the conversions, they believe the convention, even if they can't express it. And plausibly we can analyse believing a particular conversion as being disposed to make it in the right circumstances. That is, if an agent is disposed to believe *p* to degree *x* on evidence *q*, they believe that the relevant rule converts evidence *q* to degree of belief *x* in *p*. They might, in some circumstances, not know that they believe it, but believe it they do. The conventionalist says reasonable agents will always have these dispositions. Hence all reasonable agents will believe the conventions are what they actually are, even on no evidence whatsoever. Since, as was noted, what the conventions are is for Ayer an empirical fact, he imposes upon his rational agents a requirement to believe an empirical fact on no evidence at all. This is hardly plausible, so Ayer's conventionalism about probability fails.

## 1.7 Probability is not a Syntactic Relation {#probability-is-not-a-syntactic-relation}

In his (1921a) Keynes argued that probability referred to 'partial entailment' relationships, of which classical entailment is merely a limiting case. The spirit of this approach was adopted by Carnap in his (1950) and subsequent works. There are, as I have noted, strong similarities between my approach and the Keynes‑Carnap approach. However, there are two crucial points on which I differ from Carnap, and the point of this section is to briefly set out Carnap's theory and my grounds for dissenting from it.

The exposition of Carnap's position given here largely follows the exposition in T. Fine (1973: Ch. 7) and Carnap's own summary in his (1963). In simple terms, Carnap analyses probability in terms of degree of belief. The probability of *h* given *e* is the degree of rational belief in *h* given *e*. That's entirely accurate, but Carnap didn't like it as a description because it might have misleadingly subjectivist connotations. So as a next approximation he said the probability of *h* given *e* is the degree of confirmation of *h* by *e*. But this term too could be misinterpreted, as his exchange with Popper in the 1950s indicated. So he eventually defined the probability of *h* given *e* as the 'rational subjective value' in utils of a bet which pays 1 util if *h* and nothing otherwise to an agent with evidence *e*.

It isn't clear why Carnap thinks this explanation of probability should imply it is always numerically valued. The comments at (1963: 972) suggest he thinks this is necessary for probability to be used in rational decision making. In any case, he set himself the task of developing a quantitative theory of probability in account with the above analysis. Carnap thinks, correctly in my opinion, that the concept of probability[^4] he is working with is crucial to induction. Sound inductions are those where the logical probability of the conclusion given the premises is high. So he draws the following conclusions:

[^4]: Actually Carnap thought there were two concepts of probability, one based on 'logical probability' and the other based on frequency, so the text might be a bit misleading here. However, it was the logical concept, his probability~1~ which attracted most attention, and which in later writings he referred to simply as probability. Hence most commentators have adopted the convention I'm using of referring to probability~1~ as Carnap's conception of probability.

> \(a\) The reasons \[for accepting axioms of inductive logic\] are based upon our intuitive judgements concerning inductive validity, i.e. concerning inductive rationality of practical decisions (e.g. about bets); therefore:
>
> \(b\) It is impossible to give a purely deductive justification of induction.
>
> \(c\) The reasons are *a priori* (Carnap 1963: 978).

I think (c) is correct, but Carnap goes further. He has taken *h* and *e* to be sentences, not propositions, and he thinks that the probability of *h* given *e*, like the provability of *h* from *e*, can be determined by purely syntactic considerations. The position I will take is that while probability sentences are non-contingent, they are like 'All bachelors are unmarried' in being true in virtue of their non-syntactic features.

To spell out this qualitative concept of logical probability, Carnap attempts to develop a *c*-function which will give the value for the 'degree of confirmation' of *h* given *e* for any *h*, *e* in a given language, written as *c*(*h*, *e*). To narrow down the class of functions which could serve the role of *c*, he adopts a number of axioms. These fall into three categories. The first category does enough to say *c* is a conditional probability function. The second are symmetry constraints. So for example we have an axiom saying that universal substitution of one name for another throughout *h* and *e* leaves the value of *c*(*h*, *e*) unchanged. And similarly universally substituting one predicate for another from the same family[^5], or substituting one family of predicates for another family with the same number of elements leaves *c*(*h*, *e*) unchanged. Finally, adding new families of predicates to the language will leave *c*(*h*, *e*) unchanged, as will adding terms for new individuals, provided *h* and *e* contain no quantifiers. It is these invariance postulates which prompt me to describe Carnap's as a 'syntactic' theory of probability. Finally, Carnap has three axioms asserting that *c* must allow an agent to learn from experience. However, as Fine shows these don't do much to restrict the class of permissible *c*‑functions, and in some cases are simply redundant.

[^5]: A family of predicates is a set of predicates such that every possible individual satisfies exactly one of them.

I will note two types of objection to Carnap's approach. The first essentially object to his claim to have developed a quantitative theory; the alleged faults are caused by his claim that *c* is real-valued. The second object to the claim that probability is syntactic. I think both classes of objection will succeed, though the first class can be met by a simple alteration to the theory.

There are four problems with Carnap's claim to have developed a quantitative account. van Fraassen (1989: 119‑25) stresses the point that despite Carnap's aim, the axioms he gives do not suffice to specify a unique *c*‑function. Carnap of course knew that we had to posit a continuum of *c*‑functions, but could say little about how to choose between them (Carnap 1952). Fine notes that if we make a seemingly plausible extension of Carnap's axioms, if we insist that uniform substitutions of one complete description of the world for another leaves probability unchanged, we are led into inconsistency. From this we conclude that not all symmetry requirements are met, that Carnap's axioms aren't as plausible as seemed at first (since the intuitions which grounded them perhaps provide equal support to inconsistent axioms) and, Fine argues, that Carnap must say that the probability of *h* given *e* is not determined by the meaning of *h* and *e*. The point here is that if the language includes the family of predicates {*red*, *not-red*} then the probability of *a is not-red* given a tautology is 1/2, whereas if it includes the family {*dark red*, *light red*, *not-red*} the probability of *a is not-red* given a tautology is 1/3. Thirdly, as Howson and Urbach (1989: Ch. 3) urge, not all symmetry requirements can be met at once. They note that different symmetry requirements are inconsistent. Fourthly, as Keynes (1921a) notes, there is no principled way to avoid the paradoxes of indifference if we insist that all probabilities are numerically valued.

Some of these problems look like they'll go away if we allow there to be more than one permissible *c*‑function. Carnap at one point (1963: 971) goes very close to endorsing just this. However, there are a separate set of objections which can be levelled at the syntactic parts of Carnap's theory. There are two objections which can be levelled at this, the first based around the problem of non-projectability and the second around some objections of Jeffrey to Carnap's theory of evidence.

For our purposes it will be preferable to use the discussion of non-projectability in Russell (1948) rather than the more standard discussion in Goodman.[^6] As Russell notes, for many predicates *F* and *G*, the inference 'All *F*s observed so far have been *G*s' therefore 'The probability that all *F*s are *G*s is high' is sound. Or again, the probability that all *F*s are *G*s given all *F*s observed so far have been *G*s must be high. If we are to base probability around syntactic considerations and get started at all, we will have to accept this rule. However, as Russell also notes, some inferences of this form are clearly unsound. If a farmer has only seen cows in Heresfordshire so far in his life, this is no justification for believing that probably all cows are in Heresfordshire. That is, the inference is unsound when *F* is 'is a cow' and *G* is 'is in Heresfordshire'. But the unsound inference has the same syntactic form as some sound inferences. So probability can't be based on syntactic form.

[^6]: Discovery of the problems for induction caused by non-projectability is usually credited to Goodman (1954), or perhaps his (1947). However, the discussion in the earlier paper is rather brief, indeed just confined to the predicate *P* of an artificial language. Further the problem of non-projectability is urged more as a problem for the analysis of counterfactuals rather than for induction, as is now standard. So for these reasons I prefer giving credit to Russell. It would be interesting to discover when Russell discovered this problem. In his (1940) he is obviously ignorant of it. In the introduction to (1948) he mentions that parts of it are based on lectures he gave in 1944‑45, but doesn't make clear which parts. And the papers published from this time in his *Collected Papers* yield little light on the matter. My crediting Russell with this discovery is not meant to say Goodman's book was anything less than an independent discovery, and of course it moved the debate forward and promoted the idea of non-projectability in a way which in the long run proved more effective.

Perhaps there is a way out of this problem. We could, for example, restrict the language in which we allow inferences to be made, so there is no way in the language to represent the troublesome inference as being of the same syntactic form as the sound inferences. This is what Kyburg does in his logical approach. Recently Tooley (1987) has argued that if we are realist about universals, we can restrict the predicates of our canonical language to those universals which exist, and presumably all the universals are projectible. If the existence of universals is non-contingent this might do the work required, though if not the probability sentences will not be *a priori* as Carnap required.

There is, however, a bigger problem. Despite what may be inferred from some philosophy texts, we don't just make inductive inferences or use probability sentences in physical sciences. The idea behind Kyburg's and Tooley's approach is that the ideal language of science will not include troublesome predicates like 'is in Heresfordshire' or Goodman's 'grue'. While it *might* be true that no absolute positional predicates are needed in physical science[^7], this just isn't true in social sciences. At the very least we are going to need predicates like 'in a city', 'in the country' to do the most primitive sociology. So the ideal language of science generally will most likely include predicates which are not projectible.[^8] This isn't yet an argument for saying that we will always end up with gruesome predicates, just an argument for saying that quite a lot more needs to be done to show we are rid of them.

[^7]: Clearly predicates referring to relative positions are needed.

[^8]: Could we have different languages for social sciences and physical sciences? It seems like a pretty desperate move. It would be hard then to explain sentences which referred to terms from both physical and social sciences, like 'It's more probable that a recession will occur than that this atom will decay in the next *n* days'.

In any case, there's another problem for the syntactic account. For this account to be plausible, we have to be able to specify the evidence we have for a proposition in a finite sentence of some language. But this seems implausible, as Ramsey (1926a) and Jeffrey (1991) have stressed, because of vague evidence. If we view probability as a semantic relationship between propositions, rather than a syntactic relationship between sentences, this is no longer a problem, as I outline in chapter 2. The combined effect of these objections to Carnap's account is enough to suggest a different approach could be worthwhile.

## 1.8 Probabilities are not Subjective {#probabilities-are-not-subjective}

Theories of probability which are called 'subjective', either by their proponents or detractors, abound in the modern literature. Surprisingly then, it is hard to get a clear picture of what is meant by a subjective theory. So my first task in this section is to draw a brief taxonomy of subjective positions. I divide subjective positions into four types, depending on how they deal with two questions. The theories can either define probability in terms of rational degrees of belief or something less, perhaps actual or coherent degrees of belief. And they can be assertoric or expressivist theories. An assertoric theory says that probability sentences make a truth-apt claim about degrees of belief; an expressivist theory says that probability sentences make no claim about how the world is, they just express an attitude. This distinction can be quite clearly seen in looking at different subjectivist ethical theories. An assertoric subjectivist analyses *Torture is wrong* as *Someone* (*perhaps me) disapproves of torture*. On an expressivist analysis it comes out as *Boo torture*! or some more sophisticated variant on that. There is a fact as to whether I disapprove of torture, hence the assertoric theory is truth-apt, but *Boo torture*! says nothing even plausibly truth-apt. It is, I think, surprising that more subjectivists in probability have not been drawn to expressivist analyses.

One important clarification needs to be made to the above account. Despite some of the quotes I will adduce below, no one seriously believes that probability can be defined purely in terms of actual degrees of belief. If this were the case, there would be no laws of probability at all; as van Fraassen (1990) put it, any such law could be refuted by the existence of a moron. So our moron's degrees of belief have to be made coherent before they can enter into the analysis of probability sentences. As Max Black (1967) put it, degrees of belief have to be at least 'rectified' before we can use them in analysis. Rectified degrees of belief satisfy some minimal coherence requirements, but nothing more. That these coherence requirements should amount to conformity with the probability calculus is argued for by Dutch Book arguments (see chapter 2) or some variant on them. In non-probabilistic (or classical) epistemology, consistency is not normally considered a sufficient ground for reasonableness. One can consistently believe *The moon is made of green cheese*. Similarly rectified degrees of belief can contain a high degree of belief in *The moon is made of green cheese*. Such a belief state would not be reasonable on any ordinary usage of that term. Despite this, some subjectivists (especially Savage and de Finetti) use reasonable to just mean rectified, and this leads to some confusion. I find Black's terminology clearer, and I'll employ it in what follows.

So we have our four types of subjectivist theory, outlined in the table below.

+----------------+-------------------------------+-----------------------+
|                | Assertoric                    | Expressivist          |
+================+===============================+=======================+
| Rectified      | Type 1                        | Type 3                |
|                |                               |                       |
|                | de Finetti, Howson and Urbach |                       |
+----------------+-------------------------------+-----------------------+
| Rational       | Type 2                        | Type 4                |
|                |                               |                       |
|                | Keynes, Carnap                | Blackburn             |
+----------------+-------------------------------+-----------------------+

The names under each type list adherents of each position. My claim will be that there are strong objections to types 1, 3 and 4 and that type 2 is not properly regarded as subjectivist. The objections to types 1 and 3 are quite old, I will say little about them that is not said by Ayer. My argument that type 2 is not a breed of subjectivism is found entirely in Carnap. And whether or not type 4 subjectivism works seems to turn on whether or not a broadly expressivist program could work, a topic that could cover several chapters on its own. I'll simply note some of the arguments in the literature as to why it fails. So the originality of this section is confined to its organisation.

Before starting on these objections, I should note one way in which the organisation itself is derivative. Kyburg (1978: 79-80) gives a different four-fold taxonomy of subjectivist positions. The rows are the same as in my table, but the columns refer to a different property. He divides subjectivist theories into theories of decision and theories of degrees of belief. Since I don't regard theories of decision as theories of probability I could hardly adopt this division. Kyburg in turn doesn't consider the distinction between assertoric and expressivist theories. But the motivation for the four-fold taxonomy is in part his paper.

### 1.8.1 Type 1

It might be thought that type 1 subjectivism is a mere straw man, something I would set up to be knocked down to show the weaknesses of subjectivism. However, it is very hard to read the following quotes as endorsing any other type of subjective theory.

> Let us suppose that an individual is obliged to evaluate the rate at which he would be ready to exchange the possession of an arbitrary sum *S* (positive or negative) dependent on the occurrence of a given event *E*, for the possession of the sum *pS*; we will say by definition that this number *p* is the measure of the degree of probability attributed by the individual considered to the event *E*, or, more simply, that *p* is the probability of *E* (according to the individual concerned; this specification can be implicit if there's no ambiguity) (de Finetti 1937: 102).
>
> In the personalistic \[i.e. subjectivist\] concept, probability is an index -- in an operational sense to be explained later -- of a person's opinion about an event (Savage 1964: 176).
>
> We shall argue that ... \[probabilities\] should be understood as subjective assessments of credibility, regulated by the requirement that they be overall consistent. (Howson and Urbach 1989: 39)

Perhaps these might be interpreted as saying that the ordinary language concept of probability is so useless we ought replace it with the concept degree of belief. This might be one interpretation of de Finetti's later view that "Probability does not exist", printed in capitals on his (1974: i). However, these quotes seem to be claiming we can analyse *probability* simply as *degree of belief*. And this must be a mistake, because of two arguments from Ayer.

Ayer (1936: 101) rejects this kind of subjectivism about probability because of the 'obvious objection' that it doesn't allow a person to be mistaken about the probability of a proposition. Since the probability of *p* is just your degree of belief that *p*, whatever you believe is the probability of *p* will be its probability. Strictly this mightn't be quite correct. Presumably a person might believe *p* to degree 0.2 and believe they believe it to degree 0.3, and hence falsely believe the probability of *p* is 0.3. So Ayer is wrong to say the subjectivist doesn't allow mistakes, but they don't allow mistakes from perfectly introspective agents, which is still implausible.

With this objection Ayer is content to dismiss type 1 subjectivism about probability. There is another objection which we can extract from his dismissal of a simple subjectivist position in metaethics. He dismisses analyses of "*X* is wrong" as "I disapprove of *X*" by noting that a person can consistently say that they disapprove of things which are not wrong. The equivalent point is a little harder to put in epistemology because of Moore's paradox, but it can easily be brought out in a little dialogue. If the subjectivist were right, *B*'s utterance would be consistent.

*A*: It is highly probable that the moon is made of green cheese.

*B*: What *A* says is true, but it is not probable that the moon is made of green cheese.

According to type 1 subjectivism, *A* is making a report about his mental state. *B* can presumably assent to that report, he agrees *A* thinks it probable that the moon is made of green cheese, while consistently saying that *A* is mistaken. But our intuition surely is that *B*'s utterance is inconsistent, which makes type 1 subjectivism implausible.

As an aside, it is possible subjectivists were trying to capture a concept other than *probability*. For instance, in later papers, whenever Savage went to say what the subjectivist (he preferred 'personalist') is claiming, he would give his preferred definition of the *probability for a person* of a proposition. (See for example Savage 1967a and 1967b.) Plausibly he is right *vis a vis* this question, but that concept is not central to probability sentences generally. I am no more making a report about my mental state when I utter *The moon is probably made of green cheese* than when I utter *The moon is made of green cheese*.

### 1.8.2 Type 2

Type 2 is called subjectivist by Kyburg (1978) who opposes it, and Lewis (1980) who endorses it. Keynes (1921a: 4) vacillates, saying the concept is in part subjectivist, because probability is relative to evidence, and partially not, because it is independent of what anyone thinks. Carnap (1950: 37‑50) argues at length that this approach is not properly called subjectivist. I will just rehearse some of Carnap's arguments.

Carnap has two primary arguments for calling his probability~1~ concept (what I call probability) 'objectivist'. These are that probability sentences are non-contingent and that whether or not they are true is not dependent on anyone's thinking about them. We do use psychologistic terms when giving an analysis of probability, we talk about beliefs, but Carnap has two further reasons for thinking this doesn't imply subjectivism. First, we never define probability in terms of beliefs *simpliciter*, always in terms of reasonable beliefs. So ours is, in Carnap's language, a *qualified psychologism*. The second reason, which is in part a consequence of the first, is that we can eliminate the psychologistic references from formal presentations. So Carnap defines probability not in terms of degree of reasonable belief, but in what amounts to the same thing, degree of confirmation. Maybe this isn't an improvement, perhaps we can only explain confirmation by reference to reasonable beliefs, but the first two arguments seem sound enough. Hence I think it is possible to define probability in terms of reasonable degrees of belief and oppose subjectivism[^9].

[^9]: It should be remembered that Type 2 theories themselves form a large class, so that Lewis, Keynes and Carnap appear to all endorse theories from this class, but this does not imply there is close similarity between their respective views.

### 1.8.3 Type 3

The difficulty with looking at possible objections to expressivist interpretations of probability is that there has been so little said about them. This is surprising given the well-known difficulties attending type 1 subjectivism. However, at least this type of expressivist theory seems to do no better. Indeed, it isn't obvious how we avoid the two problems Ayer raises for type 1 subjectivism by saying probability sentences are expressive rather than assertive.

In fact we acquire a new problem. To say that the primary function of a sentence is expressive is no theory at all; we have to say what is being expressed. But it is hard to see how type 3 subjectivism can solve this problem. If we say that what is being expressed is a belief, it looks like probability sentences really are assertoric. After all, the type of utterances that express beliefs are assertions. Perhaps the situation is different when we express a partial belief, but it's hard to see how. And it is hard to see how we could say that probability sentences express anything else without giving up the hope of analysing probability sentences in terms of merely rectified beliefs, rather than say rational belief. So for these three reasons type 3 subjectivism seems untenable.

### 1.8.4 Type 4

Moving to type 4 subjectivism solves all three of these difficulties in one stroke, which bodes rather well for its success. The idea behind this theory is that probability sentences express commendation of certain epistemic states and disapproval of others, or perhaps express some more subtle dispositions to commend and disapprove. The idea is just to extend the analysis of ethical sentences offered in Ayer (1936), Blackburn (1984) and Gibbard (1990) to probability sentences. Indeed, Gibbard explicitly endorses an expressivist analysis of 'reasonable' and Blackburn (1980) suggests a very similar account of 'chance', though he uses the term in much the way that 'probability' would now be used.[^10] Blackburn claims that Ramsey also adopted this account, which might be correct.

[^10]: For example, he argues that we can talk about non-integer chances in a deterministic world. We can certainly talk about probabilities in a deterministic world, but standard usage now seems to be that we can't talk about such chances. See Lewis (1986: 118).

One unimportant technical point before proceeding. We can't analyse 'The probability of *p* is 0.2' as a commendation of believing *p* to degree 0.2. The simple reason is that 'The probability of *p* is 0.2' entails 'The probability of *p* is not 0.3' but we can commend believing *p* to degree 0.2 without disapproving of believing *p* to degree 0.3, because in some circumstances we might regard different, and indeed incompatible, states worthy of commendation. Similar remarks apply if we analyse probability sentences as expressions of something more complicated. But this problem is solved by just analysing an utterance of 'The probability of *p* is 0.2' as commendation of believing *p* to degree 0.2 and disapproval of all other degrees. Anti-expressivist, or cognitivist, analyses of probability in terms of reasonable beliefs will have to make a similar complication to their story, so this is no argument against expressivism generally.

That unimportant point aside, there is a more pressing difficulty for expressivist theories generally. I won't go into great detail here, in part because a fair discussion of this point would require a thesis length exposition on its own. In part, however, my lack of detail is caused by the possibility that there is less distance between my position and the expressivist position than appears at first. Some modern theorists, including some disposed to expressivism, have thought that an expressivist approach to some class of utterances, ethics being most frequently discussed, is compatible with believing utterances in that class to be truth-apt (e.g. Price (1994), Horwich (1994)). Since the traditional statement of expressivism is precisely that certain classes of utterances are not truth-apt, this might seem like a fairly substantial change, but there are reasons for the move[^11]. Price, for example, argues that the essence of expressivism in ethics lies in the claim that the function of moral utterances like 'Stealing is wrong' is significantly different from the function of non-moral subject-predicate sentences like 'Snow is white' despite their common syntactic form. The latter class have as their primary aim making (accurate) descriptions of the physical world; moral sentences have as their primary aim expressing a certain outlook. If Price is right then the difference between Type 2 and Type 4 theorists lies only in the *pragmatics* of probability sentences, not in their semantics, or for that matter their syntactical rules. This is undoubtedly an important question, but it's not one I've sought to address here. So I regard this type of expressivism as compatible with the theories I'm promoting.

[^11]: For Price, it is to escape from the Frege point I'll set out presently; for Horwich, it is because of his minimalist conception of truth.

The important problem for expressivism is what has become known as the Frege point. This was first explicitly set out in Geach (1965), though Geach had hinted at it earlier. The point is that the following argument is clearly valid.

\(1\) If stealing is wrong, then getting little brother to steal is wrong.

\(2\) Stealing is wrong.

\(3\) Getting little brother to steal is wrong.

There are two problems intertwined here for the expressivist. The first is explaining how we get the meaning of (1) from its components. That is, it clearly isn't a full explanation of the meaning of (2) to say that when uttered it expresses a con-attitude towards stealing, for this doesn't explain how it contributes to the meaning of (1). This is a decisive refutation of some primitive expressivist theories (like that in Ayer (1936)) but is no problem for modern approaches which acknowledge this question and present answers to it. However, it is a constraint on those answers that they be consonant, in some broad sense, with the expressivist analysis of (2). In part this consonance is imposed for theoretical considerations; it would hardly be plausible to say moral words like 'wrong' function in a radically different way in antecedents to the way they function in simple sentences. And it's imposed because of the second problem for the expressivist; they have to explain how the argument is valid. That is, they have to show the logical incoherence of accepting (1) and (2) and not accepting, or worse denying, (3). And in part this will require showing there is no equivocation in meaning between (1) and (2), else we will not have a clearly valid argument.

Price (1994) suggests we can get out of this with an expressivist analysis of conditionals. His theory of conditionals might be on the right track, and seems to do the work the expressivist needs[^12]. But I don't think this solves the overall problem. The problem arises because of a convergence of two facts: moral sentences occur in unasserted positions in sentences, and those sentences combine with simple moral sentences to form valid arguments. This is exemplified by conditionals like (1), but it is also exemplified by disjunctions like (1´).

[^12]: See Barker (1995) for an outline of a pragmatic theory of conditionals that seems broadly correct and compatible with Price's version of expressivism.

(1´) Either stealing isn't wrong or getting little brother to steal is.

The truth functional analysis of conditionals has prominent supporters, but it is highly controversial and it hardly seems to be a refutation of a theory that it needs to deny it. On the other hand the truth functional analysis of disjunction is so entrenched, and so explanatorily successful, that it would require some large trade offs for it to be given up. And disjunctive syllogism is slightly more contentious than *modus ponens*, but still commonly enough accepted that it would be a cost for the expressivist to give it up. So I suspect the expressivist has to explain how moral sentences function as disjuncts, and how this story combines with the ordinary story about disjunction and validity to yield the validity of the argument from (1´), (2) to (3).

The problem has been the subject of a number of attempted solutions. However, I agree with Hale's contention that these solutions fall to a simple dilemma (Hale 1993: 340). Either the solutions do not explain how arguments like (1´) and (2) to (3) are logically valid in the sense that a person asserting the premises and denying the conclusion would suffer from a *logical* shortcoming, or they fail to explain the meaning of the disjunction in a way consonant with the expressive explanation of the meaning of the disjuncts. Hale argues that the solution proposed by Blackburn in his (1984) falls to the first horn, and the new solution proposed in Blackburn (1988) falls to the second.

In (1984) Blackburn argued that we could interpret (1) (and (1´)) as expressions of a con-attitude towards disendorsing stealing but not disendorsing 'getting little brother to steal'. The problem with this approach, as Blackburn came to realise, was that it posits the wrong kind of incoherence on the part of the person who asserts the premises and denies the conclusion. Such a person seems to suffer from the moral fault of not upholding their own second-order principles, but this is hardly a logical fault, which is what the expressivist needed to show. Blackburn has subsequently developed a different approach to explaining (1) and (1´) (Blackburn 1988). Hale argues that the interpretation adopted there is ambiguous, either disjunctions and conditionals are read truth-functionally, in which case we don't have a reading consonant with the expressivist reading of simple sentences, or they are read expressively, in which case they still don't underlie the validity of the relevant arguments. There are more arguments to be had on this point, but there are enough problems here to suggest there is value in exploring a non-expressivist approach, as I do in subsequent chapters.

### 1.8.5 The Exchangeability Point

Given the objections I've made to subjectivism, the following defence of subjectivism may not seem immediately relevant, but perhaps its proponents intend it to defuse Ayer's objection that subjectivism doesn't allow for the obvious fact that epistemic states can be coherent but mistaken. In any case, the point may provide some defence of the Type 2 theory I want to defend. The idea is that coherence alone requires convergence of degrees of belief over time, so perhaps the epistemic states I described as coherent but mistaken are not really coherent at all.

The crucial concept is de Finetti's idea of *exchangeability*. I'll just deal with a very simple version of this idea, because it does well enough at bringing out all the philosophical points involved. Assume that *m* trials will be conducted, each trial having two possible results, say that for some variable *x* either *x* = 0 or *x* = 1. So there are 2^*m*^ possible outcomes for the series of trials. That is, we identify outcomes with the sequence of values of *x* according to each trial. Call the sum of an outcome the number of ones it contains. An agent regards the trials as exchangable over this sequence of trials iff they have the same degree of belief in any two outcomes with the same sum, and exchangable generally iff they would regard any sequence of *m* trials as exchangable, whatever the length of *m*.

Exchangeability is not the same thing as probabilistic independence[^13]. An agent can regard the trials as highly interdependent in the sense that once they learn the outcome of an initial sequence of trials they would change their degrees of belief about the results of subsequent trials. For example, assume a biased coin is about to be tossed 5 times, with *x* = 1 meaning it lands heads and *x* = 0 meaning it lands tails. An agent regards the coin as so biased that she is certain it will land the same way on every trial. But she has no idea of the direction of the bias, so she assigns probability 1/2 to the sequence \<1, 1, 1, 1, 1\> and 1/2 to the sequence \<0, 0, 0, 0, 0\>. Then she regards the trials as exchangable in this sense, but clearly not independent.

[^13]: Formally propositions *A* and *B* are probabilistically independent iff *Pr*(*A* & *B*) = *Pr*(*A*) · *Pr*(*B*), or, equivalently, *Pr*(*A* \| *B*) = *Pr*(*A*).

The importance of exchangeability lies in some convergence results developed by de Finetti. Assume two agents update their beliefs by conditionalisation[^14], and regard a long sequence of trials as exchangable. Then, provided they don't completely rule out some possibilities to start with, their degrees of belief about success on the next trial will converge. That is, for any ε \> 0, there is an *n* such that after *n* trials their degrees of belief in success on the next trial will differ by at most ε. One philosophical interpretation is to say that this removes the more perniciously subjectivistic elements from subjectivism. The subjectivist now has an explanation of not just why convergence of opinion occurs (most dramatically perhaps in the convergence of opinion about decay times for radioactive elements), but of why it ought occur. Perhaps, the argument could continue, anyone who differed from this great convergence would be unreasonable in a way that even Type 1 subjectivists could object to.

[^14]: That is, upon learning *B* they assign to *Pr*(*A*) whatever value they used to assign to *Pr*(*A* \| *B*).

The problem with this move is simply that there is nothing in Type 1 subjectivism which grounds the claim that agents should regard certain trials as exchangable. Indeed, in seeking to discriminate between different coherent states on the grounds of their reasonableness (i.e. between those that do and don't regard trials as exchangable) we have slipped towards Type 2 theory, which Carnap showed is not subjectivist at all. This point is made by Kyburg (1978: 67) who attributes it to discussions with Nagel.

Matters are even worse for the Type 1 subjectivist. An event can be interpreted as many different types of trial. For example, drawing an emerald from an urn can be regarded as a trial of whether the emerald is green or not-green, and whether it's round or not-round. More interestingly, we can regard it as a trial of whether the emerald is grue or not-grue. And of course once we've recognised grue we can recognise all sorts of other predicates, such as green on an even numbered trial or blue on an odd numbered trial. We can't, consistently, regard all such sequences of trials as exchangable. So we must make a selection, before the evidence comes in, as to what we will regard as the exchangable trials. But that we must make such choices before seeing any evidence is what distinguishes Carnap's Type 2 approach from de Finetti's Type 1 approach.

Indeed, we can turn around de Finetti's result to be a defence of a variant of Carnap's position. The Type 2 theorist is burdened by the necessity of saying something about what is reasonable on zero evidence. Carnap rose to that challenge by trying to give the precise numerical value of every proposition on zero evidence, but as we saw in section 1.7, his attempts seemed doomed. Keynes allowed more flexibility by letting probability values be non-numerical, and I'll essentially be following Keynes here. I think Carnap's program is best served by not trying to find *the* reasonable probability function, but the set of such reasonable functions. de Finetti's convergence theorem can be used to argue that what distinguishes elements of this set is not the value they give to particular propositions under no evidence, as Carnap thought, but what sequences of trials they regard as exchangable. Roughly, reasonable probability functions are reasonable by virtue of their content, not as Carnap thought by virtue of their form. We are, however, getting ahead of ourselves. I'll return to this matter in my defence of this theory against various objections in chapter 6.


