# Objections {#sec-chap-6}

## 6.1 Introduction {#introduction-2}

So far I have defended two distinctive views about probability. The first is that probability sentences, properly construed, are non-contingent. The second is that the probability relation between propositions is not always numerical. In @sec-chap-5, I relied on the technique of supervaluations to explain how these non-numerical relations interact. I call any analysis of probability which defends the first of these theses a *necessitarian* analysis, and any analysis which defends the second an *imprecise* analysis. (The imprecision in question is in the values of the probability relations, not in the analysis!) The first writer to defend each of these theses was Keynes (1921a). Several of the objections that I'll discuss in this chapter were first made by Ramsey and aimed at Keynes's theory, so before commencing I need to note one important difference between my theory and his. Keynes, at least in his *Treatise on Probability* held a *logical* analysis of probability. Some of the objections to Keynes I'll discuss are objections to the logical analysis, but are not objections to a necessitarian view. To defend my theses, I don't need to respond to these. Indeed, as I pointed out in section 1.7, it is because I accept some of them that I reject the logical analysis.

One other distinct feature of Keynes's theory should be noted. I mentioned in @sec-chap-5 that we could think of the probability of *p* given *q* as a function from distributions to elements of \[0, 1\]. Keynes says that probabilities can be non-numerical, but he doesn't give this analysis of their 'internal structure'. He is happy to say the probability of *p* given *q* is α, where we might know no more about α than α \> 0.2 and α \< 0.6. By means of some axioms, he shows how we can interpret addition and multiplication of these probability-values, but it is rather unclear whether or not addition and multiplication still have the same meaning they do in natural language once these axioms are added. As a consequence, when I claim that objections of Ramsey's that are directed at Keynes's refusal to discuss the structure of probability relations don't harm my theory, I am not attacking Ramsey (his objections usually hit their intended target) but merely showing why my theory isn't doomed because of his work.

Since Ramsey levelled so many distinct objections to the Keynesian project in his 1926 paper *Truth and Probability* I will spend most of this chapter analysing and where necessary responding to those objections. It is commonly assumed, particularly in non-philosophical discussions of Keynes's work[^85], that the Keynesian theory was fatally wounded by Ramsey's early attacks on it. I will argue that while Ramsey's attacks seem to work against logical theories, they aren't overly persuasive against necessitarian views. I will argue in @sec-chap-10 that Keynes held a similar, if less worked out, view of the value of Ramsey's attacks. There are several separate arguments against Keynes squeezed into Ramsey's paper. For ease of later reference, I'll first simply list what I take to be the main arguments before attempting any kind of response.

[^85]: See, for example, Bateman (1996) and Runde (1994a). From a more philosophical perspective, the same assertion is made in Zabell (1991: 224).

-   There aren't any probability relations of the kind Keynes requires.

-   If there are such relations, we can't determine what their value is when the relata are simple.

-   It is mysterious how the probability of *p* can go from being incomparable with any given number to being numerically precise by the addition of evidence.

-   It is unclear why Keynes's probability relations should obey the laws of the probability calculus.

-   Keynes's theory relies on the discredited Principle of Indifference.

-   Keynes's theory requires that our evidence be known for certain, but much of the time our evidence is vague and uncertain.

Unlike Keynes, Carnap backed up his necessitarian theory with a detailed calculus for the probability logic. For various reasons, this calculus has attracted more attention than the philosophy underlying it. Howson and Urbach (1989) provides one exception, and I deal with their objection to Carnap at the end. I suspect their worry is widely shared, and their presentation sufficiently representative, so my replies may be relevant to many critics.[^86]

[^86]: All page references in this chapter unless otherwise stated are to Ramsey (1926a).

## 6.2 There are no such things {#there-are-no-such-things}

> But let us now return to a more fundamental criticism of Mr. Keynes' views, which is the obvious one that there really do not seem to be any such things as the probability relations he describes. He supposes that, at any rate in certain cases, they can be perceived; but speaking for myself I feel confident that this is not true. (161)

In his original theory, Keynes held that we could perceive directly the probability relations between propositions, in the same way that we perceive entailment relations. Some have argued that this paragraph was a decisive refutation of Keynes's Platonist assumptions. It is, of course, nothing of the sort. Ramsey does not deny that we can perceive, in some sense of that word, entailment relations. Nor does he deny that we can perceive relations, such as having a common subject or predicate, which hold between propositions, at least on a Tractarian view of what propositions are. All he rejects is that we can perceive the particular relations Keynes posits.

From the way we have put forward the theory, this is a rather odd thing to say. He couldn't be saying that he doesn't perceive that there are some pairs of coherent belief states and evidence sets which are reasonable and some which are unreasonable. In fact he goes on to say that a person who doesn't draw inductive conclusions from their evidence is unreasonable (197). However, he doesn't believe that such an 'unreasonable' person would 'sin against formal logic or formal probability'. Now this may be so on a narrow construal of 'formal'. If, however, we are internalist about epistemic justification then we are committed to saying that we can perceive that such a person's epistemic states are *necessarily* unreasonable[^87]. So, to perceive that such a person is unreasonable is to make a perception which, if it is correct, is necessarily correct.

[^87]: This will be false unless we take 'epistemic states' to be a rigid designator, which for ease of exposition I do.

On the theory advocated here, if for every value of *n* we can perceive whether believing *p* to degree *n* on a certain body of evidence *q* is reasonable or unreasonable, then we can perceive the probability relation between *p* and *q*. We won't ordinarily be able to do this because the boundaries between the reasonable and the unreasonable will be vague, but this merely corresponds to the probability relation being vague. The crucial point is that there is nothing more (or less) to a probability relation than a bundle of facts of the form *It is* (*not) reasonable on such-and-such evidence to have this degree of belief in p*. Since Ramsey is happy to say we can perceive facts of this latter sort, it follows that we can perceive probability relations.

Ramsey goes on to say that, because other people can't agree on the value of probability relations, he believes no one else perceives them either. In part this criticism is met by the theory of vague probabilities set out in @sec-chap-5. There it was argued that reasonable people could have different degrees of belief in the same proposition on the same evidence. However, in these cases, the probability of a proposition on some evidence isn't *the* degree of belief a reasonable person would have in the proposition on that evidence. There is no such degree, even if we allow non-numerical degrees of belief. Rather there are a range of reasonable degrees, and the value of the probability relation is this set.

## 6.3 Probability Relations Between Simple Propositions {#probability-relations-between-simple-propositions}

> If, on the other hand, we take the simplest possible pairs of propositions such as 'This is red' and 'That is blue' or 'This is red' and 'That is red', whose logical relations should surely be easiest to see, no one, I think, pretends to be sure what is the probability relation which connects them. (162)

Ramsey might concede that we can work out the probability relation between two complex propositions such as our entire current evidence and "Oswald killed JFK". However, as the above quote indicates, we can't tell what the probability relation is between simple propositions. This is entirely at odds with the rest of logic, where we are prepared to rely on agreement about the relationships between simple propositions to work out the relationships between complex propositions.

The easiest way to shrug off Ramsey's objection here would be to say that it only directly attacks logical analyses of probability rather than necessitarian analyses. However, the point deserves some more discussion. After all, even if these examples aren't central on a necessitarian view, as they are on a logical view, we are still committed to saying that there is a probability relation between them.

In previous chapters I have had occasion to identify probability relations with the set of values they take. This is a convenient shorthand, though it does lose some information. In this notation, I can say quite precisely what the conditional probability of *This is red* given *That is blue* is. It is the interval \[0, 1\]. In other words, there is no degree of belief in *This is red* which is either ruled out as irrational or ruled in as the only rational response on this minimal evidence. This response to Ramsey doesn't entirely succeed, because the interval notation isn't fully informative, but it is a start.

The other response we can make to this objection is that Ramsey's assessment of simplicity is very much related to his Tractarian conception of propositions. This analysis of propositions is crucial to the logic of decision he develops, and for which the paper is more well-known. If we analyse propositions as sets of possible worlds, and assume possible worlds are the 'ultimate organic unities' (177) which are the subject of choices, we cannot use Ramsey's method for determining beliefs and desires. On my preferred analysis of propositions, *This is blue* will not be a simple proposition in any interesting sense. On the contrary, it is as heterogenous a set of possible worlds as one could care to imagine. The truly simple propositions will be those which are true at one world only. However, the linguistic representation of such propositions will be infinitely complex. Hence *simplicity* is theory-dependent, and even if I agreed with Ramsey that not being able to say what the probability relation was between simple propositions was problematic, these examples wouldn't count against my theory.

## 6.4 The Sorites Objection {#the-sorites-objection}

> \[I\]t is hard to suppose that as we accumulate instances there is suddenly a point, say after 233 instances, at which the probability relation becomes finite and so comparable with some numerical relations. (162)

On Keynes's view, as on mine, when we have observed a billion red round things, and no non-red round things, the probability that *a* is red given that *a* is round is greater than 0.99. That is, it enters into numerical comparison. This is necessary if we are to have any projectible predicates. However when we reduce the number of observations from a billion to, say, one, the resultant probability is not comparible with any numbers, or numerical relations as Ramsey puts it. Ramsey seems to think this combination is implausible, because it would require an arbitrary number of observations after which the probability does become numerically comparible.

It is a remarkably weak objection, particularly by Ramsey's standards. Assume a certain pile of sand is not a heap. I can, by adding one grain at a time, end up with a heap of sand. However, I don't have to assume there is some point, say 233 grains, at which it suddenly becomes a heap. Given this it is hard to see what the basis of Ramsey's objection is. As an attempt to give some bite to the objection in the previous section it doesn't seem particularly plausible.

## 6.5 The Probability Calculus {#the-probability-calculus-1}

> For now it is easily seen that if partial beliefs are consistent they will obey these axioms, but it is utterly obscure why Mr*.* Keynes' mysterious logical relations should obey them. (188-9)

As I noted in the introduction to this chapter, for Keynes the probability relation between two propositions may be some non-numerical value. There are then two related objections that can be made concerning the calculus of these values. The first is that it isn't at all clear what we mean when we add or multiply them. For example, if the probability of *p* given *q* is α, and the probability of ¬*p* given *q* is β, Keynes says we can conclude α + β = 1, but he doesn't say what this might mean. The second objection is that it isn't clear why he should want it to be the case that α + β = 1. I take the results of @sec-chap-3 to provide an answer to this question, though to be fair to Ramsey it is an answer which appears nowhere in Keynes.

So the more important challenge is to the meaningfulness of the mathematical notation in Keynes's theory. If P is the set of reasonable probability functions, we can identify the probability of *p* given *q* with a function from P to \[0, 1\]. In this notation we'll have to identify numbers with constant functions, i.e. identify 1 with the function, call it **1**, such that **1**(*Pr*) = 1 for all *Pr*. Now we have clear concepts of what it is to add and multiply functions, at least with common domains. For functions *f*, *g* and *h*, we can easily say *f* + *g* = *h* iff for all *x*, *f*(*x*) + *g*(*x*) = *h*(*x*). It seems plausible enough to say the '+' in the first equation means the same as the '+' in the second[^88]. Similar definitions can be given for the multiplication and division of functions.

[^88]: Though perhaps ever since Wittgenstein (1953) we ought be a little sceptical about such claims of meaning extension. I hope everyone will agree that I have used '+' in an acceptable way, and if Wittgenstein is right that will mean I have used it acceptably.

Now, with this notation, we can see how it is possible that α + β = **1** (which we are identifying with 1) how some probability relations can be non-numerical (because they are not constant functions), and how this doesn't prevent us applying addition and multiplication operations. Thus I presume the challenge as to the meaningfulness of Keynes's algebra is met, and its justification is given by @sec-chap-3.

## 6.6 The Principle of Indifference {#the-principle-of-indifference}

> Secondly, the Principle of Indifference can now be altogether dispensed with; we do not regard it as belonging to formal logic to say what should be a man's expectations of drawing a white or a black ball from an urn; his original expectations may within the limits of consistency be any he likes; all we have to point out is that if he has certain expectations he is bound in consistency to have certain others. This is simply bringing probability into line with ordinary formal logic, which does not criticise premises but merely declares that certain conclusions are the only ones consistent with them. (189)

This objection isn't applicable to my theory, but it is worth considering as an objection to logical analyses of probability. A logical analysis of probability has to elevate the Principle of Indifference to a logical theorem. This might be thought problematic because of the various paradoxes of indifference discussed in 5.8.1. Even if I can avoid the paradoxes somehow, and as I noted there allowing probabilities to be vague seems to do the trick, there is a lasting impression that the ideal solution would have been to not allow the problems to arise. So I suspect the intuition Ramsey has, that if we can do away with the Principle we ought, is just the right one to have.

It may be possible to develop a precise logical analysis of probability which is immune to all of the paradoxes, but given the calibre of the theorists who have tried and failed, it seems doubtful. It would be more plausible to think an imprecise logical analysis which avoided the paradoxes could be developed. The two grounds I have for thinking this won't be done are, again, that we would expect if it can be done it would have been done already, and that there seems to be a fairly mechanical procedure for constructing objections to any approach.

As it turns out, the formal logic of probability defended here is just the same as that Ramsey defended, with the only possible exception being that we might regard the Principal Principle as a logical rule. Nevertheless, when we say someone has made a logical error in their allocation of degrees of belief, we are just saying that their beliefs are inconsistent. In Ramsey's terms, the person making an error has failed to do something they are 'bound in consistency' to do. As Carnap points out (1950: 337), the formal component of all theories prior to his shared a common logic. So even though Keynes had argued that there were logical restrictions on what degrees of belief people could reasonably have in propositions, when he wrote the formal component of his probability logic these restrictions were not incorporated.

There is one infelicity of expression in the Ramsey quote above. When Ramsey says that a man's expectations 'may within the limits of consistency be any he likes', the modal *may* is being used rather oddly. He doesn't mean that having any old expectation would be epistemically acceptable all things considered. Rather he means that any expectation which is consistent is *logically* acceptable. As he goes on to say in his discussion of induction, there are restrictions on what is epistemically acceptable, i.e. on what is reasonable, which are not logical restrictions.

Finally, we should note the oddity in the last sentence of the quote from Ramsey. It might not be part of logic to criticise the premises that various people hold, but it is part of epistemology broadly speaking. All we ought to conclude from what Ramsey says is that the logic of probability should have little to say with regard to criticising individual probabilistic judgements. I agree; the theory defended here does not make *logical* criticisms of agent's whose beliefs are coherent. However, the boundaries of critical epistemology are not the boundaries of logic. This is true in non-probabilistic epistemology, and it is true in probabilistic epistemology. In sum, I agree with Ramsey that logical analyses of probability rely too heavily on a Principle of Indifference; but I disagree with his claim that we have no grounds for ruling as unreasonable any consistent beliefs. It is this last possibility which opens up an necessitarian theory of probability.

## 6.7 Uncertain Evidence {#uncertain-evidence}

> I think I perceive or remember something but am not sure; this would seem to give me some ground for believing it, contrary to Mr Keynes' theory, by which the degree of belief in it which it would be rational for me to have is that given by the probability relation between the proposition in question and the things I know for certain. (190)

There are two responses we can make to this objection. The first is that it doesn't apply on my conception of evidence; the second is that this is a problem for the application of the theory, rather than the theory itself. Ramsey's observation is of undoubted importance for anyone wishing to construct a machine which has reasonable beliefs, or if you doubt machines can have beliefs, functional states which behave like beliefs and would be reasonable were they believed. However, it is unclear why such practical worries should harm the theory under construction here.

On the view endorsed here, the evidence in a probability relation is a set of worlds in all of which I have certain experiences. The worlds can differ from the actual world in any way at all, as long as my experiences are held constant. The upshot of this is that my evidence isn't of the form *There is a blue book on my desk*, but rather *I am observing a blue book on my desk*. Even the latter isn't a precise representation of the evidence, since my experiences might be being caused by an evil-demon, but for practical purposes it is as close as we can get in language. In cases considered by Ramsey, the evidence wouldn't be of the form *It's probable I saw a blue book on my desk last night*. Rather, it would be of the form *I am having a dim memory of seeing a blue book on my desk*. The reasonable degree of belief in the proposition that there was a blue book on my desk might be moderately high on the basis of the evidence, but the evidence is still something that is taken to be certain.

The other response to make here is that the concept of 'reasonableness' I am using requires, at least in its technical aspects, superhuman ability. After all, it is unreasonable on this picture not to realise what can be entailed from what, not an ability many of us possess. Hence the question of how we should incorporate failing memories into the theory is of the same type as the question of how we should incorporate failing inferential processors. That is, not a relevant question at this level of abstraction. Rather it is something that need to be incorporated in less abstract theories. Is it legitimate to brush aside these concerns as, in effect, engineering problems? It is, because when we are designing practical systems to approximate ideally rational systems, we have to know what it is we're approximating. The purpose of our theoretical pursuits, *vis a vis* such projects, is to work out where the goal posts are. Once there is agreement on what the theoretical aims of a practical rational system are, we can assess how well the system achieves those aims. However, without this agreement, we can't evaluate such systems. Given then that our aim is to investigate the ideal, it might be plausible to abstract away from the difficulties such as failure of memory that worry Ramsey.

Kaplan (1996: 36-8) suggests a useful way to understand the demands that theories like this one are making. When we say that, for example, epistemic states ought be consistent, we are not saying that it is a legitimate criticism of a believer that their belief states are inconsistent. However, it is a legitimate criticism of their belief states, something the believer will usually agree with, as they'll attempt to remove inconsistencies brought to their attention. Similarly in the case of memory, it is no legitimate criticism of an agent that their memory is less than perfect, however it might well be a legitimate criticism of some states of that agent. And again the agent will often agree in the sense that they'll say it is best, other things being equal, to remember more of our evidence rather than less. However, here other things are never equal, there are always trade offs to be made, and the agent is only subject to legitimate criticism if in making this trade off they don't take the ideal of perfect evidence retention seriously enough.

## 6.8 A Recent Addition - Dependence on *A Priori* Assumptions {#a-recent-addition---dependence-on-a-priori-assumptions}

Most critics of Carnap's theory of probability have focussed on the technical aspects of his theory. There are a few good reasons for doing this. First, his technical theory is interesting for its own sake. Secondly, if there are major flaws in the particular technical theory (as seems to be the case) this counts against the general approach, both because there is arguably a burden on Carnap and his followers to produce a sound technical theory, and more generally because we might suspect that if Carnap couldn't complete this project it can't be completed. However, it would be nice to see in the literature more discussion of the philosophy behind Carnap's theory, and solid objections to it. As my necessitarian approach adopts Carnap's philosophy, but not his technicalities, I incur a duty to answer those objections directed at that philosophy.

The most serious objection is that Carnap's method is too *a prioristic*. The discussion in Howson and Urbach (1989: 52‑56) seems to be the most substantial presentation of this objection. I say it is necessary, and arguably *a priori*, which functions *Pr* are reasonable. For example, let my evidence proposition be *E*, and let *q* be the proposition that the moon is made of green cheese. Then I say it is necessary that *Pr*(*q* \| *E*) \< 0.9 for all reasonable *Pr*. Note that every *Pr* is, as well as being a conditional probability distribution, an unconditional probability distribution, since we can define *Pr*(*A*) as *Pr*(*A* \| T). Hence I'm committed to there being *a priori* probabilities. Howson and Urbach object.

> For any conditional probability distribution over the sentences of a language necessarily involves the assignment of unconditional probabilities to a partition of the space of possibilities representable within *L*. But what considerations can possibly justify any such *a priori* distribution? (Howson and Urbach 1989: 53‑4)
>
> Any *a priori* probability distribution ... is going to be arbitrary. For this reason we do not regard people who try to evaulate the probabilities of hypotheses relative to data as doing exercises in a genuine logic of generalised deduction, for we take logic to be essentially noncommittal on matters of fact. (Howson and Urbach 1989: 55)

Howson and Urbach back up these claims by pointing out, rightly, that considerations of symmetry or simplicity will not give us the *a priori* distributions. Any time we try to make the distributions symmetrical or simple relative to one set of considerations we will make them more skewed and more complex relative to others. I accept this, but not the conclusion they draw from it.

Three relatively inessential points before we start. First, in my story (unlike Carnap's), probabilities are assigned to pairs of propositions, not pairs of sentences, and propositions are just sets of possibile worlds. These 'possibilities' are in general not representable within any language. This I take it is no response at all to their objection. Secondly, I *could* avoid making my theory *a prioristic* if I let the reasonable probability functions be whatever play a certain role in the actual world. That is, I could consistently with my theory say that which probability distributions are reasonable is necessary *a posteriori*. As I noted in section 4.4, the relevant simplicity and symmetry considerations might be given by our actual practices. However, that approach has problems, and to adopt it just to avoid a charge of *a priorism* would be untenable. In any case, Howson and Urbach could rewrite their objection to deal with this. So I'll write here as if necessary and *a priori* were interchangable.

Thirdly, and this is a bit important, it isn't true on my theory that a conditional probability distribution 'necessarily involves the assignment of unconditional probabilities' *a priori*. Let an epistemic state be represented by the set {*Pr*: *Pr*(*q* \| *E*) = 0.2}. One conditional probability is quite precisely defined here, but no unconditional probabilities of contingent propositions are defined to be sharper than \[0, 1\]. I suspect the reference to unconditional probabilities was more a rhetorical flourish than a crucial part of the argument, which is why I think this point is mostly unimportant.

Those clarifications aside, I can proceed. I trust the reader agrees that having degree of belief 0.9 or higher in *q* on evidence *E* is unreasonable. If not, please reconsider. If so, read on. Following Lewis (1980) I have defined reasonable probability functions to be those which licence no unreasonable degrees of belief. So we can conclude that all reasonable probability functions *Pr* are such that *Pr*(*q* \| *E*) \< 0.9, because this just means that having degree of belief 0.9 or higher in *q* on evidence *E* is unreasonable. Call this conclusion *F*. Question: Is *F* empirical or *a priori*?

If *F* is *a priori*, then I have a response to Howson and Urbach, for I can say that what determines the set of reasonable probability functions is just the set of *a priori* facts like *F*. Note that their claim that logic is noncommittal on matters of fact is clearly mistaken unless we say *a priori* truths are not 'matters of fact'. This is perhaps a non-standard use of fact, but not an unintelligible one. However, on it, *F* does not turn out to be a matter of fact, so my 'logic' is noncommittal on matters of fact.

Hence for their objection to work, *F* must be empirical. But what evidence could there possibly be for *F*? Or perhaps more strikingly, what evidence could there be for ¬*F*? What is one meant to say about my counterpart who has had exactly the same experiences as I, but believes to degree 0.9 that the moon is made of green cheese, and does so reasonably. I very much doubt I have such a counterpart. This does not show that *F* is not empirical, it might just show that *E* entails *F*. If that were true, it might be the case that *F* is empirical but nevertheless I could have no such counterpart. The problem with that is that it is inconsistent with the conclusions about updating I derived in @sec-chap-3. That is, it is inconsistent with my insistence on Conditionalisation. In fact, it is inconsistent with virtually any interpretation of conditional probability, as this example shows.

Let *E*~1~ be strictly weaker than *E*. That is, *E* entails *E*~1~ but not *vice versa*. And let *E*~1~ be such that it doesn't entail *F*. Now say I have a counterpart whose evidence is *E*~1~. Perhaps it would be reasonable for him to believe *q* to degree 0.9. Since he doesn't know *F*, his evidence is insufficient to support it, and for all I've said *F* might be false in his world. So, for all he knows, it is reasonable to have probability functions such that *Pr*(*q* \| *E*) = 0.9 in the representation of his epistemic state. In fact, if I'm right and any precisification of a reasonable epistemic state is reasonable, he could reasonably have his degrees of belief represented by just that function. But then were he to learn *E*, there would be nothing he could reasonably do. By conditionalisation, he would have to have degree of belief 0.9 in *q*. However, he would now know *F*, so he would know that his representative probability function was unreasonable. Hence, he would know that the only thing he can reasonably do -- i.e. conditionalise -- is unreasonable. So his epistemic state provides no consistent guidance in a possibility he envisages as possible. But, by definition, no reasonable state does this. Hence, his state was unreasonable to start with; so, having a function *Pr*(*q* \| *E*) in one's representor is always unreasonable. We can rephrase all this without the assumption that precisifications of reasonable degrees are reasonable, but it's less clear. The point again is that, when my counterpart conditionalises on evidence *E*, he won't be able to completely precisify.

So my response to Howson and Urbach is in two parts. First, it is obvious we do believe in *F* and like claims. Secondly, it is implausible to say that *F* is empirical. So, that necessitarian theories are committed to *a priori* assumptions like *F* is no mark against them, and may in fact be a benefit.

