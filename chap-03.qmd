# What Degrees of Belief Are {#sec-chap-3}

## The Equivalence Analysis {#sec-0301}

In @sec-chap-2 I noted that I would take qualitative data about an agent's degrees of belief as given. So I have as data to work with sentences of the form *The agent's degree of belief in A is higher than in B*. Standardly in the literature, the agent is called You, with the capitalisation meant to indicate that *You* is being used as a name. Hence the possessive form of You, as used, is You's, but it is standard to use Your. Rather than mimic this faulty grammar I'll use a variety of agents as appropriate.

In his 1926 paper, Ramsey argued that there was no way to convert qualitative judgements of greater or smaller degrees of belief into quantitative judgements of, say, degree of belief 2/3. However, there is a relatively simple way to do this, as he pointed out in a note in 1929. To say my degree of belief in *A* is 2/3 is to say I have the same degree of belief in it as I have in *p*~1~ ∨ *p*~2~ when I know that exactly one of *p*~1~,  *p*~2~ and *p*~3~ is true and each of the *p*~i~ are equally likely. From the qualitative judgements that *A* has the same degree of belief as *p*~1~ ∨ *p*~2~ and that all *p*~i~ have the same degree of belief, we can work out a quantitative judgement.

It is rather trivial to generalise this. My degree of belief in *A* is *x* / *y* when it is the same as my degree of belief in *p*~1~ ∨ ... ∨ *p~x~* given that I believe fully that exactly one of *p*~1~, ...,  *p~y~* is true and each *p*~i~ is equally likely. By equally likely, I just mean that my degree of belief in *p*~i~ equals my degree of belief in *p*~j~ for all i, j. We will look in @sec-0310 at how this analysis might be extended to real-valued degrees of belief, but until then we'll assume that degrees of belief take rational-values only. (Note that in this chapter 'real' and 'rational' always refer to properties of numbers. Saying that a degree of belief is real doesn't entail that anyone has it, nor does saying it is rational entail that anyone should have it.) I'll call this analysis of degrees of belief, that to believe *A* to a certain degree is to believe it to the same degree as a certain disjunction, the *Equivalence Analysis*.

It might seem that the Equivalence Analysis is circular, since we have analysed degrees of belief in terms of, *inter alia*, degrees of belief. This objection misses the point somewhat. The aim of the Equivalence Analysis is to reduce quantitative degrees of belief to qualitative degrees of belief. If we were to write it as a precise definition (or more exactly a set of definitions) we would find that on one side we have quantitative sentences and on the other we have only qualitative sentences. Indeed the only qualitative relation we have used is equality of degrees of belief; we haven't even used inequalities. When we extend the analysis to real-valued or, on one account, vague degrees of belief, we will need this extra resource.

As I noted above, the idea of defining degrees of belief in this way originates with @ramsey1929b. The first writer to use something like the Equivalence Analysis in a formal theory was @koopman1940a. He argued that probability was primarily a comparative notion; if there are exclusive and exhaustive propositions *p*~1~, ..., *p~x~*, ..., *p~y~* each equally likely such that *A* is as likely as *p*~1~ ∨ ... ∨ *p~x~* then *A*'s probability is *x* / *y*, otherwise *A* doesn't have a numerical probability. To use Koopman's term, in the latter case *A* is not appraisable. In sections [-@sec-0305] to [-@sec-0309] we will look at ways of introducing non-appraisable propositions into the theory. @good1950a uses Koopman's ideas as motivation for the idea that degrees of belief ought obey the probability calculus[^22]. The theory here uses ideas from all of these writers. The components which are, to my knowledge, original are the use of what I will call models in the third section, and the use of material equivalences at the core of the analysis. It is the last fact which prompted the name *Equivalence Analysis*.

[^22]: A similar approach is used in @savage1954a. However, he uses 'almost uniform partitions' which are such that any disjunction of *x* + 1 elements is more probable than any disjunction of *x* elements for any *x* ∈ {1, ..., *y* - 1}. The motivation for this is that it makes it more plausible that the *p*~i~ can be propositions about real events, rather than dummy propositions as in the theory presented here. Although he starts, like Koopman, with comparative probability, he assumes that for any *A*, *B* either *Pr* (*A*) ≥ *Pr*(*B*) or *Pr*(*B*) ≥ *Pr*(*A*). This entails that all propositions are appraisable.

If it helps we can visualise matters by thinking of the various *p*~i~ as being the drawing of the i'th ball from an urn containing *y* balls, but I'm not sure this helps. In urn cases there is often a temptation to think that the *p*~i~ are equally likely because we are ignorant of the way the balls are distributed in the urn. This may be approximately correct in some practical cases, but it seems wrong in general. I am doubtful that we must, or even may, derive precise numerical degrees of belief through ignorance. I certainly don't want to have an appeal to the Principle of Indifference at the core of my definition of degrees of belief. Rather the kind of case I am thinking of is one where our evidence is sufficient to have an equal degree of belief in each *p*~i~. It doesn't take much of a sceptical attitude to deny that physical evidence can ever provide us with this in real world examples. Nevertheless, the situation seems a useful fiction, particularly because in practice our degree of belief in certain types of events (e.g. lotteries) do *approximate* this ideal of equal degrees of belief in all outcomes.

## Outline of Chapter {#sec-0302}

In @sec-0303 I will introduce formal models of probabilistic belief. These are important because they allow us to regain the conclusions of Dutch Book arguments. That is, on the assumption that all an agent's degrees of belief are rational numbers, then it is a coherence requirement that their degrees of belief obey the axioms of the probability calculus. Models are simply sets of propositions closed under entailment, however they are defined on a different possibility space. The core idea behind the models is that if in reality the agent believes *A* to degree *x* / *y*, some proposition of the form *A*  *p*~i~ ∨ ... ∨ *p*~j~ is true in the model, where there are *x* disjuncts on the right-hand side. The axioms of the probability calculus fall out as consistency requirements on the model. In @sec-0304 we extend this to updating methods for probabilistic beliefs, showing that the Bayesian requirements of Conditionalisation and Reflection can be justified by these models. In that section I also respond to some criticisms of these principles.

There are two complications that can be made. I have assumed in the initial sections that degrees of belief are rational and precise, but in general neither of these restrictions is permissible. In sections [-@sec-0305] to [-@sec-0309] I look at various ways of dropping the restriction that degrees of belief are precise. The most common way in the literature to do this is to have a person's degree of belief represented by sets of probability functions rather than a single function. @sec-0305 looks briefly at the motivations for imprecision and outlines this approach to representing it. I will then consider two alternatives to this approach.

In @sec-0306 I look at a simple alteration of our conditions on models to permit imprecision. In the standard model, when the agent's degree of belief in *A* is *x* / *y*, there is a disjunction *D* of *x* elements such that *A*  *D* is in the model. In imprecise models, when the agent's degree of belief in *A* is vague over the interval between *x*~1~ / *y* and *x*~2~ / *y* there are disjunctions *D*~1~ and *D*~2~ of *x*~1~ and *x*~2~ elements respectively such that *D*~1~ ⊃ *A* ⊃ *D*~2~ is in the model. It is shown that the resultant theory is equivalent, in a certain sense, to Shafer's theory of belief functions. More importantly the resultant axioms on coherent degrees of belief are *more* restrictive than the approach of @sec-0305.

Above I noted that we could think of the *p*~i~ as representing the drawing of a given ball from an urn with *y* balls in it. One way to loosen some of the restrictions on degrees of belief incurred in @sec-0306 is to allow there to be multiple urns, not necessarily independent. This approach is analysed in @sec-0307. This move doesn't change our earlier conclusions for precise degrees of belief, but it does allow us to drop some of the unwanted restrictions. Although the resultant theory satisfies some proposed axiomatisations of vague probability theory, I will argue that it loses too much structure, and that some of the theorems it fails to prove are ones we ought want. So the only two live options are the family of probability functions approach, outlined in @sec-0305, and the Shafer belief functions approach, outlined in @sec-0306.

Shafer proposes that his belief functions should not be updated by conditionalisation. This is, I suggest, a mistake, and I'll look at some examples designed to reinforce that belief. Some writers have suggested that this mistake can be remedied. We noted that Shafer belief functions are equivalent to families of probability functions with two particular properties. Although one of these properties is preserved under conditionalisation, the other is not, so it seems there is no coherent way to update Shafer functions.

In @sec-0309 I consider Walley's argument that analyses of vague degrees of belief in terms of families of probability functions is bound to give the wrong answer to certain conditionalisation problems. He argues that adopting a certain principle, *conglomerability*, leads to thinking that vague previsions are basic, not probability functions. When applied to general cases the principle of conglomerability is inconsistent, which somewhat vitiates its force when applied to the special case Walley considers.

Finally in @sec-0310 I look at extending the analysis of this chapter to real-valued degrees of belief. I say that *Bel*(*A*) = *r* iff *Bel*(*A*) is greater than (less than, equal to) *y* / *z* for integer *y* and *z* whenever *y* / *z* is less than (greater than, equal to) *r*. This way talk of real-valued degrees of belief is eliminated in favour of comparisons between degrees of belief and rationals. An appendix contains some proofs which are left out of the body of the text to ease exposition.

## Precise Models {#sec-0303}

I'll again adopt the notation *Bel*(*A*) for the agent's degree of belief, or credence, in *A*. The aim of this section is to show that, at least for the special case when *Bel* takes only rational values, the agent is unreasonable if *Bel* is not a probability function. As noted in @sec-chap-1, by definition *Pr* is a finitely-additive probability function iff it is a function from propositions to numbers satisfying:

(Pr1) 
:    *Pr*(*A*) ≥ 0;

(Pr2) 
:    *Pr*(T) = 1;

(Pr3) 
:    If  ¬(*A* & *B*) then *Pr*(*A* ∨ *B*) = *Pr*(*A*) + *Pr*(*B*).

If an agent believes, in the traditional sense, *A*, then they have the same credence in *A* as they would have in *p*~1~ were they to believe exactly one member of the set {*p*~1~} is true. Hence their credence in *A* will be 1/1, that is, 1. If they believe ¬*A* then there will be no value *y* such that they have the same credence in *A* as in *p*~1~ and exactly one member of {*p*~1~, ..., *p~y~*} is true. Hence for all *y*, *Bel*(*A*) \< 1/*y*. So *Bel*(*A*) = 0.

Since we are discussing what agents should believe, or what reason requires them to believe, we need to define what concept of reasonableness we have in mind. For these purposes I will simply adopt the standard used in Dutch Book arguments. An agent is reasonable iff their beliefs are closed under entailment and not trivial. This is simply a coherence constraint on reasonableness. For consistency with the literature, I'll use reasonable in this chapter simply to mean probabilistically consistent, or what we might call coherent[^23]. This is not meant to imply that all coherent belief sets are reasonable in some strong sense. Nor is it meant to imply that incoherence is unreasonable in an everyday sense.

[^23]: We'll come across a different use of *coherent* later in connection with vague belief functions.

Above I said that *Bel*(*A*) = *x* / *y* meant simply *Bel*(*A*) = *Bel*(*p*~1~ ∨ ... ∨ *p~x~*) where {*p*~1~, ..., *p~y~*} is a set of propositions such that the agent knows one of them is true and for any i, j *Bel*(*p*~i~) = *Bel*(*p*~j~). Put this another way, I could say that *Bel*(*A*) = *x* / *y* means that the agent has the same credence in *A* as they would have were they to believe (i.e. believe fully) *A*  *p*~i~ ∨ ... ∨ *p*~j~, where there are *x* disjuncts and {*p*~1~, ..., *p~y~*} is defined the same way.

It is clearly no constraint on rationality that if *Bel*(*A*) = *x* / *y* we can find *y* equiprobable alternatives such that we believe *A* is materially equivalent to a disjunction of *x* of these. However, it does seem to be a constraint that we should be able to consistently believe something of this sort. If it were inconsistent to believe that there was any such set as {*p*~1~, ..., *p~y~*} and *A* is materially equivalent to a disjunction of *x* elements of this set, something seems amiss.

Indeed, an even stronger constraint than this seems in order. Assume there exists a *y* such that for all propositions *A* in a finite field of propositions Γ, *y* · *Bel*(*B*) is an integer. Provided our degree of belief in any proposition in Γ is rational this will be possible for some *y*. Again assume {*p*~1~, ..., *p~y~*} is a set of propositions such that we know exactly one is true and all are equiprobable, and that no proposition about any *p*~i~ is in Γ. Then it seems to be a rationality constraint that it should be possible for any *B* in Γ, where *Bel*(*B*) = *x / y* to find a disjunction of *x* elements of {*p*~1~, ..., *p~y~*} such that it is consistent to believe *B* is materially equivalent to that disjunction. That is, it should be possible to model our probabilistic beliefs about propositions in Γ on {*p*~1~, ..., *p~y~*}. Since what it means to say *Bel*(*B*) = *x / y* is to say *B* is believed to the same degree as such a disjunction, it would be odd if it were inconsistent to say that *B* is materially equivalent to any of them.

The above is fairly informal, particularly the requirement that no proposition 'about' the *p*~i~ be in Γ. The following is a more formalised statement of it, followed by proofs that these restrictions are sufficient to show why the degrees of belief should follow the probability calculus.

Assume an agent has a certain set of beliefs, say **K**, and certain degrees of belief *Bel*(•). I want to test for coherence her credences about a certain set Γ of propositions, assuming that all these are rational numbers. A simple coherence constraint, which was argued for above[^24], is that if *A* ∈ **K** then *Bel*(*A*) = 1. Let *y* be the lowest common denominator of these degrees. Since we are considering her propositional beliefs we can represent each belief as a set of possible worlds, or more generally as a subset of the possibility space. Let *W* be the set of all worlds in the original possibility space. Since **K** is closed, it determines, and is determined by, some subset ∆ of *W*. For any proposition *A*, *A* ∈ **K** iff ∆ is a subset of *A*. The non-triviality constraint on reasonable belief sets is simply that ∆ is non-empty.

[^24]: And see also @sec-0404 for consideration of some recent objections to this proposal.

Let *P* be the set {*p*~1~, ..., *p~y~*}. I won't presume these are propositions in the sense of being subsets of *W*. Rather, to allow the agent to consider *P*, I need to extend the possibility space from *W* to *W* × *P*. Elements of *W* are possible worlds, e.g. *w*. Elements of *W* × *P* are ordered pairs, the first element of which is a possible world, and the second element a member of *P*. Because the *p*~i~ are not meant to be about the propositions in Γ, it can be assumed that all points in *W* × *P* are real possibilities. The proposition *A* on *W* will be the proposition {\<*w*, *q*\>: *w* ∈ *A* & *q* ∈ *P*} on *W* × *P*. We'll write the latter proposition as *A*^\*^, and in general use *A*, *B*, *D* for propositions on *W*, *A*^\*^, *B*^\*^, *D*^\*^ for the equivalent propositions on *W* × *P*. The proposition *p*~i~ in *P* will be the proposition {\<*w*, *q*\>: *w* ∈ *W* & *q* = *p*~i~} on *W* × *P*. I'll write that as *p*~i~^\*^. I define conjunction, disjunction and negation of propositions in *W* × *P* in the usual way as intersection, union and complementation.

The aim is then to see if the agent's probabilistic beliefs about Γ can be modelled as propositional beliefs about subsets of *W* × *P*. In the model the set of propositions on *W* × *P* which the agent believes is **K**^\*^, which again to satisfy the reasonableness constraint should be closed and non-empty. Hence **K**^\*^ determines, and is determined by, a subset ∆^\*^ of *W* × *P*. The model should satisfy the following constraints.

> \(1\) (*S* ⊆ *P* & *S*^\*^ ∈ **K**^\*^) → *S* = *P*    
> \(2\) If *Bel*(*A*) = *x* / *y* then ∃*S*: ((*S* ⊆ *P* & \|*S*\| = *x*) & (*S*^\*^  *A*^\*^ ∈ **K**^\*^))

In (2) and in what follows I will assume sets of propositions are truth-valued, and I'll simply stipulate that a set of propositions is true iff some element of it is true, false otherwise. This greatly eases the exposition in what follows. Again a proposition unstarred is either on *W* or *P*, and starred is the equivalent proposition on *W* × *P*. (2) says that **K**^\*^ is a model for *Bel* in the sense that if *A* is believed to degree *x* / *y* then it is equivalent in **K**^\*^ to a disjunction of *x* propositions such that the agent knows exactly one of *y* of these is true. (The expression \|*S*\| in (2) refers to the cardinality of *S*). (1) says that the agent doesn't know that some subset of *P* is true. If this were to occur it would of course be inconsistent with the assumption that each of the *p*~i~ is equiprobable. That assumption isn't used anywhere else in the proofs.

We now prove the following lemmas:

> (L1) ⊻(*p*~1~^\*^, ..., *p~y~*^\*^) ∈ **K**^\*^    
> (L2) (*S* ⊆ *P* & ¬*S*^\*^ ∈ **K**^\*^) → *S* = ∅

The notation ⊻ in (L1) refers to exclusive disjunction. There is a difficulty with representing *n*-place exclusive disjunction, since if we just took *A* ⊻ *B* =~df~ (*A* ∨ *B*) & ¬(*A* & *B*), we would have the result that (*p*~1~ ⊻ *p*~2~) ⊻ *p*~3~ would be true iff one or three of the *p*~i~ were true. So I allow exclusive disjunction to be an *n*‑ary connective, written ⊻(*p*~1~, ..., *p~n~*), which is true provided exactly one of the *p*~i~ is true[^25].

[^25]: @mccauley1993a defines all of the connectives as *n*-ary, partially to have a greater parallel between formal and natural language and partially to resolve this problem with exclusive disjunction.

An exclusive disjunction is true is true iff the disjunction of every element is true and the conjunction of any distinct pair of disjuncts is false. For any two propositions *p*~i~^\*^, *p*~j~^\*^, it follows from the definition of *W* × *P* that *p*~i~^\*^ ∩ *p*~j~^\*^ is empty. Since ∆^\*^ is not empty, it follows that ∆^\*^ ⊄ (*p*~i~^\*^ ∩ *p*~j~^\*^). Hence *p*~i~^\*^ & *p*~j~^\*^ ∉ **K**^\*^. Again from the definition of the *p*~i~^\*^, it follows that *p*~1~^\*^ ∪ ... ∪ *p~y~*^\*^ = *W* × *P*. Hence ∆^\*^ ⊆ (*p*~1~^\*^ ∨ ... ∨ *p~y~*^\*^), so *p*~1~^\*^ ∨ ... ∨ *p~y~*^\*^ ∈ **K**^\*^. This proves (L1).

(L3) follows directly from (L2), since if *S* ⊆ *P*, then ¬*S* will be *P* / *S*, which is also a subset of *P*. Since ¬*S*^\*^ ∈ **K**^\*^, by (L2) ¬*S* = *P*, so *S* must be empty.

These rules are sufficient to prove that the following are constraints on reasonable degrees of belief for any propositions *A*, *B* in Γ. (The proofs are in the appendix to this chapter)

::: {#thm-belmod}

## Constraints on Bel

If *Bel* can be modelled by **K**^\*^ satisfying (1) and (2) above and is defined for all propositions in Γ, then for all *A*, *B* in Γ:

> (T1) If *A* then *Bel*(*A*) = 1  
> (T2) If ¬*A* then *Bel*(*A*) = 0  
> (T3) *Bel*(*A*) + *Bel*(*B*) = *Bel*(*A* ∨ *B*) + *Bel*(*A* & *B*)  
> (T4) If *A*  *B* then *Bel*(*A*) ≤ *Bel*(*B*)  
> (T5) 0 ≤ *Bel*(*A*) ≤ 1  
> (T6) If *Bel*(*A*) = *x / y* and *Bel*(*A*) = *z* / *y* then *x* = *z*
:::

If we can prove (T1) to (T6) then it follows that *Bel* is reasonable *vis a vis* Γ iff *Bel* obeys the axioms of the probability calculus with respect to those propositions. Generally *Bel* is reasonable (or at least coherent) if it is reasonable *vis a vis* any finite set Γ of propositions.

Now it simply falls to us to show that these requirements ensure that *Bel* is a probability relation. By (T6) *Bel* is a function from propositions to numbers (i.e. it is uniquely valued). By (T5) it satisfies (Pr1), by (T1) (and (T6)) it satisfies (Pr2) and by (T2) and (T3) it satisfies (Pr3). Hence it is a probability function.

## Updating Precise Models {#sec-0304}

Dutch Book arguments have been presented to show that we should obey certain dynamic principles, such as Reflection and Conditionalisation. These are commonly referred to as Dutch Strategy arguments, and these seem most at risk from my arguments in section 2.3. We should look at whether we can reach the same conclusions as they do using the Equivalence analysis of degrees of belief. To do this I'll first look at what similar results to Reflection and Conditionalisation can be found concerning absolute beliefs.

Again the only norms I am adopting here are norms of dynamic coherence. I take it that if there is no possible world in which an agent can believe truly a set of propositions **K** then **K** is not a statically coherent belief set. However, since we are studying epistemic dynamics we have to extend this rule. So I adopt as a coherence constraint that holding a belief such that my holding that belief entails I have *or will have* a false belief shows that I am unreasonable. There are also closure constraints on rationality. Assume that ∆ is the set of all worlds in which we have all the current beliefs we actually have and all our current and future beliefs are true. The first coherence requirement is that ∆ be non-empty. As above, I will also insist that reasonableness requires that for every proposition *A* true in every world in ∆, we believe *A*.

Assume I believe that tomorrow I'll have a false belief. ('Tomorrow' refers simply to an arbitrary future time). If this is true I will, perforce, have a false belief. Alternatively, this belief will be false, so again I will have a false belief, this one. Hence I am guarunteed to have a false belief, so by the coherence constraint I am incoherent, or unreasonable. Hence it is unreasonable to believe that tomorrow I'll have a false belief. Similar constraints apply to the belief that I now believe something false. It might be thought that this implies closure is not only unnecessary for reasonableness, it is incompatible with it. However, we get more plausible results when we restrict our attention to particular beliefs.

Assume I believe *A* and believe that tomorrow I'll believe ¬*A*. Since it can't be that *A* and ¬*A*, it can't be that my beliefs on each day are true, so one of them must be false. So I must either have a false belief, and hence be unreasonable, or believe I'll have one, which is unreasonable for the reasons we noted in the previous paragraph. Hence again I am unreasonable. Alternatively, assume simply that I believe that tomorrow I'll believe *A*. Then in all worlds in ∆ tomorrow I truly believe *A*, hence all worlds in ∆ are *A*‑worlds. So by closure I should believe *A*.

The argument for Reflection is just the probabilistic version of that argument. Reflection is the principle that if I believe I will have credence *r* in *A* I must now have credence *r* in *A*. Assume that today my credence in *A* is *x / y*, and I believe that tomorrow it will be *z / y*, where *z*  *x*. If tomorrow my credence isn't *z / y*, I currently have a false belief, so tomorrow my credence must be *z / y*. (Note that by 'it must be that *B*' I just mean all worlds in ∆ are *B*-worlds). Now our coherence requirements on credences mean that for any proposition *B* which we believe to degree *v* / *y*, we can consistently add to our belief set that *B*  *p*~1~ ∨ ... ∨ *p~v~*. (Where there is little possibility of confusion I drop the ^\*^ notation.) If we are assessing our beliefs about more than one proposition, the only plausible requirement is that we could add *B*  *S* for some *S* ⊆ *P* with \|*S*\| = *v*. But when just one proposition is being considered this stronger requirement looks plausible enough.

Now, our assumptions lead us to believe that today I believe *A*  *p*~1~ ∨ ... ∨ *p~x~*, and tomorrow I will believe *A*  *p*~1~ ∨ ... ∨ *p~z~*. Since all my beliefs will be true, this means it must be that *p*~1~ ∨ ... ∨ *p~x~*  *p*~1~ ∨ ... ∨ *p~z~*. If *x* \> *z*, this means it must be that ¬(*p~z~*~+1~ ∨ ... ∨ *p~x~*). Hence by the closure requirement I should believe ¬(*p~z~*~+1~ ∨ ... ∨ *p~x~*). But (L3) above still applies, showing that I should only believe the negation of a disjunction of elements of *P* if that disjunction had zero-elements. So it can't be that *x* \> *z*. Similarly if *z* \> *x* we would end up being committed to ¬(*p~x~*~+1~ ∨ ... ∨ *p~z~*), which again is unreasonable by (L3). So reason requires that *x* = *z*. That is, if we have any beliefs about our future credence in *A*, we should believe we will believe it to just the degree we currently do.

Two comments on this proof. First, I used the strong requirement that if our degree of belief in *A* is *x / y* we should be able to consistently add *A*  *p*~1~ ∨ ... ∨ *p~x~*, rather than the weaker requirement that we be able to add *A*  *S* for some suitable *S*. If we adopt just that requirement the proof doesn't go through. For it doesn't follow that if adding *A*  *S* implies that tomorrow we will believe of some *p*~i~ or other that it is false that we are irrational. To see this, let *A* be the proposition that the coin I am about to toss will land heads. Now my credence in *A* is 1/2. That is, I could add *A*  *p*~1~, with *y* = 2. However, I believe that tomorrow I'll either believe *A* or disbelieve *A*. So if I had added *A*  *p*~1~ I believe that tomorrow I'd either believe ¬*p*~1~ or ¬*p*~2~. Since this isn't irrational, that I will come to disbelieve some *p*~i~ or other can't be irrational, and we can't plausibly strengthen our coherence constraints to make it so.

The other comment is that people have often misinterpreted coherence constraints. It might be perfectly reasonable, given that I am confident I won't be completely coherent tomorrow, to believe I will falsely believe *A*. The Reflection criteria are necessary but insufficient criteria for full rationality. There is no reason to suspect, and certainly nothing in the above proofs, to justify the idea that Reflective agents are *ceteris paribus* more reasonable than those who are not. Compare the following example. Only students who don't answer an even number of questions on the exam will get a perfect result. It doesn't follow from this that students who don't answer an even number of questions are *ceteris paribus* better students than those who make an odd number of mistakes. Nor does it follow that a student, noting she has failed to answer an odd number of questions but unable to make any more progress, should take steps to ensure she leaves out an even number of questions, particularly if this involves leaving out a good answer. This doesn't invalidate the original claim that not answering an even number of questions is a necessary condition of perfection. Nor does the fact that taking steps to be Reflective would be positively irrational in some circumstances invalidate the claim that Reflection is a criterion of full rationality.

The other constraint which is usually thought to be proven by Dutch Strategy arguments is Conditionalisation. As van Fraassen has pointed out, in its standard form this argument is noticeably weaker than other Dutch Book arguments; indeed considerably weaker than van Fraassen's own argument for Reflection [@vanfraassen1989a 173-6][^26]. Assume that my current degree of belief in *A* given *B* is 0.2, but if I were to discover *B* (and presumably nothing else) my credence in *A* would be 0.3. However, I do not know this, I believe falsely that I would be a good Bayesian and conditionalise. Then someone who knows all this can offer me bets that will lead to me losing money. But this doesn't prove any incoherence. After all, assume my credence in *A* is 0.2, but *A* is false. Then someone can offer me a bet on *A* for 10 cents, which I'll buy and lose. The fact that I'm prepared to take losing bets doesn't show irrationality if it requires more information than I have to show that the bets are losing ones. So the Dutch Book argument to show that not being a conditionaliser is irrational relies on the presumption that I know, or at least ought know, what my degree of belief in a given proposition would be under any circumstances. I clearly don't have this information, and I don't see any particular reason why I should be required to do so. Hence the traditional Dutch Book argument fails, because the Dutch Bookie can only make a book if she has more knowledge than I either have or am epistemically obligated to have, and I'm only irrational if she can make a book without more knowledge than I have or ought have.

[^26]: An argument against requiring conditionalisation based around this fact is also found in @howson1993a.

van Fraassen showed that we could revive the Dutch Book argument against a person intending to follow a strategy other than Conditionalisation. Like all dynamic Dutch Book arguments, his appears to be faulty for the general reasons I have gone through; however it is at least acceptable by its own standards. He concluded that it could be rational to not conditionalise, as long as one did it capriciously rather than in accord with a pre-ordained strategy. This conclusion has been questioned by some who thought it paradoxical that a course of action could be irrational if carried out deliberately, but rational if carried out capriciously. There is certainly a ring of paradox about it![^27]

[^27]: See, for example, @green1994a [321].

My arguments for Conditionalisation do not rely on Dutch Book considerations. Rather, they rely on the Equivalence Analysis of degrees of belief and the models used to show that degrees of belief ought obey the axioms of the probability calculus. Before we start on them however, we need to consider briefly the idea of modifying standard belief sets. There is a large literature on this, most of which is concerned with the difficult problem of how to retract beliefs. I only want to consider a simple problem, how to add a propositional belief to a coherent set of propositional beliefs.

As we noted above, if an agent has propositional beliefs **K** and **K** is closed under entailment, then **K** will determine and be determined by a set ∆ of possible worlds. There seems to be something of a consensus that the way to add a proposition, say *B*, to a set of beliefs **K**, is for the new belief set to be the closure of **K** ∪ {*B*}. Or in other words, for the new belief set to be that set determined by the set of worlds ∆ ∩ *B*. I will adopt this conclusion in what follows. There could be a slight complication if **K** contains ¬*B*. Since this would involve difficult questions about revision of belief sets, an issue on which there is no agreement amongst theorists, I'll assume ¬*B* is not in **K** and more generally that *Bel*(*B*) \> 0.

Assume *Bel*(*A* & *B*) = *x* / *y* and *Bel*(*B*) = *z* / *y*, where *z* \> 0. I have to prove that when we add *B* to **K** the only rational new value of *Bel*(*A*) is *x* / *z*. If *Bel* and **K** are rational then there is some **K**^\*^ as outlined above containing *S*^\*^  *B*^\*^ for some *S* such that \|*S*\| = *z*. Although there is no requirement that *S* be {*p*~1~, ..., *p~z~*} we can rename the *p*~i~ such that this is the case. After all, there is no distinction between the various elements of *P*. For convenience, I'll call the set {*p*~1~, ..., *p~z~*} *S~z~*. There will also be a set *S* such that (*A* & *B*) ^\*^  *S*^\*^ is in **K**^\*^, \|*S*\| = *x* and *S* ⊆ {*p*~1~, ..., *p~z~*}.

Now **K**^\*^ is a propositional belief set. So we can see what beliefs that agent should have if they started with that set and added *B*, or equivalently, added *B*^\*^. The new set, which we'll name **K**~*B*~^\*^, is simply the closure of **K**^\*^ ∪ {*B*}. Since **K**^\*^ contains *B*^\*^  *S~z~*^\*^ and *B*, it contains *S~z~*^\*^. Let the new set of beliefs and degrees of belief the agent has after coming to believe *B* be called **K**~*B*~ and *Bel~B~* respectively. Since **K**~*B*~^\*^ contains ⊻(*p*~1~, ..., *p~z~*), if for any proposition *D*, there is a set *S* such that *S* ⊆ *S~z~* and **K**~*B*~^\*^ contains *D*  *S*, then *Bel~B~*(*D*) = \|*S*\| / *z*. If the agent is reasonable then by (T6) any way of calculating *Bel~B~*(*D*) will give the same answer.

Now we know that there is some *S* such that \|*S*\| = *x* and **K**^\*^ contains *S*^\*^  (*A* & *B*)^\*^. Hence **K**~*B*~^\*^ also contains *S*^\*^  *A*^\*^, since it is a superset of **K**^\*^. And we also know *S* ⊆ *S~z~*, as (*A* & *B*)  *B*. So it follows that *Bel~B~*(*A* & *B*) = *x* / *z*. Since **K**~*B*~ contains *B*, we know that *Bel~B~*(*B*) = 1, and by (T4) *Bel~B~*(*A* ∨ *B*) = 1. Hence by (T3) *Bel~B~*(*A*) = *Bel~B~*(*A* & *B*) = *x* / *z* as required.

In summary, the argument is that given **K** and *Bel* represent the agent's original belief state, the agent's beliefs must be as they would be as if the agent believed **K**^\*^. We know how to reasonably amend **K**^\*^ by adding *B*, since how to add beliefs to a non-probabilistic belief state is non-controversial. Hence whatever the new agent's beliefs are, say **K**~*B*~ and *Bel~B~*, we know that they must be capable of being modelled by **K**~*B*~^\*^, which we can determine. From this and the simple assumption that *Bel~B~*(*B*) = 1 it follows that *Bel~B~*(*A*) = *Bel*(*A* & *B*) / *Bel*(*B*).

The conclusion of this argument is *stronger* than the conclusion of the parallel Dutch Book argument. As I noted above, that argument could only prove that it was irrational to adopt any strategy other than Conditionalisation. However, this argument shows that it is irrational not to adopt the strategy of Conditionalisation.

## Introducing Imprecision {#sec-0305}

So far I have assumed that all degrees of belief are precise, or in Koopman's term, appraisable. It is time to relax that assumption. I will look in detail at the arguments for this move in @sec-chap-5, but I will introduce them here to motivate what follows. Like precise degrees of belief, imprecise degrees of belief should be capable of being modelled. This I take as a coherence constraint. I will introduce my preferred method of modelling and then look at two advantages of it. The section concludes with a look at the history of this approach to imprecision, and some possible reasons for moving away from it. This leads naturally to the alternatives outlined in subsequent sections.

### Why Be Imprecise?

There are three arguments for the conclusion that we shouldn't require that reasonable degrees of belief be precise. The first is from introspection, the second from the possibility of ignorance and the third from the possibility of rational disagreement.

When we look at our credences in various propositions, it seems highly plausible that these are not precise. My credence in *Oswald killed JFK* is not a precise number; that is, there is no fair lottery with *y* tickets such that my degree of belief in this proposition equals my degree of belief that one of the first *x* tickets will win.[^28] I might be wrong, but I don't think this is because of a failure of rationality on my part. Rather, I have too little clear information on the Kennedy assassination to form a precise credence.

[^28]: If we are allowing real degrees of belief we have to say this more carefully in terms of sequences of fair lotteries. See @sec-0310 for the technical details.

If we believed a betting analysis of credences we could follow a process derived by Borel to measure, to any degree of accuracy we wanted, my degree of belief in this proposition. For arbitrarily large *y* we could keep offering me a choice of bets on Oswald being the killer or on one of the first *x* tickets winning. As long as I choose the Oswald‑bet, we increase *x* until I first choose that bet. In response to Borel I can firstly run through my earlier objections to the betting analysis, but I can make one extra point. After a while (say after *x* grows beyond 0.4*y* or thereabouts) my choices would simply be arbitrary, and would not directly reflect my credences. The assumption that my credences determine what I would do in all circumstances is just the completeness assumption that is at issue here.

If we allow credences to be imprecise we have a good way to represent complete ignorance. Classically, ignorance was represented using Laplace's Principle of Indifference. If we didn't know anything about what would happen, we allocated equal probability to each possibility. Unfortunately when applied indiscriminately this led to inconsistency. Even when applied consistently it led to results which really were absurd.

For example, let *q* be the proposition that there is intelligent life somewhere else in our galaxy. Put some numbering on all the stars in the galaxy (there's only finitely many of them), and for each star i, let *q*~i~ be the proposition that there is intelligent life on some body orbiting that star[^29]. We are presumably completely ignorance about *q*, and about each *q*~i~, so by the Laplacean principle we assign credence 1/2 to each proposition. To see this more clearly, note that when we are considering whether or not it is the case that *q*, there are two possibilities: *q*, ¬*q*. The Laplacean principle says to assign equal credence to each possibility, which in this case means assigning 1/2 to *q* and 1/2 to ¬*q*. However, the same reasoning applies to each *q*~i~. Since each *q*~i~ entails *q*, it follows by simple applications of the probability calculus that we believe absolutely that if there's intelligent life orbiting some other star in the galaxy there is intelligent life orbiting all of them. This is hardly the kind of thing we ought be able to infer from ignorance. When we allow credences to be imprecise on the other hand, we can say that for *q* and each *q*~i~ that our credence in it is vague over the interval \[0, 1\]. We will have to say something about what this means, but it turns out not to have any absurd consequences.

[^29]: I intend 'orbiting' to be read widely enough that the moon does orbit the sun, which indeed it does in a way.

These two arguments are fairly well known, however the third argument is new. In large part this is because it only arises when we analyse probability, as it has been analysed here, as reasonable credence. It is a commonly observed fact that people with the same evidence, or the same relevant evidence, have different credences in a proposition. This can happen even when it seems plausible to say that each is acting reasonably. It would be a benefit for a theory of probability if it licensed this conclusion. That is, if it allowed us to say that reasonable people with the same evidence could, at least some of the time, have divergent credences in a proposition. And it turns out if we allow credences to be imprecise we can say exactly this.

These three arguments are enough, I hope, to motivate an investigation into the technical properties of imprecise credences. I leave the detailed arguments until later, because I want to use some of the technical apparatus developed here in presenting these arguments.

### The Many Models Approach

When all the credences of a coherent agent are precise, they can be represented by a single probability function *Pr*. On my preferred theory, when they are imprecise they can be represented by a family of probability functions P. What it means to say that her credence in *q* is vague over an interval \[α, β\] is that this is the smallest interval such that for all *Pr* ∈ P , *Pr*(*q*) ∈ \[α, β\]. Following @vanfraassen1990a I will call P her 'representor'.

If an agent's credences can be represented in this way there is a trivial sense in which they can be modelled. Every *Pr* in P can be modelled, as shown in section 3.3. Hence we can simply say that the model for each *Pr* is a possible model for the agent. One way of putting this is to say that the correct model for the agent is vague over these possible models. A better way is to say that there are many models for this agent. So I call this the Many models approach.

Apart from arguments from authority there are three reasons for taking this line. First, by using supervaluations it allows us in a sense to have the best of both worlds. In standard cases supervaluationist approaches allow us to keep classical logic, while still saying that the truth value of some propositions is vague. Here it allows us to keep the probability calculus in all its power, while allowing us to say that the probability of some propositions is vague. This will be developed further in @sec-chap-5.

The second reason follows from some technical work done by @smith1961a and @williams1976a[^30]. A *gamble* is a bet that has a certain (finite) payout in each possible world. The bets we have been looking at so far pay \$1 -  at some worlds and ‑ at others, where is the price of the bet. However, these more general gambles can have all sorts of different payouts. This allows us to talk, for example, about the addition of gambles. We can think of gambles as functions from worlds to reals. Then in the usual way we add functions, we can add gambles. So for all worlds *w*, and gambles *x*, *y*, *x* + *y*(*w*) = *x*(*w*) + *y*(*w*). Now let *D* be the class of strictly desirable gambles[^31]. We adopt as coherence axioms that *D* is closed under addition and positive scalar multiplication. We also assume that *D* includes all gambles which always take positive values and no gamble which always takes negative values.

[^30]: The exposition here follows @walley1991a.

[^31]: I include the term 'strictly' because if we think that we will only accept desirable gambles then vagueness leads to a kind of dynamic incoherence, set out in @sec-chap-10. When we add 'strictly' it should be clearer that we don't think that *D* exhausts the class of gambles we would accept if offered. Put another way, vague decision theory is only dynamically coherent if there are gambles we are neither disposed to accept nor reject; calling the class of gambles we are disposed to accept the 'desirable' gambles may lead to the misapprehension we're disposed to reject the rest.

Now we can, in the usual way, work out the expectation value of a gamble according to a probability function *Pr*. So we can work out the set of expectation values that a gamble takes according to every element of a family P . It turns out that whenever *D* obeys the above properties, there is a family P of probability functions such that a gamble *x* is strictly desirable iff its expectation value is positive according to every element of P .

This result can be put the following way. If the agent's credence in every proposition in a field are not such that they can be represented by a family of probability functions, then (assuming a simple decision theory) that agent's decision-making will be incoherent in this sense. There is a set of gambles {*y*~1~, ..., *y*~n~} such that each *y*~i~ is strictly desirable, but *y*~1~ + ... + *y*~n~ is not strictly desirable. This looks to be a bad consequence. Note that this is a much stronger requirement than the usual Dutch Book requirement on credences. An agent who is incoherent in this sense need not be susceptible to a Dutch Book. However, whether the individual considers some gambles strictly desirable will be dependent on the way the gambles are presented, not the contents of those gambles.

For example, assume that propositions *q*~1~ and *q*~2~ are disjoint. Assume that for *q*~1~, *q*~2~ and *q*~1~ ∨ *q*~2~ the agent's credence is vague over \[0.25, 1\]. Let *y*~i~ be a gamble which pays 80 cents if *q*~i~, -20 cents otherwise, for i ∈ {1, 2}. Then the agent will find *y*~1~ and *y*~2~ strictly desirable, but not *y*~1~ + *y*~2~. There is no way this agent can have a Dutch Book made against them, there is no set of strictly desirable bets which have in sum a uniformly negative payout, but something about their degrees of belief seems defective. It is this 'something' which Smith and Williams are interested in, and it turns out that were their degrees of belief represented by some family of probability functions this incoherence would go away.

As we said in @sec-0205, the betting analysis might not be a successful analysis for degrees of belief. However, it is a useful analogy, one of the best analogies we have. And as we've seen already in this chapter, most of the controversial conclusions argued which are usually argued for by Dutch Books arguments are correct. So I take Smith and Williams's arguments to be persuasive but not necessarily compelling arguments for the Many models approach.

### Disjunction And Inequalities

There is a third argument for taking this approach to representing or modelling vague degrees of belief. Let *q* be some proposition in which my degree of belief is vague over an interval of width greater than 1/*y* for some finite integer *y*, and smaller than (1/*y*, (*y* - 1)/*y*). These are fairly minimal constraints. Let *q*~1~ be the proposition that ticket #1 in a fair lottery with *y* tickets will win. Assume that the fairness of the lottery entails that it is independent of *q* which ticket will win.

This seems enough for it to be intuitively clear that my *Bel*(*q* ∨ *q*~1~) is greater than *Bel*(*q*). This is despite the fact that the intervals over which the two are vague overlap. We now have three options. First, we could bite the bullet and deny that *Bel*(*q* ∨ *q*~1~) really is greater than *Bel*(*q*), introspective evidence notwithstanding. Since I take introspective qualitative data as my fundamental given, this seems implausible.

Whenever *Bel*(*A*) is vague over \[α, β\], let *Bel*~\*~(*A*) = α and *Bel*^\*^(*A*) = β. The second option then is to say that *Bel*(*A*) is greater than *Bel*(*B*) simply means *Bel*~\*~(*A*) \> *Bel*~\*~(*B*), or alternatively it means that and *Bel*^\*^(*A*) \> *Bel*^\*^(*B*). There are some difficulties with updating under this approach, but as using my preferred updating rule (i.e. conditionalisation) to cause problems would probably be begging the question I won't stress this point[^32]. Rather I want to note some difficulties this approach has with negation. We showed above that in the precise case if *Bel*(*A*) \> *Bel*(¬*A*) then *Bel*(*A*) \> 1/2. Since the results we obtained there followed so simply from the Equivalence Analysis, I prefer approaches to vague credences which keep as many of these results as possible. However, that approach to inequalities does not preserve this result.

[^32]: See however the discussion in @sec-chap-9 of the analogous decision-rule, called Maxi.

Assume *Bel*(*A*) is vague over \[0.45, 0.6\]. I presume this means *Bel*(¬*A*) is vague over \[0.4, 0.55\]. I leave until @sec-chap-8 discussion of approaches which might not have this result, but it might be noted that this holds (suitably interpreted) for almost all the approaches to vagueness on the market.[^33] On the approach to inequalities advocated here, it follows that *Bel*(*A*) \> *Bel*(¬*A*), but it is not the case that *Bel*(*A*) \> 1/2. Without a motivation for accepting this, I take it that this approach to inequalities fails.

[^33]: This includes approaches where all we specify is a lower bound for *Bel*(*A*). Usually the upper bound will be determined in effect by 1 ‑ *Bel*(¬*A*).

The third option, and it seems the only plausible one, is to say that we have to look at more than the bounds of *Bel*(*A*) and *Bel*(*B*) to determine what inequalities hold between them. And under the Many Models Approach we do just this. We say *Bel*(*A*) \> *Bel*(*B*) for an agent represented by P iff for all *Pr* in P , *Pr*(*A*) \> *Pr*(*B*). A similar definition is given for equality of credences, from which it follows that several credences will be incomparable.

Again, this isn't a totally compelling argument for the advocated approach. There might be other ways at looking at the 'structure' of the intervals other than the family of probability functions approach. And there might be ways of saving the second option, say by motivating the rejection of the theorem on which I relied. However, again the argument developed in this subsection seems to be at least persuasive.

### History of This Approach

The idea of representing vague degrees of belief by sets of precise probability functions has gained dramatically in prominence in recent years. The views of three of its modern proponents (Levi, van Fraassen and Jeffrey) are examined in detail in @sec-chap-7. This subsection looks at its prehistory. The motivation stretches back to the distinction between *risk* and *uncertainty* drawn by the American economist Frank Knight in his [-@knight1921a], and the non-numerical probabilities promoted in Keynes's [-@keynes1921a]. However, neither theorist could be said to have had this particular formal representation of their informal ideas in mind[^34].

[^34]: In @sec-chap-10 I argue that this is a formal representation of Keynes's ideas, though this might be contentious. Knight is a bit harder to classify because he seems to be drawing a different distinction in different chapters of his book. See @schmidt1996a.

Usually credit for the sets of probability function approach is given to the statistician Cedric A. B. Smith. His [-@smith1961a] expressed many ideas which were important for the subsequent development of the theory. In particular the coherence constraints mentioned above are originally his. In discussion on that paper, I. J. Good claims the idea is quite similar to his 'black box' approach, which was first set out in his [-@good1950a], particular on page 32 of that book. However, inspection of that book, and in particular that page, reveals nothing which could be construed as a statement of the theory. These matters are always a little vague, but it seems to me the first statement of Good's black box theory is his [-@good1962a]. Whenever Good first had this idea, which in all likelihood was well before Smith's paper, my reading is that he was beaten into print.

As far as priority goes none of this is, however, particularly relevant. Credit for the idea is, I think, best awarded to the econometrician Gerhard Tintner and economist A. G. Hart. Tintner's [-@tintner1941a] is a brief attempt to capture formally some of the informal ideas that drove Knight's work. It isn't particularly clear but I think it does just enough to be given priority. This summary of his ideas is from his opening paragraph; hopes that its lack of clarity are just caused by its being a concise summary are dashed by inspecting his text.

> Subjective risk deals with the case in which there exists a *probability distribution* of anticipations which, however, is itself known with certainty (probability one). Subjective uncertainty assumes that there is an *a priori* *probability of the probability distributions* themselves, i.e. a distribution of the probability distributions [@tintner1941a 298, italics in original].

It isn't clear in Tintner why we oughtn't reduce the second-order probability distributions to a first order one. Indeed, if we take his probability distributions to be hypotheses about the objective chance of a propositions, and his *a priori* probability to be degree of (rational) belief in these hypotheses, this is precisely what ought be done. Perhaps were he trying to write a philosophy paper and not an econometrics paper he would have been clearer on these issues. Or perhaps he just made a dumb error. It's hard to say from the text which is correct, and charity demands that we at least not accept the latter.

Hart's [-@hart1942a] is somewhat clearer on these points. He thinks that there will be occasions under which we should just reduce the set of distributions to a single distribution, in particular where a decision is needed now. However, when we don't need to act immediately on our partial beliefs then having vague degrees of belief is just the reasonable step of keeping one's options open. I will return to this possible connection between vague degrees of belief in @sec-chap-11 and preference for delaying decision.[^35] Hart also notices that vague degrees of belief serve as an explication of at least something Knight was trying to capture with the distinction between risk and uncertainty.

[^35]: When I formally set out the idea I associate holding money, or gambles with a more stable money value, with delaying action. Hence I won't need an implausible distinction between acts and omissions.

> 'Risk' is taken to denote the holding of anticipations which are not 'single valued' but constitute a probability distribution having known parameters. 'Uncertainty' is taken to denote the holding of anticipations under which the parameters of the probability distribution are themselves not single valued [@hart1942a 110].

Neither Tintner's nor Hart's work receives a great deal of attention, and the papers of Smith and Good from 1961 and 1962 respectively appear to have eventually had greater effect. That effect though was somewhat delayed. Indeed to bring the story up to the 1980s the only papers that need to be mentioned are two by Peter Williams. His [-@williams1976a] was a formal development of some of the ideas in Smith, which from our perspective is notable for being the first to note the suitability of supervaluational semantics to this account of probability. And his [-@williams1978a] review essay of @shafter1976a introduced the set of probability functions approach to a much wider audience.

### What might go wrong?

It might be wondered why I have kept such a tight connection with precise degrees of belief as we stride into the brave new world of vagueness. Why not represent our vague degrees of belief as a single vague model rather than as a set of precise models? The arguments above presented some sort of case for this path, but it's hardly irrefutable.

The conclusion of this chapter will be that the reasons we don't move to a single vague model are that the only plausible way of doing so is (i) *more* restrictive than the approach advocated here and (ii) incapable of being coherently updated. There is no plausible single model which allows an agent to have as its representor any family of probability functions. In the next section we will look at a type of single model which is plausible, but which puts unjustified restrictions on the types of families which are rationally permissible. In @sec-0307 we'll look at a way of loosening these restrictions. Unfortunately, in doing so we lose some restrictions we should (I hope) like to keep. For better or worse, the most plausible way to represent vague degrees of belief is by reference to these precise degrees of belief.

## The Single Model Approach {#sec-0306}

In the previous section we set out one possible paradigm for the representation of vague beliefs, what I call the Many Models Approach. In this section we will look at a different approach, which looks like a more natural generalisation of the approach to representing precise degrees of belief. It turns out this approach is *more* restrictive than the Many Models Approach, so we will look at whether the extra restrictions can be justified. I will conclude, somewhat tentatively, that they are not. However, I don't think this approach is so implausible that we should abandon it altogether, and in @sec-0308, which is on updating, we will return to it.

### The Model

In @sec-0303 we set out a method for modelling an agent's beliefs about a set of propositions Γ such that the agent had a precise degree of belief in each of them. The plausibility of such modelling followed from the definition of degrees of belief set out in @sec-0301. The idea is that there is a closed set of propositions **K**^\*^ on a possibility space *W* × *P* which satisfies the following two conditions:

> \(1\) (*S* ⊆ *P* & *S*^\*^ ∈ **K**^\*^) → *S* = *P*    
> \(2\) If *Bel*(*A*) = *x / y* then ∃*S*: ((*S* ⊆ *P* & \|*S*\| = *x*) & (*S*^\*^  *A*^\*^ ∈ **K**^\*^))

The motivation for (2) was that if the agent had credence *x* / *y* in *A*, their credence in *A* is just the same as it would be were they to believe *A*  *S* for some proposition *S* in which their credence is, by definition, *x* / *y*. However, there is no reason in general for saying that the agent will have a precise credence in every proposition. Hence (2) should be replaced with (3) and (4).

> \(3\) Iff *Bel*(*A*) ≥ *x / y* then ∃*S*: ((*S* ⊆ *P* & \|*S*\| = *x*) & (*S*^\*^ → *A*^\*^ ∈ **K**^\*^))    
> \(4\) Iff *Bel*(*A*) ≤ *x / y* then ∃*S*: ((*S* ⊆ *P* & \|*S*\| = *x*) & (*A*^\*^ → *S*^\*^ ∈ **K**^\*^))

So as not to increase the confusion, I'll here use '→' for material implication. It might be noticed that (3) and (4) entail (2), so the new axioms are stronger than those for (2). This seems right; even if the agent needn't have precise degrees of belief in every proposition, for every proposition where their credence is precise then (2) ought be satisfied.

I haven't required here, as I did for the precise models approach, that *y* be related in any interesting way to the end-points of the intervals over which the agent's credences are vague. If (3) and (4) are plausible requirements, it seems they are plausible for any value of *y*. To make it clear which *y* we are using, we will use *P~y~* for integer *y* to represent the set {*p*~1~, ..., *p~y~*}. The claim is then for any reasonable epistemic state and any integer *y* there is a model on *W* × *P~y~*.

The idea behind (3) is that if \|*S*\| = *x* then for any proposition *B* such that *S*^\*^  *B*^\*^ is in the model, *Bel*(*B*) = *x / y*. As I showed above, for precise models it is a theorem that whenever *B* → *A* is believed fully, it is a coherence requirement that *Bel*(*B*) ≤ *Bel*(*A*). I assume (without strictly proving it) that this holds for imprecise credences too, and this is sufficient for the reverse direction of (3). The forward direction follows from the fact that **K**^\*^ is a model of the agent's credences. If the largest *S* satisfying *S*^\*^ → *A*^\*^∈ **K**^\*^ had a cardinality *x* ‑ , then it would seem the agent's credence in *A* should, according to the model, be (*x* ‑ ) / *y*. Hence either the model is inaccurate, or the agent's beliefs are incoherent in some way. A similar justification applies to (4).

### The Three Prisoners Problem {#sec-tpp}

To see how this might work, consider the well-known Three Prisoners Problem (TPP). There are three prisoners, *a*, *b* and *c*, scheduled to be executed. The governor has decided to reprieve one of them, chosen at random by a fair chance mechanism. He makes his decision, and notifies the guards of it. However, he doesn't tell the prisoners. Prisoner *a*, desperate for information, asks a guard if he is to spared. The guard won't say, so *a* asks him for the name of one of the other prisoners who isn't going to be spared, and the guard agrees to answer this. We'll look in @sec-0308 about how *a* should react to the guard's answer, but here we'll just show how *a*'s beliefs can be modelled to demonstrate the Single Model Approach[^36].

[^36]: This problem is usually credited to @gardner1961a, though he introduces it as a problem that was 'doing the rounds' at the time (pg 227). If we were being fussy about priority we'd have to say that the problem is a variation on a problem found at the beginning of Bertrand's classic [-@bertrand1889 2-4]. In that problem we have three boxes containing two coins each. The first contains two gold coins, the second a gold and a silver coin and the third contains two silver coins. A box is chosen by a fair chance mechanism and then a coin is drawn from that box by an unknown mechanism. The coin drawn is silver. What is the probability that the other coin in the box is gold? (Bertrand asks the complementary question: What is the probability the other coin is silver?) As in the problems I discuss, the popular but incorrect answer is 1/2. If we say the coin selection mechanism is fair, then a simple application of Bayes's Rule gives the answer 1/3. If we leave it unknown I take it the probability of drawing a gold coin is just the same as *a*'s probability of being reprieved after hearing the guard's news.

Assume, for the sake of simplicity, that *a* knows the guard to be perfectly reliable. Then there are four possible outcomes. Let the proposition c~f~c~k~ for prisoners c~f~, c~k~ mean that c~f~ is to be reprieved and the guard says c~k~ is to executed. The original possibility space *W* is {*ab*, *ac*, *bc*, *cb*}. If *b* or *c* are to be spared the guard has no choice about what to say, so *Bel*(*bc*) = *Bel*(*cb*) = 1/3. (I am assuming here that Lewis's Principal Principle holds: when we know the chance of a proposition *q* is *x*, we ought to believe *q* to degree *x* .) However, if *a* is to be reprieved, the guard has a choice about what to say, and *a* has no way of knowing what he will do. In Gardner's formulation of the problem the prisoner asks the guard to decide in this case what to say by tossing a coin, but my prison guards aren't *that* cooperative. Hence, *a*'s credence in *ab* might be vague over \[0, 1/3\]. However, the proposition *ab* ∨ *ac* is equivalent to the proposition '*a* is reprieved', which *a* knows has chance 1/3. Hence his credence in *ab* ∨ *ac* should be 1/3.

We can represent all of *a*'s beliefs as a model on *W* × *P*~3~. The model **K**^\*^ contains the closure of the following formulae: *p*~1~  *bc*, *p*~2~  *cb*, *p*~3~  *ab* ∨ *ac*. To see how (3) and (4) are satisfied for *A* = *ab*, note that in the model we have ⊥ → *ab*, ⊥ = ∅^\*^ and \|∅\| = 0, so ∅ is the *S* for (3). For (4), we have *ab* → {*p*~3~}^\*^ in the model and \|{*p*~3~}\| = 1. Hence {*p*~3~} is the *S* in (4). We can show how *ac* is satisfied in a similar way.

### Shafer Functions

To show that every set of vague credences which can be represented under this approach can be represented using the Many models approach, I will show that this approach is equivalent to one advocated by @shafer1976a. His book built on some earlier suggestions of Dempster [-@dempster1967a; -@dempster1968a], so the functions of the theory are usually referred to as Dempster-Shafer functions, or just Shafer functions. They are also called 'belief functions', particularly by Shafer. For simplicity I will presume that the probability space Ω is finite. Shafer functions can be extended unproblematically to infinite spaces, but the mathematics becomes more complicated, and there are no particularly interesting philosophical issues that arise out it.

As Ω is a set of possibilities[^37], we can define its power set **Po**(Ω). If we wanted to define a probability function on Ω we could allocate some mass to each possibility. Then the probability of a proposition *A* on Ω (where *A* is the union of some possibilities) is the sum of the mass of every element in *A*. To define a Shafer function on Ω we allocate a mass to every element of **Po**(Ω). Then *Bel*(*A*) in the model is the sum of the masses of every subset of *A*. The plausibility of *A*, written *Pl*(*A*), is defined as 1 ‑ *Bel*(¬*A*). For ease of reference, I'll write *Bel*~S~ for a Shafer function. This way the notation doesn't presuppose that *Bel*~S~ functions are reasonable *Bel* functions.

[^37]: I don't call them possible worlds because that would imply there are infinitely many of them. Rather Ω is a finite partition of the possible worlds, with each element of the partition being called a 'possibility'.

    I am calling the power set function **Po**, rather than **P** as would be standard, so as not to cause confusion with P which is used to represent a family of probability functions. If I had to I could just differentiate the functions by the different fonts used, but unless it's necessary this seems inappropriate.

Formally, when there is a possibility space Ω, a mass function *m*: **Po**(Ω) → \[0, 1\] and a Shafer belief function *Bel*~S~ the following must hold.






**FIND THESE!!***

![](media/image3.emf) ![](media/image4.emf)








It should be clear that once the mass function *m* is defined the *Bel*~S~ function is also defined. It is less obvious that once the *Bel*~S~ is defined function the mass function is determined, however this can be proven [@shafer1976a 39]. Usually I'll define mass functions only, because they are clearer and shorter.

We can represent prisoner *a*'s state of belief as a Shafer function. The possibility space Ω is still {*ab*, *ac*, *bc*, *cb*}, and the mass function is given by *m*({*bc*}) = *m*({*cb*}) = *m*({*ab*, *ac*}) = 1/3. The *Bel*~S~ and *Pl* functions defined by this mass function are such that for any proposition *A* on Ω, the prisoner's degree of belief in *A*, as we have defined it, is vague over the interval \[*Bel*~S~(*A*), *Pl*(*A*)\].

@shafer1981a points out that we can think of Shafer functions in the following way. Imagine that we will find out for certain that *B*, where *B* is some proposition on Ω. (Shafer uses an analogy with being sent a single coded message.) Assume that for every *B* we have a precise degree of belief that it will be what we will find. Then *Bel*~S~(*A*) is our degree of belief that we will find out *A*. As has been argued (see for example @pearl1988a) we can think of *Bel*~S~(*A*) as the probability of *A* being provable, or more generally of the probability of *A*, for some interpretation of *A*. In @sec-chap-8 I'll develop a constructivist theory of probability based around this intuition. That theory arguably does a better job than Shafer's own at capturing the intuitions he is promoting.

Intuitively, this seems to show that the Shafer functions can model *some* important epistemic property, but it is doubtful that it is credence. If *Bel*~S~ is the appropriate representation for some agent, then she should be convinced that her credence in *A* is at least *Bel*~S~(*A*), not that it's exactly *Bel*~S~(*A*). Even on Shafer's preferred analogy of the secret message, it seems more plausible to say that *Bel*~S~ defines a lower bound on credences, not the actual credences. If we can prove *A* from the message this would be conclusive reason to believe *A*, but it is possible that we won't be able to prove *A* yet *A* still be true. Hence we should say our credence in *A* is bounded below by our credence in *A*'s provability. From here I'll assume that the point of a Shafer function is to set bounds on credences, as this seems to be the only plausible interpretation. This move is not original; as will be seen below, there have been some detailed investigations into its plausibility.

Shafer functions are important to this project because of @thm-belshaf, a proof of which is given in appendix 3B.

::: {#thm-belshaf}

## Bel-to-Shafer

Let *Bel* be a vague belief function such that there is a model **K**^\*^ for *Bel* satisfying (1), (3) and (4). Let Γ be any finite field of propositions such that *Bel*(*A*) is vague over an interval bounded by rational numbers for all elements of Γ. Then there is a Shafer function *Bel*~S~ such that for all *A* in Γ, *Bel*(*A*) is vague over the interval \[*Bel*~S~(*A*), *Pl*(*A*)\].

:::

The proof works by constructing a mass function from **K**^\*^. The core idea is to allocate mass 1/*y* to the set of possibilities which are left open by *p*~i~ for each i ∈ {1, ..., *y*}. Now to complete the proof that Single Model Approach is stricter than the Many Models Approach, we only need the following theorem, which was proved by @dempster1968a.

::: {#thm-dempster}

## Dempster

Let *Bel*~S~ be a Shafer function defined on a possibility space Ω. Then there is a convex family P of probability functions defined on the same possibility space with the following properties.

> \(i\) For each *A* ⊆ Ω and *Pr* ∈ P, *Bel*~S~(*A*) ≤ *Pr*(*A*) ≤ 1 ‑ *Bel*~S~(¬*A*) = *Pl*(*A*)    
> \(ii\) For each *A* ⊆ Ω, there is a *Pr* ∈ P such that *Pr*(*A*) = *Bel*~S~(*A*).    
> \(iii\) For each *A* ⊆ Ω, there is a *Pr* ∈ P such that *Pr*(*A*) = *Pl*(*A*).

Let *W*(*Bel*~S~) denote the largest such family. Generally, for a given convex family of probability functions, define *inf*(*A*) as the lowest value that *Pr*(*A*) takes for *Pr* in the family[^38]. Then *sup*(*A*) = 1 ‑ *inf*(¬*A*) is the maximal value that *A* takes. It is well-known (see for example @shafter1976a [5]) that all Shafer functions satisfy the following property, which we'll call **complete monotonicity**. Let *A*~1~, ..., *A*~n~ be propositions (not necessarily disjoint) on Ω.

[^38]: Or if this does not exist, the greatest number *x* such that *x* \< *Pr*(*A*) for all *Pr* in the family. I am generally dealing with closed families here, so I won't attend to the added complications arising from this possibility.




*Bel*~S~(*A*~1~ ∨ ... ∨ *A*~n~) ≥ ![](media/image5.emf)





For example, for n = 2, the condition requires that *Bel*~S~(*A*~1~ ∨ *A*~2~) ≥ *Bel*~S~(*A*~1~) + *Bel*~S~(*A*~2~) - *Bel*~S~(*A*~1~ & *A*~2~). However, if we substitute *inf*(*A*) for *Bel*~S~(*A*), complete monotonicity is not a property of all families of probability functions. To take a simple example (this is due to @williams1978a), consider the family {*Pr*: *Pr*(*A*~1~) ≥ 1/2 & *Pr*(*A*~2~) ≥ 1/2}. In this family, *inf*(*A*~1~) = *inf*(*A*~2~) = *inf*(*A*~1~ ∨ *A*~2~) = 1/2 and *inf*(*A*~1~ & *A*~2~) = 0. This shows that the Single Model approach is strictly stronger than the Many Models approach. So if we are to adopt this approach to modelling reasonable imprecise degrees of belief we need a justification for this extra restriction.

All families *W*(*Bel*~S~) have a further property, which is independent of monotonicity, but which isn't shared by all families of probability functions. I'll call it, for want of a better name, **event defined**. Families which have this property are such that once we have given all the intervals in which the probability of every proposition falls, we have said all there is to say about the family. Formally, a family P is event defined relative to a possibility space Ω iff it meets the following condition.

> **Event Defined**    
> For all *w*~i~ in Ω, let *x*~i~ be any element of \[*inf*({*w*~1~}), *sup*({*w*~1~})\]. Then there is a *Pr* ∈ P such that for all i, *Pr*({*w*~i~}) = *x*~i~.

A family is event defined *simpliciter* iff it is event defined relative to all possibility spaces Ω. Since *W*(*Bel*~S~) is simply defined with respect to the interval within which the degree of belief of each proposition falls, it follows it is event defined.

Let *Bel* be a belief function represented by the convex monotone family of probability functions P and modelled on **K**^\*^. We will say that *Bel* is event defined iff P is. It doesn't follow from (1), (3) and (4) that P must be event defined. However, if it is not event defined then **K**^\*^ is not a good model for it in the following sense. Let P ~E~ be the smallest event defined family containing P.[^39] The model for P ~E~ will also be **K**^\*^. Hence the model loses the extra precision of P over P ~E~.

[^39]: This family can be constructed in the following way. Assuming we are allowed to quantify over propositions, it is the family {*Pr*: ∀*A* *inf*(*A*) ≤ *Pr*(*A*) ≤ *sup*(*A*)}.

So if we require that all vague belief functions must be capable of being modelled under this approach, we aren't requiring that *Bel* be event defined, but we are requiring it to be monotone (I will drop the 'completely' unless clarity demands it). What arguments are there, then, that non-monotone belief functions are somehow less reasonable than monotone ones? Well, the plausibility of (1), (3) and (4) is an argument already. However, the non-monotone family of probability functions Williams describes also seems reasonable enough. So we might need some stronger arguments to get to monotonicity.

### Uncertainty Aversion

@schmeidler1989a has one reason. He argues that any belief function which licences uncertainty averse behaviour will be monotone. I'm not convinced uncertainty aversion is a requirement of rationality. Schmeidler doesn't say it is; I'm adapting his argument somewhat, because some people might reason this way. Schmeidler develops his account of uncertainty aversion by explicit analogy with the standard account of risk aversion. And herein lies the difficulty, for the standard theory of risk aversion is mistaken. That is, there is no way which a person is averse to risk in the everyday sense iff they are *risk averse* in the standard economist's sense. By analogy, Schmeidler's account of uncertainty aversion does not capture correctly the everyday concept of aversion to uncertainty. Hence we have no reason to apply it as a coherence constraint.

Standardly, an agent is said to be risk averse with respect to some good (usually money) iff the marginal utility of that good is declining. More formally, if U(*x*) is the amount of utility received from *x* units of that good, the agent is risk averse iff U´´(*x*) \< 0 for all *x*. This is thought to represent risk aversion because it means that the agent will always prefer receiving (*a* + *b*) / 2 units of the good to having a 1/2 chance of receiving *a* units and a 1/2 chance of receiving *b* units if *a* *b*. In particular the agent will prefer the status quo to a bet on *p*, where the chance of *p* is known to be 1/2, and she wins *a* units if *p* and loses *a* units if ¬*p*. It is a direct consequence of this definition, and the definition that the expected utility of a gamble is the sum of the utility of each possible outcome times its probability, that if an agent is risk averse they will prefer the mix of any two gambles to at least one of those gambles. If *f* and *g* are gambles their mixes, α*f* + (1 ‑ α)*g* for α ∈ \[0, 1\] are defined as bets which pay whatever *f* pays if *p* and whatever *g* pays if ¬*p*, where *p* is a proposition whose chance is known to be α.

Following this, Schmeidler defines uncertainty aversion in the following way. The binary relation ≥ is a preference ordering on gambles. The set of gambles (what Schmeidler calls 'acts') is called L.

> A binary relation ≥ on L is said to reveal uncertainty aversion if for any three acts *f*, *g* and *h* in L and any α in \[0, 1\]: If *f* ≥ *h* and *g* ≥ *h* then α*f* + (1 ‑ α)*g* ≥ *h*. Equivalently we may state: If *f* ≥ *g*, then α*f* + (1 ‑ α)*g* ≥ *g*. [@schmeidler1989a 582]

I don't want to criticise Schmeidler's extraction of a principle of uncertainty aversion from the classical principle of risk aversion. What I will criticise is that classical principle of risk aversion. The arguments given by @hansson1988a seem to me to show conclusively that the classical concept of risk aversion does not accurately capture the everyday concept of aversion to risk. Hence we can run similar arguments to show that Schmeidler's concept does not capture the everyday concept of aversion to uncertainty. Any good ideas in what follows here can be found in Hansson.

Joe is a gambler, a person who likes risk. He also is a student of the foundations of mathematics, and as such would like to get a copy of Russell and Whitehead's *Principia Mathematica* (hereafter *PM*). I offer Joe a choice between a copy of *PM* for certain or a gamble. The gamble pays 3 copies of *PM* if a coin lands heads, or nothing if it lands tails. Joe, who has little use for any extra copies of *PM*, chooses the single copy. Hence we can infer that Joe's utility curve in copies of *PM* is convex, hence Joe is risk averse. But we said at the start that Joe liked risk. Indeed we can specify that if I had increased the payout of the gamble to say 6 copies of *PM*, Joe would have accepted it even though he had less use for the 5 extra copies than for the original one. This seems sufficient to say Joe is *attracted* to risk, not averse to it.

The example shows not only that the classical concept of risk aversion is mistaken, but how we might try and fix the difficulty. Let *f* and *g* be goods such that the worth to us of *g* if we already had *f* is equal to the worth of *f*. In some cases we can easily measure this. For instance in Joe's case we can ensure that the extra copies of *PM*, whether it's 2 or 5 extra copies, get used the way Joe would have used them if he already had a copy. So we might give them to Joe's friends and say they're a present from Joe, or if he was to use the surplus volumes to prop up table legs we can give him equivalent sized blocks of wood, and so on. Often we won't be able to make this comparison, but sometimes we will. Now, it seems plausible to say that Joe is risk averse iff he prefers *f* to a 1 in 2 chance of getting *f* + *g*. Since we specified that Joe would prefer a 1 in 2 chance of getting *f* + *g* to getting *f*, even when getting *g* + *f* was less than twice as useful as getting *f*, Joe is attracted to risk. Put this way, standard theories of rationality require that everyone be risk neutral.

I conclude that there is no reason to think that the concept picked out by *U*´´(*x*) \< 0 is our concept of risk aversion. So the concept picked out by Schmeidler is not our concept of aversion to uncertainty. Hence this can't provide a reason for thinking that it is a requirement of rationality.




Check for X and P when they should be in math





### Fagin-Halpern Models

@fagin1991a define *probability spaces* as follows. Probability spaces are triples \<*S*, X, *P*\> such that *S* is a set of possibilities, X a σ-algebra on *S* and *P* a probability function defined on X. Now for any proposition *A*, (i.e. for any subset *A* of *S*), its inner probability, *Pr*~\*~ and outer probability *Pr*^\*^ are defined as follows.

> *Pr*~\*~(*A*) = *sup*{*Pr*(*X*) \| *X* ⊆ *A* and *X* ∈ X }    
> *Pr*^\*^(*A*) = *inf* {*Pr*(*X*) \| *X* ⊇ *A* and *X* ∈ X }

It turns out that *Pr*~\*~ is a DS belief function. More surprisingly, for every Shafer function *Bel*~S~ on a set *S*, there is a probability space \<*S*, X, *P*\> such that *Pr*~\*~ = *Bel*~S~. (A similar relation holds between *Pr*^\*^ and *Pl*). The motivation behind Fagin and Halpern's approach is that we can't assign a numerical probability to every subset of *S*. However, for some sets the agent will have "sufficient information to assign a probability." (349) These are the *measurable* sets. We can work out the limits on the probability of the other sets by reference to the measurable sets. A similar motivation appears to be at work in @jeffrey1983a.

Even though everything to this point has been written in terms of bounds, this could be analysed in terms of families of probability functions. Indeed this is exactly the approach Fagin and Halpern take. A probability space \<*S*, X, *P*\> determines a family, P , of probability functions on *S* such that for every function *Pr* ∈ P and every element *X* of X, *P*(*X*) = *Pr*(*X*). Then *Pr*~\*~(*A*) is just the minimal value of *P*(*A*) in P.[^40]

[^40]: Fagin and Halpern like the analysis in terms of families of probability functions because of concerns about updating. We shall return to these in @sec-0308.

Fagin and Halpern don't appear to take this approach, but we could view their account of probability spaces as an argument for thinking reasonable families of probability functions should have belief functions as their lower bounds. The problem with this argument is that it is hard to see what argument could be given for saying that we should be able to represent vague degrees of belief as probability spaces. A person who has literally no belief might be represented by the vacuous family of probability function {*Pr*: *Pr* is a probability function}. For any *a posteriori* proposition, this person's degree of belief in it is vague over \[0, 1\].

Assume an agent is in this unfortunate state. On Fagin and Halpern's approach, the only way we can move away from this is by coming to have a precise degree of belief in some proposition. If there is no proposition in which the agent has a precise degree of belief our belief, then she must have vacuous beliefs. This seems unreasonable; we can be partially (rather than completely) vague about something without being precise about anything. Hence I don't think it is a requirement of rationality that our degrees of belief be representable by Fagin-Halpern probability spaces, so this can't provide an argument as to why the lower bound on reasonable families of probability functions should be a Shafer function.

### Convexity {#sec-030606}

Even if we were assured that if an agent's representor should, if convex, be monotone, there is a case to be made against requiring convexity in the first place. If this argument appears to succeed it will strengthen our case that the single model approach is too restrictive. @jeffrey1987a argues that the set of probability functions {*Pr*: *Pr*(*A* & *B*) = *Pr*(*A*) · *Pr*(*B*)}, which just says that *A* and *B* are probabilistically independent, can represent a reasonable epistemic state. However, as can be easily seen, it is not a convex family. The functions *Pr*~0~, *Pr*~0~(*A*) = *Pr*~0~(*B*) = *Pr*~0~(*A* & *B*) = 0, and *Pr*~1~, *Pr*~1~(*A*) = *Pr*~1~(*B*) = *Pr*~1~(*A* & *B*) = 1, are both in P, but no linear mixture of them is. Since on the single model approach an agent's representor can only be a convex set of probability functions, the plausibility of Jeffrey's claim that for some *A*, *B* this set represents a reasonable state suggests again the single model approach is too restrictive.

### Summary

The core aim of this section was to put forward a single model approach to modelling vague degrees of belief. It turned out this approach was stricter than the many models approach of the previous section. Hence I looked at various possible arguments for this extra restriction, but it turned out that none of these were particularly compelling. In the next section I'll look at a different type of model which is less restrictive.

## Many-Urn Models {#sec-0307}

The models discussed in @sec-0306 seemed too restrictive. Belief states which intuitively seem coherent cannot be modelled within the scope of this theory. Hence we might be tempted to look for a different type of model, such as the Many-Urn model set out in this section. I will discuss this model's properties, show how it gets around some of the difficulties of the Single Model discussed above, and then set out some other benefits and costs of this model. I conclude that it is ultimately not a viable approach, because it is too permissive.

### The Model

The example of a belief state which couldn't be modelled using the Single Model approach was one represented by the family of probability functions {*Pr*: *Pr*(*A*~1~) ≥ 0.5 & *Pr*(*A*~2~) ≥ 0.5}. The idea here is that the agent has degree of belief greater than 0.5 in each of two propositions, but is completely ignorant about the probabilistic dependence between these two propositions. We can't represent this in the single model because within that approach there is no way to represent this kind of ignorance. We can have vague degrees of absolute belief, but the allowable vagueness in degrees of conditional belief is somewhat restricted.

Many-Urn models are designed to deal with this specific case. I said earlier it was possible to think of each of the *p*~i~ as expressing the proposition that the i'th ball is drawn from an urn with *y* balls in it. In Many-Urn models this analogy is developed in the following way. Proposition *p*~ij~ says that the i'th ball is drawn from the j'th urn. It is assumed here that there are several urns, each containing *y* balls, with a ball to be drawn from each urn. While it is assumed that the initial chance of drawing each ball from a given urn is equal (in a sense the urns are fair) it is not assumed the drawings are independent.

Most, if not all, of the interesting properties of many-urn models are displayed by 2-urn models, so we will restrict our attention to these. We'll start by doing the precise version, and then weaken this to the imprecise version. So *p*~i1~ is the proposition that the i'th ball was drawn from urn 1, and *p*~i2~ the proposition that the i'th ball was drawn from urn 2. Let *P*~1~ be the set of propositions *p*~i1~ for all i, and *P*~2~ the set of propositions *p*~i2~, again for all i. In this approach, the agents probabilistic beliefs are modelled as propositional beliefs about subsets of *W* × *P*~1~ × *P*~2~, with **K**^\*^ and ∆^\*^ defined as above. A model must then satisfy the following two constraints, which are just translations of (1) and (2). The definition of the asterisk operator is trivially extended to the new setting.

> (1´) (*S* ⊆ *P*~1~ × *P*~2~ & *S*^\*^ ∈ **K**^\*^) → *S* = *P*~1~ × *P*~2~    
> (2´) If *Bel*(*A*) = *x / y* then ∃*S*: ((*S* ⊆ *P*~1~ ∨ *S* ⊆ *P*~2~) & (\|*S*\| = *x* & *S*^\*^  *A*^\*^ ∈ **K**^\*^))

If a function *Bel* satisfies these conditions, then it must be a probability function. The proof of this is a little less neat, because we can't prove the equivalent of (T6). That is, there doesn't seem to be any contradiction in saying there are sets *S*~1~ ⊆ *P*~1~ and *S*~2~ ⊆ *P*~2~ such that \|*S*~1~\|  \|*S*~2~\| and *S*~1~^\*^  *A*  *S*~2~^\*^ ∈ **K**^\*^. So we'll just stipulate that *Bel* is a function. Although this possibility isn't formally inconsistent, it is rather odd since we have in some sense *Bel*(*S*~1~^\*^) = *Bel*(*S*~2~^\*^) and *Bel*(*S*~1~^\*^)  *Bel*(*S*~2~^\*^). Of course this use of '=' isn't formally permissible if *Bel* is a relation rather than a function, but it is a motivation for restricting our attention to belief functions.[^41]

[^41]: Formally, the idea I have in mind is that we say *Bel*(*A*) = *x / y* means that the relation *B*(*A*, *x / y*) holds, where the holding of this relation is consistent with the relation *B*(*A*, *x*~1~ /*y*), where *x*~1~  *x*.

Given this, the proofs that *Bel* must satisfy the three conditions of probability functions follows quite easily. It can be simply observed that *Bel* only takes non-negative values, since sets only have non-negative cardinality, and *Bel* is equal to the ratio of the cardinality of two sets. Similarly the proof that *Bel*(T) = 1 is just as above. The only difficulty is in proving *Bel* satisfies (Pr3). First we'll prove (L4).

> (L4) ¬∃i, j *w*: \<*w*, *p*~i1~, *p*~j2~\> ∉ ∆^\*^.

Assume there are i, j satisfying this condition. Then let *S* = {\<*p*~α1~, *p*~β2~\>: α  i & β  j}. By the definition of ∆^\*^ it follows that *S*^\*^ ∈ **K**^\*^. Hence by (1) *S* = *P*~1~ × *P*~2~, a contradiction. This proves (L4).

Now assume that for some *A*, *B* such that ¬(*A* & *B*) is provable, **K**^\*^ includes *S*~1~^\*^  *A*^\*^ and *S*~2~^\*^  *B*^\*^ such that *S*~1~ ⊆ *P*~1~ and *S*~2~ ⊆ *P*~2~. Let *p*~i~ be an element of *S*~1~ and *p*~j~ an element of *S*~2~. It follows that for any world *w*, \<*w*, *p*~i1~, *p*~j2~\> is an element of *S*~1~^\*^ ∩ *S*~2~^\*^. Since **K**^\*^ is closed it contains ¬(*A*^\*^ & *B*^\*^), so by substitutivity of material equivalents it contains ¬(*S*~1~^\*^ & *S*~2~^\*^). Hence for all *w*, \<*w*, *p*~i1~, *p*~j2~\> ∉ ∆^\*^, contradicting (L4). Hence *S*~1~ and *S*~2~ are each subsets of *P*~1~ or of *P*~2~. By a similar proof we can show that they cannot have any common members, hence \|*S*~1~ ∪ *S*~2~\| = \|*S*~1~\| + \|*S*~2~\|, and \|*S*~1~ ∪ *S*~2~\| ⊆ *P*~1~ or *P*~2~, so *Bel*(*A* ∨ *B*) = *Bel*(*A*) + *Bel*(*B*), as required.

### Imprecision

So moving to a Many-Urn model does not show that *Bel*, if always rational-valued, may be something other than a probability function. The only interesting difference arises when we move to imprecise models, for there we do perceive a real difference. As above, we drop axiom (2´) and import axioms (3´) and (4´).

> (3´) Iff *Bel*(*A*) ≥ *x / y* then ∃*S*: ((*S* ⊆ *P*~1~ ∨ *S* ⊆ *P*~2~) & \|*S*\| = *x*) & (*S*^\*^ → *A*^\*^ ∈ **K**^\*^)    
> (4´) Iff *Bel*(*A*) ≤ *x / y* then ∃*S*: ((*S* ⊆ *P*~1~ ∨ *S* ⊆ *P*~2~) & \|*S*\| = *x*) & (*A*^\*^ → *S*^\*^ ∈ **K**^\*^)

Again, if *Bel*(*A*) always takes precise values (3´) and (4´) reduce to (2´). However, if it doesn't then we can have a more general system of models. Indeed, we now have a system so general that we can model the family of probability functions we mentioned at the start of this section. **K**^\*^ is the following set, with *y* = 2.

> **K**^\*^ = *Cn*({*p*~11~ → *A*~1~, *p*~12~ → *A*~2~})

If *Bel* can be modelled in this way, then *Bel*~\*~ and *Bel*^\*^ have the following rather neat properties.

> (M1) *Bel*~\*~(T) = 1    
> (M2) *Bel*^\*^(⊥) = 0    
> (M3) If *A*  *B* then *Bel*~\*~(*B*) ≤ *Bel*~\*~(*A*)    
> (M4) *Bel*~\*~(*A*) = 1 - *Bel*^\*^(*A*)    
> (M5) If  ¬(*A* & *B*) then *Bel*~\*~(*A* ∨ *B*) ≥ *Bel*~\*~(*A*) + *Bel*~\*~(*B*) and *Bel*^\*^(*A* ∨ *B*) ≤ *Bel*^\*^(*A*) + *Bel*^\*^(*B*)

@walley1991 [600] calls a function that satisfies these conditions 3-coherent. They are regarded as sufficient conditions for a lower probability function by @suppes1974a, @wolfenson1982a and @fine1983a. I won't go through the proofs because they are quite similar to the proofs already given. (In particular the proof for (M5) just uses the same move we made to prove (Pr3).) However, this is not sufficient to show that any *Bel* which can be modelled in this way is coherent. In particular, the following theorem, which seems intuitively plausible, is not validated by this model.

> (M6) Assume *A*~0~, *A*~1~, ..., *A*~n~ are pairwise disjoint, and that *Bel*~\*~(*A*~0~ ∨ *A*~1~) + ... + *Bel*~\*~(*A*~0~ ∨ *A*~n~) \> 1. Then *Bel*~\*~(*A*~0~) \> 0.

If a betting analysis of credences is adopted it is clear why we should want (M6). Assume that *Bel*~\*~(*A*) is our minimum buy-price for *A*‑bets, and that the assumptions in (M6) hold. Then we will be prepared to buy bets on *A*~0~ ∨ *A*~1~ through to *A*~0~ ∨ *A*~n~ in such a way that we will have spent more than \$1. This set of bets can only have a return greater than \$1 if *A*~0~. Hence we are prepared to buy a set of bets which have the same effect as placing a bet on *A*~0~. But if *Bel*~\*~(*A*~0~) = 0 then we have said we are not prepared to bet on *A*~0~ at any odds. So there is some incoherence in our betting practices.

Even if we don't adopt a betting analysis there is something wrong with belief functions which don't obey (M6). Assume *Bel*~\*~(*A*~0~) = 0. Then *Bel*(*A*~0~) is vague over an interval which includes 0. So it should be coherent with the rest of our beliefs that *Bel*(*A*~0~) = 0. If it isn't we can say we've ruled that possibility out, so we aren't vague over it after all. So assume *Bel*(*A*~0~) = 0. Since for all i in {1, .., n} *Bel*(*A*~0~ ∨ *A*~i~) ≥ *Bel*~\*~(*A*~0~ ∨ *A*~i~) from the definition of *Bel*~\*~, and *Bel*(*A*~0~) = 0 it follows that *Bel*(*A*~0~ ∨ *A*~i~) = *Bel*(*A*~i~), so we get *Bel*(*A*~1~) + ... + *Bel*(*A*~n~) \> 1, a contradiction. So we couldn't have permitted *Bel*(*A*~0~) = 0 after all, so we were not vague over that value.

To show that the Many-Urn model does not validate (M6), assume the possibility space is Ω = {*A*~0~, *A*~1~, *A*~2~, *A*~3~, *A*~4~}, and **K**^\*^ is defined as the following, with *y* = 3. (This example is derived from one in @walley1991a [86]).

> **K**^\*^ = *Cn*({*p*~11~ → (*A*~0~ ∨ *A*~1~), *p*~21~ → (*A*~0~ ∨ *A*~2~), *p*~12~ → (*A*~0~ ∨ *A*~3~), *p*~22~ → (*A*~0~ ∨ *A*~4~)})

For any proposition *A*, *Bel*~\*~(*A*) is given by the following rules.

- If there is no *A*~i~ (i ∈ {1, ..., 4}) such that {*A*~0~, *A*~i~} ⊆ *A*, *Bel*~\*~(*A*) = 0;
- If {*A*~0~, *A*~1~, *A*~2~} ⊆ *A* or {*A*~0~, *A*~3~, *A*~4~} ⊆ *A* and *A*  Ω, *Bel*~\*~(*A*) = 2/3;
- If *A* = Ω, then *Bel*~\*~(Ω) = 1; and
- If *A* satisfies none of the above conditions, *Bel*~\*~(*A*) = 1/3.

It can be seen by inspection that **K**^\*^ is a model for *Bel* in the sense set out here. Note particularly that although (*p*~11~ & *p*~12~) → *A*~0~ ∈ **K**^\*^, there is no subset *S* of *P*~1~ × *P*~2~ such that *S* → ⊥ ∈ **K**^\*^ (as is required by (1´)), nor is there a subset *S* of either *P*~1~ or *P*~2~ such that *S* → *A*~0~ ∈ **K**^\*^. Hence *Bel*~\*~(*A*~0~) = 0. However, Σ*Bel*~\*~(*A*~0~ ∨ *A*~i~) = 4/3, contradicting (M6).

### Summary

Many‑Urn models have a number of nice properties. First, they enable us to remove some of the restrictions of the Single urn models discussed in @sec-0306. Secondly, they preserve the conclusion that precise belief functions should be probability functions. Thirdly, the lower bounds of the belief functions they define satisfy the requirements given by Suppes, Wolfenson and Fine for lower probability functions. However, these functions do not satisfy other intuitively plausible requirements, such as (M6). For this reason I think this approach to modelling probability functions oughtn't be taken.

## Updating {#sec-0308}

So far I have discussed three possible approaches to modelling vague belief functions. The third of these I found reason to reject, but the other two seemed to be live possibilities. I take it that the failure of the positive arguments in @sec-0306 doesn't constitute a negative argument. So this section looks at how credences ought be updated according to these two approaches.

On the many models approach the question is a rather trivial one, as I have already said how each model should be updated. A set of probability functions is updated by updating each function. If an agent's credences are represented by a family of probability functions P , then their credences after receiving evidence *B* should be represented by the family P ~*B*~, defined as follows.

P ~*B*~ = {*Pr*( · \| *B*): *Pr* ∈ P }.

As noted earlier, I take conditional probability as basic, so *Pr*( · \| *B*) is defined even when *Pr*(*B*) is zero. If we adopt a betting analysis of degrees of belief then there is an argument to show this is the only coherent way to update degrees of belief. @alley1991a [297] uses the betting analysis and a condition he calls full conglomerability to get this conclusion. I don't want to rely on Walley's arguments because, for reasons I'll set out in the next section, I don't find full conglomerability a persuasive restriction on updates.

### Updating Shafer Functions

The above I take to follow uncontroversially from what I have already shown. However, when we look at the Single Model Approach, and the associated Shafer functions, which updating approach should be adopted becomes a slightly harder question. One of the difficulties is that Dempster and Shafer themselves adopt an updating rule which seems incoherent.

Recall that for every Shafer function *Bel*~S~ defined on a possibility space Ω, there is an associated mass function *m*: **Po**(Ω) → \[0, 1\]. The Dempster and Shafer (DS) updating rule is easiest to explain in terms of this mass function. When we receive evidence *B*, the possibility space shrinks from Ω to Ω ∩ *B*. The mass of every subset *A* of this space is given by the following formula.





![](media/image6.emf)





The idea is that the mass previously assigned to *C* goes to *C* ∩ *B*. However, the mass that was previously assigned to subsets of ¬*B* cannot be allocated in this way. So we normalise to take Σ*m* back up to 1. This gives the following formula for the conditional degree of belief in *A* given *B*.





![](media/image7.emf)





We can define *Pl*(*A* \| *B*) in a similar way, or just set it to be 1 ‑ *Bel*~S~(*A* \| *B*). If we try and extend the analysis of updating in section 3.4 to the imprecise models, it seems we will get a similar updating rule. Presumably the updating rule will take **K**^\*^ to *Cn*(**K**^\*^ ∩ {*B*}). More directly, it will take ∆^\*^ to ∆^\*^ ∩ *B*. Now instead of having *y* dummy propositions, there will only be *y* · (1 ‑ *Bel*(¬*B*)) such propositions left, as *y* · *Bel*(¬*B*) of them are contained within ¬*B* in the model **K**^\*^.

Now recall how we defined Shafer functions from the single models. For each *p*~i~, we assigned mass 1/*y* to the smallest set *A*~i~ ⊆ *W* such that *p*~i~ → *A*~i~ ∈ **K**^\*^. Now consider what happens when we take the intersection of **K**^\*^ with *B*. In the new model, the smallest set *A*~i~ such that *p*~i~ → *A*~i~ ∈ *Cn*(**K**^\*^ ∩ {*B*}) will be *A*~i~ ∩ *B*. If *A*~i~  ¬*B*, this set will be empty. This in effect means that some dummy propositions will be discarded. For the remaining dummy propositions, we will assign mass 1/( *y* · (1 ‑ *Bel*(¬*B*))) to each *A*~i~ ∩ *B*. This is exactly the same construction as Dempster and Shafer use to create the updated mass function. Hence viewed this way, when we update a Single Model the updated belief function is determined by the DS updating rule.

### The Three Prisoners Problem Again

In @sec-tpp we described the Three Prisoners Problem (TPP). At that stage we only discussed the problem of how to represent prisoner *a*'s initial belief state. Now we want to look at the problem of how to update that prisoner's belief given what the guard says. The situation is entirely symmetric about *b* and *c*, so assume without loss of generality the guard says *b*. Then the only possibilities left are *ab* and *cb*. The mass previously allocated to {*ab*, *ac*} will now be entirely allocated to *ab*, and then every mass allocation will be scaled up because *bc*, which previously received mass 1/3, is now impossible. Hence *m*({*ab*}) = *m*({*cb*}) = 1/2, so *Bel*~S~(*ab* \| *ab* ∨ *cb*) = 1/2.

This is absurd. Originally *a* believes to degree 1/3 that he will be reprieved. However, if the guard says that *b* will be executed, his degree of belief that he will be freed rises to 1/2, and the same happens if the guard says *c*. But the guard is going to say one of them. As Gardner points out, were this reasoning correct *a* should believe to degree 1/2 that he is going to be freed before the guard says anything, since he knows that whatever the guard says that will be his degree of belief.

Formally, what has gone wrong is that our belief rule has breached a condition called finite conglomerability. Let Π be a partition {*B*~1~, ..., *B*~n~} of the possibility space such that for every *B*~i~, *Bel*~\*~(*A* \| *B*~i~) \> *Bel*~\*~(*A*). Then *Bel*~\*~ is not finitely conglomerable. This seems to be a problem because whatever element of Π is true (and we know exactly one of them is true), *Bel*~\*~(*A*) would rise were we to discover it. We will look in the next section at the plausibility of conglomerability more generally. It suffices to note that here, even without the general rule, it seems the result mandated by the DS updating rule is absurd.

### Updating Single Models

If the DS updating rule leads to absurdity, and we update Single Models by using that rule, that model has a difficulty. Fortunately it is one which can be solved. The problem rests with how we interpreted the updated models. In the updated model *Cn*(**K**^\*^ ∩ {*B*}) there are *y* · (1 ‑ *Bel*(¬*B*)) dummy propositions. However, not all of these dummy propositions are created equal.

In the precise case, when we updated on *B*, all of the dummy propositions remaining in the model were on an equal footing. All of the dummy propositions outside *B* had been discovered to be attached to a false proposition. This is in effect what it is for a dummy proposition to be false. However, we learnt nothing new about whether the dummy propositions inside *B* were attached to a false proposition. Hence we increase the 'weight' of each dummy accordingly. In the imprecise case things aren't quite so simple. Consider the model for the TPP set out in 3.6.2, which I'll repeat here.

> **K**^\*^ = *Cn*({*p*~1~  *bc*, *p*~2~  *cb*, *p*~3~  *ab* ∨ *ac*})

When *a* finds out *ab* ∨ *cb* (in other words, when the guard says *b*) he finds out that *p*~1~ is attached to a false proposition. However, while he has found out nothing to discredit *p*~2~, the new information he has *might* be sufficient to make *p*~3~ false. What went wrong with the approach to updating described above was that we lost this information. All dummies which weren't definitely ruled out by the new evidence were treated on a par. What it seems should have been done was to divide the dummies into three categories, those that are definitely out, those that are definitely in, and those that might be in or out. The new evidence *ab* ∨ *cb* puts *p*~1~ into the first of these categories, *p*~2~ into the second and *p*~3~ into the third.

Given this, how do we work out conditional degrees of belief? Or since these degrees will presumably be vague, how do we work out their bounds? To do this we need a more fine-grained taxonomy of what happens to the dummy propositions under conditionalisation. The required taxonomy depends on the proposition whose conditional degree of belief is being evaluated. We'll call this *A*.

> \(i\) α of the dummy propositions definitely go to *A*.    
> \(ii\) β either go to *A* or are excluded.    
> \(iii\) χ either go to *A* or go to ¬*A* or are excluded.    
> \(iv\) δ either go to ¬*A* or are excluded.    
> \(v\) ε definitely go to ¬*A*.    
> \(vi\) φ are definitely excluded.

By excluded I mean that they are ruled out by the new evidence. In the above example, letting *A* = *ab*, α = 0, β = 1, χ = 0, δ = 0, ε = 1, φ = 1. The minimal value for *A* given *B* will be reached if we exclude all the dummies in (ii) and allocate all the dummies in (iii), (iv) and (v) to ¬*A*. The maximal value will be reached when we assign all the dummies in (i), (ii) and (iii) to *A* and exclude those in (iv). So we get the following formulae.

> *Bel*~\*~(*A* \| *B*) = α /(α + χ + δ + ε) *Bel*^\*^(*A* \| *B*) = (α + β + χ) / (α + β + χ + ε)

Now, it follows from the above definitions that the following formulae hold.

> φ / *y* = *Bel*~\*~(¬*B*) = 1 - *Bel*^\*^(*B*)    
> α / *y* = *Bel*~\*~(*A* & *B*)    
> ε / *y* = *Bel*~\*~(¬*A* & *B*)    
> (α + β + χ) / *y* = *Bel*^\*^(*A* & *B*)    
> (χ + δ + ε) / *y* = *Bel*^\*^(¬*A* & *B*)    

Combining these two sets of formulae, we get the following.

> (C1) *Bel*~\*~(*A* \| *B*) = *Bel*~\*~(*A* & *B*) / (*Bel*~\*~(*A* & *B*) + *Bel*^\*^(¬*A* & *B*))    
> (C2) *Bel*^\*^(*A* \| *B*) = *Bel*^\*^(*A* & *B*) / (*Bel*^\*^(*A* & *B*) + *Bel*~\*~(¬*A* & *B*))

These should both have some intuitive force. To make *A* given *B* as unlikely as possible, we make *A* & *B* minimally likely and ¬*A* & *B* maximally likely. The reverse applies for maximising the likelihood of *A* given *B*. Indeed this result has been reached by a number of different authors. (See for example Fagin and Halpern (1991) and the references therein.) More importantly, as is again pointed out by these authors, if *Bel* can be modelled by a Single Model and P is its associated family of probability functions, then the bounds on *Bel*(*A*) we get by conditioning P on *B* are just those given by (C1) and (C2).

So we have some justifications from within the model to use conditionalisation as our updating rule, even if we don't adopt the Many Models approach. We noted that not every family of probability functions can be represented by a Single Model. In particular only the monotone families can be represented. If we are to adopt the Single Model and update by conditionalisation it becomes important to work out whether the updated family can also be represented. It is rather non-trivial to prove, but @fagin1991a have shown that it always can be. That paper contains their proof and references to some other proofs developed independently of theirs.

### Difficulties

So far it seems there is nothing wrong with adopting the Single Model as long as we update by conditionalisation and not by DS updating. However, there is a cloud on the horizon. Updating by just using (C1) and (C2), which from now on I'll call 'FH conditionalisation' after Fagin and Halpern, is not commutative. Updating with respect to *B*~1~ and then with respect to *B*~2~ is not necessarily the same as updating with respect to *B*~2~ and then *B*~1~.

This seems rather odd. After all, it looks like all we've done is conditioned every element of a family of probability functions, and conditionalisation is commutative. The problem arises because the property of being Event Defined is not preserved under conditionalisation. If P is Shafer bound then when we conditionalise every element of it we will get a new Shafer bound family, and its bounds will be given by (C1) and (C2). However, even if the old family was defined by its bounds, the new family is not. This is somewhat easier to see with an example.

Let *A*, *B*, *C* and *D* be exclusive and exhaustive possibilities, with the DS belief function on them defined by *m*(*A*) = *m*(*B*) = 1/4, *m*({*C*, *D*}) = 1/2. Let φ~1~ = *A* ∨ *B*, φ~2~ = *A* ∨ *B* ∨ *C*. Then FH conditionalisation with respect to φ~1~ and then with respect to φ~2~ gives a different result to FH conditionalising with respect to φ~2~ and then with respect to φ~1~. Hence it can't be that each is equivalent to conditionalising with respect to φ~1~ & φ~2~.[^42]

[^42]: This is pointed out by @paris1994a.

The problem can be defined in another way. Recall our definition of *W*(*Bel*~S~) as the largest family of probability functions bounded by *Bel*~S~. Define *Bel*~S~(· \| *B*) by (C1) and (C2). This is a DS belief function, so we can determine *W*(*Bel*~S~( · \| *B*)). We can also defined *W~B~*(*Bel*~S~) as the result of conditionalising every element of *W*(*Bel*~S~) on *B*. For any proposition *A*, the maximal and minimal value of *Pr*(*A*) in *W*(*Bel*~S~( · \| *B*)) and *W~B~*(*Bel*~S~) will be the same. However, the following theorem cannot be strengthened to be an equality.

> *W*(*Bel*~S~( · \| *B*)) ⊇ *W~B~*(*Bel*~S~)

When *W*(*Bel*~S~( · \| *B*)) is strictly larger *W~B~*(*Bel*~S~), we lose information by just modelling the belief state by the boundaries of every proposition. Hence the Single Model approach, even in its most plausible form, has real difficulties. If we insist on modelling agents this way we will lose information about their belief states. This loss of information may lead to bizarre results, like the non-commutativity of updating.

### Figures

These results are not unknown. Fagin and Halpern themselves point out the failure of commutativity under their form of conditionalisation, and supporters of the DS updating rule are aware of its odd consequence in the TPP. Why then are these rules kept? In large part it is because our theoretically preferred model, representing belief states by sets of probability functions and updating them by conditionalisation, has such high computational costs. Given this, some results of @vanfraassen1990a might be important, because they point the way to a reduction in these costs.

Rather than taking sets of probability functions as his given, van Fraassen takes bounds on expected values of gambles. This gives us more information than just looking at the bounds on bets. For example, we saw above that there was no way by looking just at its bounds to represent the distinct information contained in P  = {*Pr*: *Pr*(*A*~1~) = *Pr*(*A*~2~)}. However, we can represent this by looking at bounds on gambles. In this case the gamble that pays 1 if *A*~1~ & ¬*A*~2~, and -1 if ¬*A*~1~ & *A*~2~ will have expectation 0, whereas if P was the set of probability functions its expectation would be vague over the range \[-1, 1\]. We can't do everything we want with just expectation, but as van Fraassen shows we can do quite a bit.

In fact we don't even need to take bounds on gambles as basic. Rather, we can just take as our basic information that certain gambles have non-negative expectation. Every time we say that a gamble has non-negative expectation according to an agent's belief, we rule out some probability functions as being possible members of that agent's representor (i.e. those functions according to which the gamble has negative expectation). Now, say P is the set of all those probability functions such that for each function in P each gamble in a finite class *G* has non-negative expectation. If this occurs, say P is a *G*'s figure. Any family which is a figure of some finite class of gambles is a figure. The complexity of a figure is the size smallest class for which it is a figure. Van Fraassen gives a constructive proof of the following theorem.

> *Figure Theorem*    
> If P is a figure of complexity *n*, then conditionalising P with respect to *B* results in a figure of complexity at most *n* + 2.

The proof of this is summarised in @sec-090202. This shows that once we have represented P as the figure of a class of gambles there is a simple algorithm for updating it. Given this, some of the motivation for using incoherent but efficient updating rules should be removed.

### Gilboa and Schmeidler's Updating Rule

There are other approaches in the literature to updating vague degrees of belief. The theory of @gilboa1993a is based around *v*-functions, which are called non-additive probabilities. Again, Ω is a possibility space and propositions are subsets of Ω. There are only three restrictions on *v*, which are listed below.

> *v*(∅) = 0; *v*(Ω) = 1; and If *p*  *q* then *v*(*p*) ≤ *v*(*q*).

Returning to our prisoner *a*, the function defined by *v*(*p*) = *Bel*~S~(*p*) for the DS belief function defined in @sec-tpp. would be a non-additive probability. There are a continuum of updating methods for non-additive probabilities, we'll just consider the extremum cases. These are described by Gilboa and Schmeidler as 'optimistic' and 'pessimistic' rules. The pessimistic rule we've already seen, it is just the Dempster-Shafer rule. The optimistic rule is defined by the following formula.

> *v~r~*(*p*) = *v*(*p* & *r*) / *v*(*r*).

The TPP seems to be just the right place to apply optimistic and pessimistic rules. The proposition that *a* is to reprieved is *ab* ∨ *ac*. Let this be *p*, and let *r* be the proposition that the guard says *b* is to be executed. As we noted above, on the pessimistic view *v~r~*(*p*) = 1/2. On the optimistic view *v~r~*(*p*) = 1. Of course this wouldn't change if the guard had said that *c* rather than *b* was to be executed. Optimism is all well and good, but this looks more like fanciful thinking!

We might have been a little unfair to Gilboa and Schmeidler. They formulate their rule in terms of preferences over acts rather than degrees of belief. Still, should *a* have a choice, then, under the optimistic rule, he should accept a gamble which pays \$1 if he's reprieved and has an arbitrarily high penalty if he isn't. (Of course he already faces such a penalty, so decision theory starts to break down here). Again this seems like optimism gone mad. None of the axioms Gilboa and Schmeidler employ look that implausible, yet their results are absurd. It would be an interesting research program to see where the absurdity creeps in, but this isn't our research program.

### Summary

At the start of this section there were two approaches to modelling which seemed plausible. For one of these, the Many Models approach, it is trivial to give an updating rule, because we have already given an updating rule for the precise case. For the Single Model approach, we had to do more work to find an updating rule which was even plausible. However, once we did this we found that updating a belief function can take us from a function which can be accurately modelled (i.e. one that is event defined) to one that cannot. Hence this approach to modelling seems mistaken. The section concluded with a summary of two other discussions of updating vague belief states.

## 3.9 Conglomerability and Lower Envelopes {#conglomerability-and-lower-envelopes}

Walley (1991: 294‑327)[^43] argues that we should accept as a rationality requirement a principle he calls conglomerability. From this he concludes that (i) probability functions should be countably, not just finitely, additive and (ii) we can't always represent vague epistemic states by sets of probability functions. I think (i) is questionable and (ii) is false, but in each case his argument for these conclusions does not go through. The principle of conglomerability he relies on is so close to principles which are demonstrably not principles of rationality that we shouldn't adopt it as an axiom of rationality. If it falls out from some uncontroversial principles of rationality that our epistemic states should be conglomerative, all well and good. However, arguments designed to show that this principle is intuitively compelling face a fundamental difficulty which Walley does little to overcome.

[^43]: All page references in this section to this book, unless otherwise stated.

### 3.9.1 Conglomerability

In Walley's theory, like many others, the basic conception is the expected value of a gamble. He doesn't require this to be a precise number, so we can introduce vague or imprecise degrees of belief. For example we might be able to say no more than that a certain gamble is more valuable than \$10 and less valuable than \$20. Saying this does not commit us to saying there is some number *x* in \[10, 20\] such that the gamble is worth precisely *x* dollars.

Under fairly minimal assumptions about rationality, *viz* that if gamble α is preferred to β, then α ‑ *c* will be preferred to β ‑ *c*, for some constant *c*, all the information about an agent is contained in their set of desirable gambles. For our purposes, nothing is lost if we merely talk about greatest lower bounds on the expected value of a gamble α, or what Walley calls lower previsions. [^44]

[^44]: For most of his book Walley works with lower previsions. However, as he notes (160, 616) the class of desirable gambles is slightly more informative, particularly with regard to conditional expectations. Why just lower prevision, rather than upper and lower previsions? Well, if *x* is the lower prevision of -α, then ‑*x* should be the upper prevision of α. Hence we only need the lower boundaries. The lower boundaries are also easier to compare with other theories, such as Shafer's.

What I'll call a propositional gamble is a gamble which takes value 1 at some worlds and 0 at all the rest. These are important in a theory like Walley's for many reasons, not least being that the probability of a proposition is the expectation of a propositional gamble. For any propositional gamble define its associated proposition to be the set of worlds at which it takes value 1. Conversely the gamble which has payout 1 iff a certain proposition is true (and nothing otherwise) is the gamble on that proposition.

For any gamble α, and propositional gamble , we can define the conditional gamble α. As might be clear from the notation, for all worlds *w*, α(*w*) = α(*w*) · (*w*). So at the worlds where 's associated proposition is true, α has the same payouts as α, and in all other worlds it has zero payout. So in a natural enough sense this is a conditional gamble. Now we can define the conglomerability axiom.

Let Π be a partition of the possibility space. Every element *q*~i~ of the partition has an associated gamble ~i~. The principle of conglomerability is that if for all i, α~i~ is a desirable gamble, then α is a desirable gamble. Walley justifies this principle as follows. (Slight changes to reflect consistency with my notation have been made.)

> The conglomerative principle ... requires a type of consistency between current beliefs and current dispositions to update beliefs. It can be justified as follows. Suppose that α~i~ is desirable for all *q*~i~ in Π. This means that You will be willing to accept α after You observe some set in Π, whatever set You observe. Knowing this, You should be prepared to accept α now. (294)

So far we have not mentioned the cardinality of Π. Walley claims that this principle should hold even if Π is an infinite partition. The core of his argument for this is the following line: "Indeed the cardinality of Π seems to be irrelevant in the arguments given to support \[this\] principle." (295). If this is right then the principle is in very bad shape, for we'll see below that it is untenable given a faily simple assumption when Π is an infinite partition.

### 3.9.2 Conglomerability and Finite Additivity

Walley notes that if we accept this principle we have to insist on countable additivity of probability functions. The argument turns on the following example, which is fairly well known (321). (See for example de Finetti (1972: 98‑100)). Let Ω (the possibility space) be the set of non-zero integers. For ease of exposition, we'll say *x* is a random variable ranging over the non-zero integers. Identify Ω with the product space Θ × Π by identifying each integer *n* with the pair (sign(*n*), \|*n*\|). The elements of Π are the pairs {-*m*, *m*} for all natural *m*. For simplicity call this the set *q~m~*. Hence the elements of Θ are simply the signs {--, +}. For simplicity we'll call the proposition that *x* is positive +, and that it is negative --.

Say we have a probability function *Pr* on Ω with the following properties. For any natural *m*, *Pr*(*x* = *m* \| +) = 2^--*m*^. On the other hand, *Pr*( · \| --) is a finitely additive distribution, such that for any natural *m*, *Pr*(*x* = ‑*m* \| --) = 0. Finally, *Pr*(+) = *Pr*(--) = 1/2. Obviously *Pr* itself, as well as its conditionalisation on --, is merely finitely additive.

Now let α be a gamble such that α(*x* = *m*) = ε for all positive *m*, and α(*x* = *m*) = -1 for all negative *m*. (We're assuming here that all bets with positive expected value according to *Pr* are desirable, and all those with negative expected value are not desirable). Assume ε is an arbitrarily small, but not infinitesimal, positive value. Now α is not a desirable gamble, since its expected payout is (ε - 1)/2. However, for any element *q*~i~ of Π, α~i~ will be desirable. The reason is that *Pr*(*x* = i \| *q*~i~) = 1, and *Pr*(*x* = --i \| *q*~i~) = 0. Hence the expected value of the bet is now ε · 2^-i^, which is positive. So *Pr* is not conglomerable.

de Finetti's reaction to this case is rather disconcerting. He shrugs it off as one of those odd results we get when dealing with infinites. Now while we do often get odd results when working with infinite sets, this in itself hardly licences an 'anything goes' approach. We should have a principled reason why this particular odd result is acceptable. If an odd result is forced upon us by a principle which is more compelling than its negation, that result might be acceptable. de Finetti's approach could only work if the intuitive appeal of allowing merely finitely additive probability distributions was greater than the intuitive appeal of conglomerability. And without some more work (part of which is reported below) this can't get off the ground.

### 3.9.3 Conglomerability and the Many Models Approach

My main interest in conglomerability is in a possible argument from conglomerability to an objection to the many models approach. It turns out that if you take conglomerability as your main criterion on an updating rule, and take previsions to be primitive as Walley does, then there are epistemic states which seem reasonable enough but which cannot be represented by sets of probability functions. Or more precisely, they can't be represented by sets of countably additive probability functions, despite being conglomerable. Since for precise probability functions, conglomerability entails countable additivity, this will be a difficulty if conglomerability is a plausible constraint on epistemic states.

This is all consistent with the results of Smith (1961) and Williams (1976) reported above. The state here can be represented by sets of non-countably additive probability functions, but they cannot be represented by conglomerable probability functions. Indeed, there are no conglomerable probability functions which dominate the state in this sense. A probability function *Pr* dominates a lower prevision [*P*]{.underline} iff for all gambles α, the expected value of α according to *Pr*, which I write as *E~Pr~*(α), is greater than or equal to [*P*]{.underline}(α). Dominating probability functions are important because the way Walley's previsions are represented according to our approach is by the set of probability functions which dominate them.

Before we look at the supposed counterexample, we need one more bit of terminology. We discussed above the vacuous epistemic state, the one represented by the set of all probability functions. In Walley's terminology, the vacuous prevision is where the lower prevision of every gamble is its infimum payout. This wouldn't make sense unless every gamble has a lowest payout, and Walley helps himself to that assumption, apparently to facilitate exposition (58). However, as we'll see below, the assumption does much more work than this.

Here's the example which Walley claims shows the Many Models Approach can't work. Let *P*~1~ be the finitely additive probability function *Pr* described in the previous section, with Ω, Θ and Π defined the same way. Let *P*~2~ be a finitely additive probability function such that *P*~2~(*x* = --*m*) = 3^‑*m*^ for any natural *m*, *P*~2~(*x* \> 0) = 1/2 and *P*~2~(*x* = *m*) = 0 for any positive *m*. Now, neither *P*~1~ nor *P*~2~ are conglomerable, however their lower envelope [*P*]{.underline} is conglomerable. [*P*]{.underline} is defined in the following way. For all gambles α, [*P*]{.underline}(α) = *min*(*P*~1~(α), *P*~2~(α)). The conditional prevision[^45] [*P*]{.underline}( • \| *q*~i~), where *q*~i~ is any element of Π, is vacuous. To see this, note that *P*~1~(α~i~) = α(*x* = i) and *P*~2~(α~i~) = α(*x* = -i), so [*P*]{.underline}(α~i~) = inf(α~i~). Indeed, [*P*]{.underline} satisfies all of the coherence requirements that Walley discusses.

[^45]: Conditional previsions can be technically defined in the following way. [*P*]{.underline}(α \| *q*) = [*P*]{.underline}(α), where α is any gamble (previsions are defined primarily on gambles and only derivatively on propositions) and *q* is the associated proposition of propositional gamble .

The next step is to show that no conglomerable probability function dominates [*P*]{.underline}. Any such probability function will, by virtue of a more general theorem, be of the form *Pr* = λ*P*~1~ + (1 ‑ λ)*P*~2~, for λ ∈ \[0, 1\]. Now it is shown that for any value of λ, *Pr* is not conglomerable. For λ = 0 the proof is simply the same as for showing the finitely additive function described in the previous section is not conglomerable. For λ \> 0, the proof is slightly more complex.

Note that as *n* → ∞, *Pr*(*x* = *n*) / *Pr*(*x* = -*n*) = 2λ^-1^ (1 -- λ) (2/3)^n^ → 0, so *Pr*(*x* = *n* \| *q*~n~) → 1. Choose *N* large enough so that *Pr*(*x* = *n* \| *q*~n~) ≥ 1 - λ/3 for *n* \> *N*. Then *P*~1~(*x* \> --*N*) = 1/2 and *Pr*(*x* \> --*N*) = λ*P*~1~(*x* \> --*N*) + (1 -- λ)*P*~2~(*x* \> --*N*) ≤ λ/2 + (1 -- λ) = 1 -- λ/2. However, when *n* \< *N*, *Pr*(*x* \> --*N* \| *q*~n~) = 1, and otherwise *Pr*(*x* \> --*N* \| *q*~n~) ≥ 1 -- λ/3. Hence for all elements *q*~i~ of Π, *Pr*(*x* \> --*N* \| *q*~n~) \> *Pr*(*x* \> ‑*N*), so *Pr* is not conglomerable.

To summarise, the epistemic state represented by [*P*]{.underline} satisfies all the coherence requirements that seem plausible. However, if it is a requirement on functions generated by precise models that they be conglomerable, and hence countably additive, that state cannot be represented by a set of probability functions. That is, it can't be modelled by a set of precise models. Hence we should take a different approach to modelling than the Many Models approach, perhaps the approach based on previsions suggested by Walley.

### 3.9.4 The Two Envelope Paradox

There are several problems with the principle of conglomerability. The most serious is derived from some recent work by Broome (1995) and Arntzenius and McCarthy (1997) on the two envelope paradox. It turns out that if we drop Walley's restriction to bounded gambles (i.e. gambles with maximal and minimal payouts) the principle of conglomerability leads to inconsistency. That is, it says of two gambles whose sum is uniformly negative that they are each desirable. To get this result we do not need to postulate gambles with infinite payouts, all payouts will still be finite, but we do need gambles with infinite expected payouts.

As a small aside, it might be noted that these gambles, which have all finite payouts and infinite expected payouts, are rather odd. We can guarantee that the actual payout will be less than the expected payout, which casts some doubt on the principle that we should value gambles according to their expected payout. The argument here will only use the principle that we should value gambles according to their expected payout when that expectation is finite, which seems a more plausible principle.

The two envelope paradox can be set out as follows. One of two envelopes is chosen at random, and an amount of money *x* is placed in it. The game arbiter calculates *x* using a chance device such that *Ch*(*x* = 100 · 2^*n*^) = 2^*n*^ / 3^*n* + 1^ for any integer *n*, and *Ch*(*x* = *k*) = 0 otherwise[^46]. Then 2*x* is placed in the other envelope. The contestant is then given one envelope and asked, before they look inside the envelope, whether they would pay \$10 to swap it for the other one. Assume the contestant knows the method that has been used to calculate the money to be placed in each envelope.

[^46]: A simple chance process would generate this distribution. Start with \$100 in the envelope. Find a fair die tossing mechanism, and keep throwing a 6-sided die until either a 1 or 6 lands face up. After every throw which doesn't land 1 or 6, double the amount of money in the envelope.

Now at first (and at last I hope) it looks patently absurd that this gamble could be desirable. As far as the contestant knows, there is a complete symmetry between the envelopes. So how could it possibly be worthwhile to trade? Put more forcibly, if the trade is desirable, then presumably it is worthwhile to swap back, again for \$10, since the contestant will now be in the same epistemic position. But the net effect of these two trades is to incur the sure loss of \$20, and that can't be rational. By the normal standards of decision theory this is the worst of all possible mistakes, to incur a sure loss[^47].

[^47]: Scott and Scott (1997) object to the conclusion that, if you'd trade once you'd keep on trading on the ground that the reasoning supposedly relies on the contestant in later rounds of trading forgetting the results of their earlier calculation. However, the original argument for trading only goes through if you, in effect, ignore how you would feel were you to be holding the other envelope, so I don't see how this objection works. If you'd ignore the dynamic possibilities of the other position once, there's no reason to not keep on ignoring them.

    It might be noticed here that the contestant is making the kind of assumptions which I showed in @sec-chap-2 to be unreasonable. In particular they seem to not be thinking about future trade. But some kind of blindness like this will be necessary if we are to hold a betting analysis of degrees of belief, as all the writers in this field do.

However, look what happens when we bring the principle of conglomerability to work on the matter. Let *y* be the amount that's in the envelope the contestant is holding, and let Π be a partition {*q*~100~, *q*~200~, ... } where *q*~i~ represents the proposition *y* = i. If i \> 100, then the expected value of trading given *q*~i~ can be easily calculated to be 11i / 10, and if i = \$100 the expected value of trading given *q*~i~ is \$100. Hence no matter what is in the contestant's envelope, if they knew what it was they'd happily give up that and ten dollars for the other envelope. So by the principle of conglomerability, they should be prepared to give up their envelope and the ten dollars now. That's absurd, so I take it the principle of conglomerability fails here.

Call the envelopes *A* and *B*. The core problem, as Arntzenius and McCarthy explicitly point out, is that there are distinct partitions of the possibility space Π~1~ and Π~2~ such that the expected worth of *A* is higher (at least \$20 higher) given every element of Π~1~ and the expected value of *B* is higher (again at least \$20 higher) given every element of Π~2~. Given this it's clear that unrestricted uses of the principle of conglomerability will lead to inconsistency.

Should this be a problem? After all, Walley did explicitly restrict himself to bounded gambles. I think it is a problem. Consider Walley's short argument for extending the principle of conglomerability to infinite partitions. He simply noted that we didn't appear to use the cardinality of the partition in the argument for conglomerability. Well we didn't appear to use the fact that gambles are bounded either, but we must have used one of them, or else we would have 'proved' an inconsistent principle. Alternatively, if we really have used neither of these facts, the two envelope case shows that there's something wrong with Walley's proof. Unfortunately it is little help in saying what precisely is wrong, but this alone is no reason to hang onto the principle.

Even if it turns out that the principle is consistent when applied only to bounded gambles (and I have no reason to think it is inconsistent in this setting) there remains a philosophical difficulty. We know that when gambles can be unbounded we should not follow the principle of conglomerability. This alone seems enough motivation to deny that the principle is sufficiently compelling to count as an axiom when gambles are bounded. If cases which are roughly alike should be treated in a roughly alike manner, and the principle of conglomerability is provably mistaken when dealing with unbounded gambles, it seems implausible that it could be compelling enough to count as an axiom when we move to bounded gambles. So I conclude that Walley shouldn't have adopted conglomerability as an axiom. If he did not, he would not have been able to prove that reasonable probability functions should be countably additive, and hence that there are no reasonable probability functions which dominate [*P*]{.underline}.

### 3.9.5 Intuitions

The main motivation for the principle of conglomerability is its intuitive plausibility. However, it isn't at all clear that intuitions in this area should be trusted. Many of the intuitive arguments for conglomerability (or the 'sure-thing' principle as it is more commonly known) seem to be equally good arguments for Probabilistic Disjunctive Syllogism (PDS). This is the following argument, where *A*, *B* and *C* are propositions.

*PDS*: *Pr*(*A* \| *B*) \> *e*, *Pr*(*A* \| *C*) \> *e*, *Pr*(*B* ∨ *C*) = 1  *Pr*(*A*) \> *e*.

We can run the intuitive argument easily enough. Assume *Pr* represents your epistemic state. Since *B* ∨ *C* is known, you can assume that you'll find out one or other of them. In either case your degree of belief in *A* will be greater than *e*. So by Reflection, it should be greater than *e* now. But PDS is clearly invalid as the following example (due to Gillman (1992)) shows.

Assume that a certain deck has only three cards, the ace of hearts, the ace of spades and the two of clubs. A hand of two cards is dealt from it by a fair chance mechanism (i.e. each possible hand is equally likely to be dealt). Let *A* be the proposition that this hand contains both aces, *B* be that it contains the ace of spades, and *C* that it contains the ace of hearts. Clearly enough, *Pr*(*A* \| *B*) = *Pr*(*A* \| *C*) = 1/2, *B* ∨ *C* is known, and *Pr*(*A*) = 1/3, violating PDS. The problem is that we moved too quickly between a proposition being true and it being the evidence on which you conditionalise. The following can't both be true (assuming you're reasonable).

\(i\) You will find out *B* only, or you will find out *C* only.

\(ii\) *Pr*(*A* \| You find out *B*) = *Pr*(*A* \| You find out *C*) = 1/2.

The intuitive argument for PDS assumed that (i) and (ii) both held in cases like this, which seems to have been the mistake. I only bring this case up to note that intuitions in this area are notoriously unreliable. Many seemingly reasonable people fall for traps like PDS, even though it can be shown with simple cases like this to be a mistake. I suspect that conglomerability *vis a vis* infinite partitions is another principle which, although intuitively plausible at first, is not acceptable on reflection.

### 3.9.6 Summary

Walley argued that we should adopt conglomerability as a coherence constraint on reasonable models. If we do this there are seemingly reasonable vague epistemic states which cannot be represented by sets of reasonable models, refuting our approach. However, the principle of conglomerability can easily lead to inconsistency. I argued that this is sufficient reason to be at least open-minded about it in contexts where it is consistent. Hence Walley's refutation does not work.

I might be able to generate the problem again if I accepted countable additivity for other reasons as a constraint on probability functions. In @sec-chap-5 I'll look at some arguments for and against this constraint, with an eye to seeing what problems this will imply for our models.

## 3.10 Real-Valued Degrees of Belief {#real-valued-degrees-of-belief}

It was essential to my proofs earlier that if *Bel* was precise it should be a probability function that its range was the rationals. This assumption is unduly restrictive and ought be discharged. The most natural way to do this is by defining real-valued degrees of belief in terms of inequalities. Again these definitions are all made relative to some finite field of propositions; I'll take that qualification as implicit in all that follows.

Clearly *Bel*(*A*) = *r* and *r* \> *x / y* (for integers *x*, *y*) should entail *Bel*(*A*) \> *x / y*. Similarly *Bel*(*A*) = *r* and *r* \< *x / y* should entail *Bel*(*A*) \< *x / y*. I'll assume that these entailments hold, and collectively they give a complete account of the meaning of *Bel*(*A*) = *r* for irrational *r*. As noted above, despite its quantitative representation, we can give a qualitative account of the meaning of *Bel*(*A*) \> *x / y*. So this theory is again reductive, it says the meaning of the quantitative expression *Bel*(*A*) = *r* can be explained in purely qualitative terms.

Our task now is to work out coherence constraints on real-valued degrees of belief. That is, when are sets of expressions {*Bel*(*A*) = *r*~1~, *Bel*(*B*) = *r*~2~, ...} coherent. I assume that the set includes an expression for the value of *Bel*(*A*) for every *A* in the field under consideration. We explicate each element of this set as an infinite set of inequalities, and hence the original set is explicated as the union of each of these original sets. The coherence constraint I postulate is just that there be a model for any finite subset of this infinite set of inequalities.

The restrictions on models are just those proposed in earlier sections. Indeed, the models used here are very similar to the 'single models' proposed as a representation of imprecision in @sec-0306. The idea is that there is a closed set of propositions **K**^\*^ on a possibility space *W* × *P* which satisfies the following three conditions:

\(1\) (*S* ⊆ *P* & *S*^\*^ ∈ **K**^\*^) → *S* = *P*

\(2\) Iff *Bel*(*A*) ≥ *x / y* then ∃*S*: ((*S* ⊆ *P* & \|*S*\| = *x*) & (*S*^\*^ → *A*^\*^ ∈ **K**^\*^))

\(3\) Iff *Bel*(*A*) ≤ *x / y* then ∃*S*: ((*S* ⊆ *P* & \|*S*\| = *x*) & (*A*^\*^ → *S*^\*^ ∈ **K**^\*^))

Again, I use '→' for material implication. The number of elements of *P* is *y*. From these constraints it can be deduced that *Bel* must be a probability function. Because the proofs are so similar to those appearing in appendix 3A for the case where *Bel* takes rational values, I won't give all of them. Rather I'll just illustrate how they all work by giving the proof that for disjoint *A*, *B*, *Bel*(*A*) + *Bel*(*B*) ≥ *Bel*(*A* ∨ *B*). This proof can easily be 'reversed' to show *Bel*(*A*) + *Bel*(*B*) ≤ *Bel*(*A* ∨ *B*), and hence *Bel*(*A*) + *Bel*(*B*) = *Bel*(*A* ∨ *B*). The proof may be easier to follow after reading appendix 3A where some steps are set out in more detail.

Assume *Bel*(*A*) + *Bel*(*B*) \< *Bel*(*A* ∨ *B*). Then there is some integer *y* satisfying *Bel*(*A*) + *Bel*(*B*) \< *Bel*(*A* ∨ *B*) ‑ 1/3*y*. Hence there are integers *x*~1~, *x*~2~ and *x*~3~ such that the following four inequalities hold:

*Bel*(*A*) \< *x*~1~ / *y*;

*Bel*(*B*) \< *x*~2~ / *y*;

*Bel*(*A* ∨ *B*) \> *x*~3~ / *y*;

*x*~1~ + *x*~2~ \< *x*~3~

So by conditions (2) and (3) there are sets *S*~1~, *S*~2~ and *S*~3~ satisfying the following conditions.

*S*~1~ ⊆ *P* and \|*S*~1~\| = *x*~1~ and *A*^\*^ → *S*~1~^\*^ ∈ **K**^\*^.

*S*~2~ ⊆ *P* and \|*S*~2~\| = *x*~2~ and *B*^\*^ → *S*~2~^\*^ ∈ **K**^\*^.

*S*~3~ ⊆ *P* and \|*S*~3~\| = *x*~3~ and *S*~3~^\*^ → (*A* ∨ *B*)^\*^ ∈ **K**^\*^.

Since *A* and *B* are disjoint, *S*~1~ and *S*~2~ must be disjoint. But since *x*~3~ \> *x*~1~ + *x*~2~ there is at least one element of *S*~3~ which is neither an element of *S*~1~ nor *S*~2~. Call that element *p~i~*. Since *S*~3~^\*^ → (*A* ∨ *B*)^\*^ ∈ **K**^\*^, {*p~i~*}^\*^ → (*A* ∨ *B*)^\*^ ∈ **K**^\*^. As the material implication can be contraposed, from *A*^\*^ → *S*~1~^\*^ ∈ **K**^\*^ it can be inferred that ¬*S*~1~^\*^ → ¬*A*^\*^ ∈ **K**^\*^, and similarly ¬*S*~2~^\*^ → ¬*B*^\*^ ∈ **K**^\*^. Now since *p~i~* is both an element of ¬*S*~1~ and ¬*S*~2~, it follows that {*p~i~*}^\*^ → ¬*A*^\*^ ∈ **K**^\*^, and {*p~i~*}^\*^ → ¬*B*^\*^ ∈ **K**^\*^. Hence {*p~i~*}^\*^ → ((*A* ∨ *B*) & ¬*A* & ¬*B*)^\*^ ∈ **K**^\*^. Since the consequent is a contradiction, ¬{*p~i~*}^\*^ ∈ **K**^\*^, contradicting (1). Hence the assumption must be false. Similar proofs show that *Bel* must obey all the other axioms of the probability calculus if it is to be coherent.

This style of definition might be incapable of being used to explain what we mean by saying an agent's degree of belief in some proposition takes a particular infinitesimal value. Indeed, I'm not at all sure what we could mean by such an expression. If we can make sense of assuming there is an urn with some non-standard number of balls in it, each of which is equally likely to be chosen, then we can explicate infinitesimal degrees of belief without recourse to a limiting procedure. If, however, we are dubious about the sense of that then recourse to limits won't save the infinitesimals. One way of making sense of these lotteries with infinitely many outcomes might be by moving away from urns and towards natural processes like atom decay or darts being tossed at a dartboard. This is an area that demands more space than it can be afforded here.

## Appendix 3A Proof of Theorem 3.3.1 {#appendix-3a-proof-of-theorem-3.3.1}

In the proofs I will use *S* (occasionally subscripted) to refer to a subset of *P*.

If  *A* then by closure *A* ∈ **K**, hence *Bel*(*A*) = 1. Hence (T1). Assume ¬*A* is a theorem (i.e.  ¬*A*) and *Bel*(*A*) = *x / y*. Then there is an *S*^\*^ such that \|*S*\| = *x* and *S*^\*^  *A*^\*^ is in **K**^\*^. Since **K**^\*^ is closed, this implies ¬*A*^\*^ and hence ¬*S*^\*^ are in **K**^\*^. By (L3) this implies *S* is the null set, i.e. \|*S*\| = 0. Hence *Bel*(*A*) = 0, i.e. (T2) holds.

Assume *Bel*(*A*) = *x* / *y* and *Bel*(*B*) = *z* / *y*. It follows that there are *S*~1~ and *S*~2~ such that *S*~1~^\*^  *A*^\*^ and *S*~2~^\*^  *B*^\*^ are in **K**^\*^, \|*S*~1~\| = *x* and \|*S*~2~\| = *z*. It follows from (L1) and the way *S*~1~ and *S*~2~ are defined that *S*~1~^\*^ ∨ *S*~2~^\*^  (*S*~1~^\*^ ∪ *S*~2~^\*^) and *S*~1~^\*^ & *S*~2~^\*^  (*S*~1~^\*^ ∩ *S*~2~^\*^). We can tell from (L1) that if any two subsets of *P*, say *S*~3~ and *S*~4~, are such that *S*~3~  *S*~4~ then *S*~3~ = *S*~4~. Finally, if two sets are subsets of *P* then their intersection and union are also subsets of *P*. By the transitivity of material equivalences, it follows that *A*^\*^ ∨ *B*^\*^  (*S*~1~^\*^ ∪ *S*~2~^\*^), hence *Bel*(*A* ∨ *B*) = \|*S*~1~ ∪ *S*~2~\| / *y*. Similarly *A*^\*^ & *B*^\*^  (*S*~1~^\*^ ∩ *S*~2~^\*^), so *Bel*(*A* & *B*) = \|*S*~1~ ∩ *S*~2~\| / *y*. We know from set theory that \|*S*~1~\| + \|*S*~2~\| = \|*S*~1~ ∪ *S*~2~\| + \|*S*~1~ ∩ *S*~2~\|. Dividing both sides of this equation by *y* and substituting terms which we have already shown to be identical gives us (T3).

If *A*  *B* then *A* → *B* is a theorem. Assume *Bel*(*A*) = *x / y* and *Bel*(*B*) = *z / y*. As above, it follows that there are *S*~1~ and *S*~2~ such that *S*~1~^\*^  *A*^\*^ and *S*~2~^\*^  *B*^\*^ are in **K**^\*^, \|*S*~1~\| = *x* and \|*S*~2~\| = *z*. Since **K**^\*^ is closed it contains *A*^\*^ → *B*^\*^ and hence *S*~1~^\*^ → *S*~2~^\*^, or equivalently, ¬(*S*~1~^\*^ & ¬*S*~2~^\*^). Assume *p*~i~^\*^ is in *S*~1~^\*^ and not in *S*~2~^\*^. Since *p*~i~^\*^  *S*~1~^\*^ & ¬*S*~2~^\*^, and ¬(*S*~1~^\*^ & ¬*S*~2~^\*^) is in **K**^\*^, it follows that ¬*p*~i~^\*^ is in **K**^\*^. By (L1), ¬*p*~i~  *P* / {*p*~i~}, so (*P* / {*p*~i~)) ^\*^ is also in **K**^\*^. However, this contradicts (L2), which says that the only subset Θ of *P* such that Θ^\*^ is in **K**^\*^ is *P*. Hence there is no such element *p*~i~, so *S*~1~ is a subset of *S*~2~. Hence *x* ≤ *z*, so *Bel*(*A*) ≤ *Bel*(*B*), as required for (T4).

We have actually proved something more general than (T4). Since all that was used was that **K**^\*^ contains ¬(*A*^\*^ & ¬*B*^\*^), it follows that whenever *A* → *B* ∈ **K**, then *Bel*(*A*) ≤ *Bel*(*B*), provided the agent is rational.

(T5) follows immediately from (T4). Since *A*  *T*, where *T* is a tautology, and Bel(*T*) = 1, Bel(*A*) ≤ 1. And since ⊥  *A*, where ⊥ is the falsum, and Bel(⊥) = 0, 0 ≤ Bel(*A*).

From the definitions of degree of belief, it would not be contradictory to say that (T6) failed to obtain. The reason is that *Bel*(*A*) = *x / y* just means that there is some set *S* of cardinality *x* such that *Bel*(*S*) = *Bel*(*A*), not that all sets *S* satisfying *Bel*(*S*) = *Bel*(*A*) have this cardinality. This might make us question the use of '=' signs when discussing *Bel*( · ). Fortunately, however, we can prove that it is a requirement of coherence that (T6) holds. Assume that it doesn't. So there are sets *S*~1~ and *S*~2~ of different cardinality such that the agent believes both *A*^\*^  *S*~1~^\*^ and *A*^\*^  *S*~2~^\*^ and hence by closure *S*~1~^\*^  *S*~2~^\*^. Let *S*~3~ be the set (*S*~1~ ∪ *S*~2~) / (*S*~1~ ∩ *S*~2~). Since *S*~1~ and *S*~2~ are of different cardinality *S*~3~ is non-empty. From *S*~1~^\*^  *S*~2~^\*^ it follows that the agent believes ¬*S*~3~^\*^, and hence by (L3) that *S*~3~ is empty. This contradicts our assumption, so *S*~1~ and *S*~2~ must be of the same cardinality, as required for (T6).

## Appendix 3B Proof of Theorem 3.6.1 {#appendix-3b-proof-of-theorem-3.6.1}

I am assuming *Bel* is such that there is a model satisfying (1), (3) and (4), and trying to prove the lower bound on *Bel* is a *Bel*~S~ function.

\(1\) (*S* ⊆ *P* & *S*^\*^ ∈ **K**^\*^) → *S* = *P*

\(3\) Iff *Bel*(*A*) ≥ *x / y* then ∃*S*: ((*S* ⊆ *P* & \|*S*\| = *x*) & (*S*^\*^ → *A*^\*^ ∈ **K**^\*^))

\(4\) Iff *Bel*(*A*) ≤ *x / y* then ∃*S*: ((*S* ⊆ *P* & \|*S*\| = *x*) & (*A*^\*^ → *S*^\*^ ∈ **K**^\*^))

For any *p~i~* ∈ *P*, let *A~i~* be the strongest proposition such that *p~i~*^\*^ → *A~i~*^\*^ ∈ **K**^\*^. That such a proposition exists, and is an element of Γ, is guaranteed by the fact that **K**^\*^ is finite and closed under entailments, and that  Γ is a field, so if *A* ∈ Γ and *B* ∈ Γ then *A* & *B* ∈ Γ. I construct a *Bel*~S~ function by constructing its associated mass function *m* as follows.

Let *S~A~* = {*p~i~*: *p~i~* ∈ *P* & *A* = *A~i~*}. For each proposition *A* in Γ, *m*(*A*) = \|*S~A~*\|  / *y*. Since the sum of the *m*(*A*) is 1, *m* clearly is a mass function and hence generates *Bel*~S~ and *Pl* functions. The only task is to show that the functions thus generated are the bounds on *Bel*. For an arbitrary *A*, let *Bel*(*A*) = \[*x*~1~ / *y*, *x*~2~ / *y*\] and *Bel*~S~(*A*) = *x*~3~ / *y*. So the task is to prove that *x*~1~ = *x*~3~.

It might be thought that there could be some other cases, such as the possibility that *Pl*(*A*) was higher than *x*~2~. However, this will be dealt with, indirectly, once I have shown that *x*~1~ = *x*~3~ for arbitrary *A*. To prove this I will first prove the following lemma.

*Lemma*

Assume *Bel* satisfies the conditions in theorem 3.6.1. Let the bounds of *Bel*(*A*) be \[*x*~1~ / *y*, *x*~2~ / *y*\]. Then the bounds of *Bel*(¬*A*) are \[1 - *x*~2~ / *y*, 1 - *x*~1~ / *y*\].

Since *Bel*(*A*) ≥ *x*~1~ / *y*,[^48] there is a subset *S* of *P* such that \|*S*\| = *x*~1~ and *S*^\*^ → *A*^\*^ ∈ **K**^\*^. Hence ¬*A*^\*^ → ¬*S*^\*^∈ **K**^\*^. But \|¬*S*\| = *y* - *x*~1~. So *Bel*(¬*A*) ≤ 1 ‑ *x*~1~ / *y*. Assume this bound is not tight, i.e. there is some number *x*~3~ \> *x*~1~ such that *Bel*(¬*A*) ≤ 1 ‑ *x*~3~ / *y*. Then there is a subset *S* of *P* such that \|*S*\| = *y* - *x*~3~ and ¬*A*^\*^ → *S*^\*^ ∈ **K**^\*^. But that implies ¬*S*^\*^ → *A*^\*^∈ **K**^\*^, and since \|¬*S*\| = *x*~3~ \> *x*~1~ this implies *Bel*(*A*) ≥ *x*~3~ / *y* \> *x*~1~ / *y*. This contradicts the assumption that *Bel*(*A*) ≤ *x*~1~ / *y*. The other half of the proof, showing the lower bound on *Bel*(¬*A*) is 1 - *x*~2~ / *y* is entirely parallel and hence is omitted.

[^48]: when *x*, *y* and *z* are reals with *y* ≤ *z*, by *x* \< \[*y*, *z*\] I mean *x* \< *y*. Generally when *x* is a real and *S* a set of reals, by *x* \< *S* I mean *x* is less than every element of *S*. This usage is quite standard.

Now it is a general fact about the *Bel*~S~ and *Pl* functions generated by a common *m* function that *Bel*~S~(*A*) = 1 - *Pl*(¬*A*). So if there was a case such that *Pl*(*A*) was higher than *x*~2~, then it would be the case that *Bel*~S~(¬*A*) \< *Bel*(¬*A*). Hence we need only show *x*~1~ = *x*~3~

By definition, *x*~3~ is the number of *p~i~* such that *A~i~* ⊆ *A*. Let the set of all such *p~i~* be *S*. By the definition of the *A~i~*, this means *S*^\*^ → *A*^\*^ ∈ **K**^\*^. Hence *x*~1~ ≥ *x*~3~. Since *Bel*(*A*) = *x*~1~ there must be some set *T* ⊆ *P* such that *T*^\*^ → *A*^\*^ ∈ **K**^\*^, and \|*T*\| = *x*~1~. Let *p~j~* be an arbitrary element of *T*. By the closure of **K**^\*^ we know that *p~j~*^\*^ → *A*^\*^ ∈ **K**^\*^, hence *A~j~* ⊆ *A*. So every element of *T* is, by definition, also an element of *S*. Hence *x*~1~ ≤ *x*~3~. From this it follows *x*~1~ = *x*~3~ as required.

## Appendix 3C More on Dutch Book Arguments {#appendix-3c-more-on-dutch-book-arguments}

There are many arguments from the betting analysis of degrees of belief to the conclusion that the probability calculus provides coherence constraints on degrees of belief other than the ones I considered in @sec-chap-2. These include the Savage-style arguments from postulated coherence constraints on preferences, as in Maher (1993) and Kaplan (1996) and the 'depragmatised' Dutch Book arguments as in Howson and Urbach (1989), Christensen (1996) and Hellman (1997). All of these as presented make the simple mistake that is fatal to standard Dutch Book arguments of confusing use-value for exchange-value, but whereas this was an incurable problem for standard Dutch Book arguments it might seem curable here. Since these arguments don't postulate an actual retrade, it might be possible to stipulate, as I did above, that retrade is known to be barred and still get the conclusion that's desired. Even if this problem can be resolved (and I only accept for the sake of the argument that it can be) there's a further difficulty at hand; all of the arguments mentioned are question-begging. I have left discussion of these arguments until after @sec-chap-3 because it is easier to see these arguments are question-begging once we know what's at issue in the dispute. The primary conclusion of these arguments is that degrees of belief functions should obey *Addition*. This is what distinguishes their position from that adopted by, say, Shafer, or indeed most writers who deny that the probability calculus provides coherence constrains. However, *Addition* is a premise of every one of their arguments, which destroys their dialectical effectiveness.

The depragmatised Dutch Book arguments are effectively refuted by Maher (1997), so I won't say much about them. Maher is rather harsh on orthodox Dutch Book arguments, thinking they are refuted by the declining marginal utility of money. As a rather large class of authors have pointed out (particularly Savage who he is openly following) we can avoid this difficulty by denominating all bets in tickets to a lottery known to be fair. Now this will be ineffective against those who deny that the marginal utility of lottery tickets is constant, but that's a much smaller class than those who deny *Addition* applies to degrees of belief. The retrade error orthodox Dutch Book arguments make is sufficient to dispose of them, so this mistake of Maher's is of little consequence.

A typical 'depragmatised' Dutch Book argument is the one in Christensen (1996). He does not believe that an agent who's degree of belief in *p* is *x* should be prepared to buy a *p*‑bet for *x* dollars. However, he does say that the agent should "evaluate such \[trades\] as fair" (Christensen 1996: 456). So degrees of belief may 'sanction' (his term) certain odds even if the agent does not desire to accept these sanctioned bets. Now making the safe enough assumption that degrees of belief that sanction trades which lead to sure loss are defective he concludes that degrees of belief which do not satisfy the probability calculus are defective. As Maher (1997: 301‑3) points out, the argument so far doesn't get the conclusion Christensen wants. Indeed for some simple Shafer functions which are not probability functions no sure‑loss trades will be sanctioned. To get Christensen's conclusion, we need the extra premise that if two trades are sanctioned their sum is sanctioned. But given the definition of 'sanction' this just is the premise that degrees of belief must satisfy *Addition*. So the argument is question-begging against the writer who denies *Addition*. Maher shows that similar problems beset the arguments in Howson and Urbach (1989) and Hellman (1997).

The alternative Maher supports is based around 'representation theorems'. A similar approach is taken by Kaplan; I'll focus on Maher simply because the issues that arise are exactly the same. The basic idea is that it is a requirement on the coherence of an agent's preferences that there exist a probability function and a set of utility functions equivalent up to affine transformation such that the agent prefers gamble *f* to *g* iff the expected utility of *f* given the probability function and any of the utility functions is greater than that of *g*. The probability function will give us the agent's degrees of belief in all propositions. Strictly Maher does not quite believe this, the argument requires that preferences be complete, and Maher does not think this is plausible. If we drop that assumption we get the conclusion that the agent's degrees of belief should be represented by sets of probability functions, not a single probability function, as has been urged here. However, for convenience he assumes first that completeness holds, and I'll follow this lead. Nothing pertaining to the success or otherwise of the argument turns on this point.

There are a few immediate problems with this approach. Maher needs to assume that if an agent has a higher degree of belief in *p* than in *q* they will prefer a *p*‑bet to a *q*‑bet. The retrade problem could arise here, but maybe we can stipulate that retrade is barred and avoid that problem. For the sake of the argument I'll accept that. The bigger problem is that when it is unlikely that we will ever see *p* or ¬*p* confirmed we may well prefer a *q*‑bet to a *p*‑bet even if we have a higher degree of belief in *p*. I would prefer a bet on the Yankees winning the next World Series to a bet on Oswald being Kennedy's assassin, even though I have a higher degree of belief in Oswald's guilt than the Yankees's success, because betting on the Yankees gives me *some* chance of getting a payout. This point is one of the motivations for the intuitionist approach to probability developed in @sec-chap-8.

Maher is aware of this point, but his attempt to dispose of it is disastrous. His example is comparing a bet on the truth of the theory of evolution, construed as the claim that all life on earth is descended from a few species, with betting on its negation (1993: 89). Taking scientists as his expert function he asks some biologists which of these bets they would prefer, on the assumption that there are extraterrestrials who have been observing earth from its formation and will adjudicate on the bet. He is rather happy that they all plump for betting on Darwin. But this is a perfectly useless result. The objection was that we can have degrees of belief on unverifiable propositions, but our attitudes to bets on these propositions will be quite different to our attitude towards bets on verifiable propositions. He has attempted to counter this by simply making the problematic proposition verifiable. So the realist who thinks meaning and truth conditions go beyond verification conditions will be unsatisfied. And the anti-realist who accepts that verification conditions to determine truth conditions is no happier; she will regard the existence of the extraterrestrials as new evidence. As it happens the evidence will be both evidence for *p* and evidence for ¬*p*, but that isn't too strange to the intuitionist. So immediately we have a problem, although again I'd be prepared to accept for the sake of the argument it can be finessed.

The major problem for Maher is that his argument is just as question-begging as the Dutch Book arguments he criticises, though in a more subtle and interesting way. For Maher's argument to work we have to accept some constraints on preferences, such as Transitivity. The argument is only as strong as the argument for these constraints. He has nine axioms which must be justified in some way. The most interesting is Independence, which he construes as follows. **D** is in our language a set of gambles and **X** a set of propositions, *f* ≤ *g* means the agent either prefers *g* to *f* or is indifferent between them, *f* ≡ *g* means that *f* and *g* are exactly the same gamble, they have the same payouts in all possible worlds, and *f* ≡ *g* on *A* means that on all possible worlds in which *A*, *f* and *g* have the same payouts.

> For all *f*, *f* ´, *g*, *g*´ ∈ **D** and *A* ∈ **X**, if *f* ≡ *f* ´ on *A*, *g* ≡ *g*´ on *A*, *f* ≡ *g* on ¬*A*, *f* ´ ≡ *g*´ on ¬*A* and *f* ≤ *g* then *f* ´ ≤ *g*´ (Maher 1993: 190).

The idea is that if *f* and *g* have the same payouts given some proposition, say *A*, varying that payout cannot make us change our preference between *f* and *g*. The most famous examples where intuition says this may be violated are the Allais and Ellsburg 'paradoxes'. Since uncertainty plays a larger role in it, I'll briefly sketch the Ellsburg paradox. An urn contains 90 balls. Thirty of these are yellow, and remainder are either black or red in unknown proportion. The payouts for the four gambles in question are given in this table.

+----------------+:---------------+:---------------+:----------------+
|                | Red            | Black          | Yellow          |
+----------------+----------------+----------------+-----------------+
| *f*            | \$1            | 0              | 0               |
+----------------+----------------+----------------+-----------------+
| *g*            | 0              | 0              | \$1             |
+----------------+----------------+----------------+-----------------+
| *f* ´          | \$1            | \$1            | 0               |
+----------------+----------------+----------------+-----------------+
| *g*´           | 0              | \$1            | \$1             |
+----------------+----------------+----------------+-----------------+

Many subjects prefer *g* to *f*, since they know the chance of a yellow ball being drawn but not that of a red ball, but prefer *f* ´ to *g*´ since they know the chance of a red or black ball being drawn but not that of a black or yellow ball. With these preferences, and setting *A* to 'A black ball is not drawn' we can see this violates Maher's independence axiom. No objection yet, many people are just irrational. The real problem arises with Maher's argument that people who choose in this way are irrational. The following two choice trees set out two 'tree form' versions of the choices facing these subjects.

![](media/image8.emf)The left-hand tree represents the choice between *f* and *g*. The subject is told that if a black ball is drawn they will receive nothing, but if it is not drawn they will have a choice between betting on red and betting on yellow. So far we have a standard enough dynamic choice problem. Maher proposes to make it synchronic by requiring that subjects specify in advance what they would do if they reached the square, that is if a black ball is not drawn. This, he claims, makes the situation exactly as if the agent was choosing between *f* and *g*. Now the right-hand tree is the same as the left-hand tree in all respects but one. If a black ball is drawn the agent receives \$1, not nothing. But the only choice the agent has to make is exactly the same as in the left-hand tree, so they ought make the same choice. We can concede to Maher here that it would be irrational to specify, in advance, a preference for *g* over *f* in the left-hand tree and for *f* ´ over *g* in the right-hand tree. This is, however, insufficient for his conclusion.

The problem lies in his assumption that "it seems uncontroversial that the consequences a person values are not changed by representing the options in a tabular or tree form" (Maher 1993: 71). As Seidenfeld (1994) makes clear, this is exactly what is controversial in these circumstances. Indeed this premise, call it Reduction, is expressly denied by a number of heterodox decision theorists, and by writers who deny *Addition* on the occasions they talk about decision theory. There is a good reason for this. Let *B*, *R* and *Y* be the propositions that a black, red and yellow ball respectively is drawn. Many writers will hold that *Bel*(*B* ∨ *R*) may be greater than *Bel*(*B*) + *Bel*(*R*). When evaluating the worth of choosing *f* ´ in the original, tabular, it seems plausible that it is *Bel*(*B* ∨ *R*) that matters, not *Bel*(*B*) + *Bel*(*R*). However, in the tree form problem all that matters to *f* ´ is *Bel*(*B*), for the possibility that we won't need to choose, and *Bel*(*R*), for the possibility that we do. Strictly *Bel*~¬B~(*R*) · *Bel*(¬*B*) seems more relevant, but perhaps that can be taken to be *Bel*(*R*).

The point is that Maher has to either assume agents only consider *Bel*(*B*) and *Bel*(*R*) when assessing *f* ´, not *Bel*(*B* ∨ *R*), or that *Bel*(*B* ∨ *R*) is some function of *Bel*(*B*) and *Bel*(*R*) so that we can ignore that complication, in his 'uncontroversial' assumption. The first option is implausible; surely, when comparing *f* ´ and *g*´, we just compare *Bel*(*B* ∨ *R*) with *Bel*(*B* ∨ *Y*). More interestingly, I claim that the second is question-begging. Given that virtually everyone agrees that in some cases, for example lotteries, degrees of belief should be probability functions, in some cases the function which gives us *Bel*(*B* ∨ *R*) from *Bel*(*B*) and *Bel*(*R*) must be addition. Hence he must assume that *Bel*(*B* ∨ *R*) = *Bel*(*B*) + *Bel*(*R*) for the move from tabular to tree form to be plausible. But this is just what he was trying to prove, so the argument is question-begging.

There might be a different argument lurking around here that Maher could fall back upon. He might argue that Reduction is so obvious that if this amounts to *Addition* then *Addition* too is obvious. I suspect this argument has some force with those persuaded by the betting analysis in the first place. So we are back where we started, with the plausibility of Maher's argument standing and falling with the betting analysis. Again this is rather pointless from a dialectical perspective, since most heterodox theorists reject the betting analysis. Indeed, this thesis is nearly unique both in providing a defence of orthodoxy without understanding degrees of belief as dispositions to bet, as I've done in this chapter, and in providing a heterodox theory both motivated and justified by the betting analysis, as I do in @sec-chap-8.

I ought note in advance that when I get to decision theory in Part two I will accept Reduction. However, this assumption plays a quite different role in my argument to that which it plays in Maher's. I already have arguments for *Addition*, so the close connection between Reduction and *Addition* is a reason for me to accept Reduction, not a reason to suspend judgement, as it is here. My decision-theoretic arguments are designed to deduce the consequences of the epistemic principles already developed, not, as in Maher, to develop epistemic principles. So what counts as question-begging is quite different.

