# Philosophical Predecessors {#sec-chap-7}

As has already been mentioned, several authors have argued that imprecise or non-numerical degrees of belief ought to be permissible. The aim of this chapter is to look at four recent exponents of this view, and in particular at their motivations for allowing imprecision and the technical frameworks they develop to deal with them. My discussion of the writer to whom my position is closest, Keynes, will wait to a separate chapter investigating the connections between Keynes's theory of probability and his economic theories.

## Levi {#sec-0701}

In several papers and books, Isaac Levi has developed a theory for reasoning and decision making which allows that degrees of belief, what he calls *credal probabilities*, can be imprecise. He also allows that values can conflict, which might be captured by allowing utilities to be imprecise, but except to the extent this impacts on his epistemolgy that is not a subject I'll discuss here. In @sec-chap-9 I'll discuss Levi's decision theory, but here I'll focus more narrowly on his epistemological innovations.

### Levi's Argument for Imprecision

The argument @levi1974a[^89] gives for allowing imprecision is as follows:

[^89]: Unless otherwise stated, all references in this chapter are to the subject of the section in which the reference occurs.

> \(1\) Replacement in bodies of beliefs is only rational if it can be construed as a contraction followed by an expansion.    
> \(2\) It is sometimes permissible to replace credal probability functions.

Therefore,

> \(3\) Sometimes it is permissible to not rule out more than one credal probability function. That is, vagueness must be allowed, at least between the times that a rational agent has one precise credal probability function and the time she has a different one.

Although I agree with the conclusion, I disagree with each of the premises. Since my objections to (1) will show that the argument is unsound, I won't say much about (2). If the idea behind (2) is that evidence can show us our choice of probability function was in error, it is clearly a mistake. One of the conditions on a function being reasonable is that its conditionalisation under any evidence at all is reasonable. Levi occasionally argues something like this, but in later works, particularly @levi1980a, the motivation for (2) is a pragmatist theory of belief. Different probability functions, he says, might be warranted if our values change. Without a wholesale discussion of what's wrong with pragmatist epistemologies, I couldn't give a reasonable account of my objection to (2). So I'll stick to discussing (1).

The motivation for (1) comes from Levi's non-probabilist epistemology, so for a while I can simply discuss that. In Levi's theory, rational agents have belief sets which are sets of sentences closed under entailment. So rational agents believe all the logical truths, as well as all of maths and set theory. They also believe that each one of their beliefs is an item of knowledge. For this reason Levi refers to the sets as bodies of knowledge. This is a little misleading since many of an agent's beliefs are clearly not knowledge for the simple reason that they are false. Given this use of words, we can't take Levi's claim that knowledge is infallible at face value. What he means is that if an agent takes herself to know *p*, in effect believes *p*, then ¬*p* is not, for her, possibly true [-@levi1980a 13], where 'possibly' is interpreted epistemically. In more familiar terms, anything which contradicts our beliefs is not a doxastic possibility.

Now this might seem to conflict with some venerable pragmatist doctrines, particularly the doctrine that all of our beliefs are open to revision. In his words, there is a worry that Levi's infallibilism implies incorrigibilism. That, he assures us, in not the case, though as mentioned he does in effect take beliefs about the 'conceptual scheme', i.e. maths and set theory, to be incorrigible. He allows that corpora can be contracted, and hence allows that agents can give up beliefs. But there is a worry as to why an agent would give up a belief which was, to her mind, necessarily true. One reason is that the agent might have ended up with an inconsistent corpus of beliefs, say by coming to believe some observation sentences. This is clearly plausible, but it's not obvious that any other good reason exists. Certainly, Levi's attempt to motivate other grounds seems quite weak.

> Other good reasons exist for contracting a corpus. Suppose the initial corpus contains some theory *T*~1~. A second theory *T*~2~ contains *T*~1~. From *X*'s initial point of view, *T*~2~ is certainly false. Yet it may be superior in all other respects to *T*~1~ as a means for furnishing systematic explanations in some domain ... In such cases, *X* might be prepared to suffer a loss of information due to the removal of *T*~1~ from his corpus in order to be in a position to take the truth of *T*~2~ to be seriously possible. [-@levi1980 60]

However, persuasive this sounds in the abstract, in real cases it sounds implausible. Remember that *T*~2~ isn't just a theory the agent doesn't believe, it is a theory believed to be false. Now try to imagine a circumstance in which the elegance or systematicity of a theory you believe false, say Aristotle on any physical science, or perhaps Marx on any social science, could make you give up beliefs you currently have. Maybe I'm just dogmatic, but to my mind if a theory is known to be false it doesn't matter how pretty it is. Still, the point about inconsistency in the corpus necessitating contraction probably is enough here. Levi notes that inconsistency can easily arise if we make rule-governed expansions, so we need contraction for this reason.

He also allows that beliefs can be replaced. An agent can replace a belief in *p* with a belief in ¬*p*. "Replacements are shifts from corpora to other corpora inconsistent with the initial ones." [-@levi1980a 63] The history of science, as well as everyday life, tells us that replacements often happen. But their justification is, if anything, even weaker than the justification of contractions. From the agent's point of view, replacement amounts to adopting something false as a belief, not just giving up a true belief. Levi claims we can get a justification if we 'decompose' (his term) the replacement into a contraction followed by an expansion.

We might interpret this in two ways. First, Levi might be making the empirical claim that all reasonable replacements consist of contractions followed (at a later time) by an expansion. So the decomposition might be a closer analysis of what actually happens. Alternatively, he might be claiming that all reasonable replacements can be rationally reconstructed as justifiable contractions followed by justifiable expansions, even if these aren't temporally distinct in the mind of the agent. The text isn't particularly clear on which he intends, but I think each possibility is worth investigating. I claim the first turns out to be false, and the second turns out to be unjustifiable.

We can see that the empirical decomposition claim is false by looking at everyday examples. Jack reads in the morning newspapers that Smith will be the starting pitcher for the Yankees in tonight's game. He hears this repeated on the lunchtime news. Jack is a fan of Smith, so he decides to go to the game to watch Smith. As it will turn out that Smith isn't the starting pitcher, we can't say Jack knows Smith will be the starting pitcher, but it is something we should say is in his corpora of beliefs as Levi puts it. The evidence for this is that Jack believes he knows this, he will answer the question 'Who will start pitching for the Yankees?' with the answer 'Smith' and he acts as if he knew Smith was the starter[^90].

[^90]: It might be objected by some that the criteria for belief being used here is too weak. Even if this were true, it's not something Levi can claim, as he allows beliefs on much weaker evidence than this. In one example he allows an agent to believe *p* even though she knows the objective chance of ¬*p* is positive [-@levi1980a 275]. In another he lets the agent infer *p* although her credal probability for ¬*p* is 0.09 [-levi1980a 136]. The consistency of this approach seems dubious to me, but I would need much more space to address all the possible arguments here.

Unbeknowst to Jack, Smith gets injured in the pre-game warmup and Jones becomes the starting pitcher. Jack only realises this when he walks into the ground, hears on the ground public address system that the opening pitch is about to be thrown, looks to the mound and sees Jones pitching. He quickly comes to believe, on the basis of his overwhelming sensory evidence, that Jones is the night's starting pitcher. Does he first drop his belief that Smith will start and then adopt the belief that Jones starts? No; the two happen at exactly the same time. And this is quite reasonable. So Levi's construal of Jack's epistemic dynamics can be at best a rational reconstruction.

If, however, we take the rational reconstruction route, the purported justification of replacement becomes implausible. Levi notes that when an agent removes *p* from their corpus, they leave open the possibility that they will at a later time expand to include ¬*p*, which they believe to be false. Hence every contraction creates the possibility of expanding into (perceived) error. In replacement this possibility is realised. Against the claim that this shows all contraction to be unjustified, Levi claims that agents are allowed to be 'myopic' (again his term) when contracting. Agents are allowed to simply ignore the long-run effects of their actions, in this case the possible error to which they'll be led. The defence of this is simply that the alternatives are worse. "I cannot prove that I am right in singing the praises of mypoia. Nevertheless, the alternatives seem far less attractive." [-@levi1980 71]

The problem with the myopia account is that it is completely implausible on the rational reconstruction model. The idea behind it is that when the agent expands her beliefs to include ¬*p*, she no longer believes *p*, so she isn't coming to believe a falsehood. And she is allowed to myopically ignore the possibility that she will do this when she originally contracts. But on the rational reconstruction view, her beliefs do include *p* at the very time (or at least right until) she believes ¬*p*. Given this, she has to myopically ignore a possibility that's actually happening at the time she contracts. This is implausible, so I conclude something's wrong with Levi's account of replacement. And this implies that this argument for (1) fails, so he has no reason to infer (3).

### Levi's Calculus

Despite all that, (3) is correct, so it is worthwhile looking at how Levi incorporates this into his account of rational belief. For Levi, rational belief states can be represented by sets of probability functions. Anything the agent believes receives probability 1 according to each of these functions, though the converse need not hold. The functions are two-place, so conditionalisations on propositions with zero credal probability are defined, but not conditionalisations on propositions inconsistent with what the agent believes [-@levi1980a 221]. The functions need only satisfy finite additivity; Levi claims the arguments for countable additivity in the literature are generally question-begging [-@levi1980a 224-7].

His attitude to updating is a little confusing at first glance. "I myself am willing to endorse *confirmational conditionalisation* even though I reject *confirmational tenacity*. Consequently, I do not think that shifts in credal state due to expansion should be *temporal credal conditionalisations* in all cases." [-@levi1980a 85, my italics]. In simple terms, he adopts a position similar to @vanfraassen1989a, where agents are not required to have exhaustive rules for updating, but to the extent they have rules they shouldn't conflict with conditionalisation. However, the story is a bit more complicated than that.

Levi does think that (ideally rational) agents should have 'commitments' as to how to update under any information, and these should obey conditionalisation. This is his rule of confirmational conditionalisation. However, these commitments are, like everything else, subject to revision. The agent need not tenaciously hang on to them in all circumstances. To insist on this would be to adopt confirmational tenacity, and Levi rejects it. So an agent can have a 'commitment', but not be committed to implementing it[^91]. This strikes me as an unusual use of *commitment*, and my use of scare quotes is intended to be a reminder of this. Hence when some evidence comes in the agent can, at that stage, choose to drop some commitments, and hence the new epistemic state that's adopted need not be the conditionalisation of the old one on the evidence. This is what he means when he denies all shifts are temporal credal conditionalisations. If we restrict the term *commitment* to those 'commitments' the agent is committed to implementing, and insist the agent can't both have a 'commitment', and be committed to *not* implementing it, we get van Fraassen's position as I set out in the previous paragraph.

[^91]: It isn't entirely clear from Levi's text, but possibly we could read this as saying agents must be disposed to update by conditionalisation, but this disposition may be finkish.

In @sec-chap-5 I allowed that any precisification of a reasonable epistemic state was itself reasonable. Levi appears to adopt a different position. He explicitly endorses "the contention that one should not rule out \[probability\]-functions unless one has a warrant for doing so." [-@levi1980a 89]. However, he allows that warrant may be interpreted liberally to include the agent's values and goals. So the difference here is not as wide as it first appears. My argument for allowing arbitrary precisification turns on my differences with Levi's decision theory, which I'll discuss in the next chapter.

The most interesting aspect of Levi's calculus is that he insists reasonable epistemic states be represented by *convex* sets of probability functions. So the set must be closed under linear mixtures provided the weights are all non-negative. There is a prima facie argument against this. As noted in @sec-030606, @jeffrey1987a argues that the epistemic state containing just the information that *A* and *B* are probabilistically independent is reasonable despite not being convex.

Prima facie claims, however, do not settle the matter, so I ought to look at Levi's arguments for convexity. His arguments rely on his decision theory, which I think is mistaken, but I'll ignore that complication here. The problem is that the argument he gives doesn't go far enough. He argues that the epistemic state represented by just the two probability functions, *Pr*~1~(*p*) = 0.4 and *Pr*~2~(*p*) = 0.6 (we are only interested here in the truth or otherwise of *p*), is unreasonable. I agree; indeed, given the Equivalence Analysis, I am at a loss to know what we might mean by saying this is an agent's epistemic state.

The problem for Levi is that even if states like this one are unreasonable (or meaningless) this doesn't clinch the case for convexity. And he openly admits that he has 'no proof' [-@levi1980a 192] for requiring convexity beyond cases like this one. Consider the following property of sets of probability functions.

*Continuity*: A set S of probability functions is continuous iff for any proposition *A* and any numbers *x*, *y*, *z*, if there exist *Pr*~1~, *Pr*~2~ ∈ S such that *Pr*~1~(*A*) = *x* and *Pr*~2~(*A*) = *y* and *x* ≤ *z* ≤ *y*, then there exists a *Pr*~3~ ∈ S such that *Pr*~3~(*A*) = *z*.

A set of probability functions satisfies Continuity (for short, is continuous) iff the values of *Pr*(*A*) takes for *Pr* in that set is an interval for every proposition *A*. The set which Levi argues is unreasonable isn't continuous. However, the set Jeffrey argues is reasonable is continuous. This suggests that the problem with Levi's set isn't its lack of convexity, but its lack of continuity. I think the meaning considerations from @sec-chap-3 make continuity a plausible constraint, but its purpose here is simply to show that Levi hasn't refuted Jeffrey's *prima facie* objection to convexity.

## Van Fraassen {#sec-0702}

The most obvious debt this dissertation owes to Bas van Fraassen's work is his development and promotion of supervaluations. Not only did he do much to develop the technique and promote it within the philosophical community, (particularly in @vanfraassen1966a), he has also argued that it ought be applied in the case of imprecise credences [@vanfraassen1990a]. Some of the most interesting applications of this idea to date are also due to van Fraassen. In @vanfraassen1989a he uses this idea to develop an analysis of agnosticism to make his agnosticism about unobserved entities plausible, and in @vanfraassen1995a he uses the possibility of vagueness to fend off some objections to his principle of Reflection. Here I want to concentrate on an interesting technical result which I flagged in @sec-chap-3, and which might have some technological implications.

### Why Imprecision Is Allowed

In this dissertation I've taken the line that even perfectly rational agents may have imprecise degrees of belief. van Fraassen isn't as much concerned with what perfectly rational agents think as with what everyday agents think. If we had precise credences, he notes, we could only specify our epistemic states with an infinite number of judgements. However, since we are finite beings, "our expressible opinion must be expressible in a finite number of judgements" [-@vanfraassen1990a 353]. Hence our degrees of belief must be imprecise.

It's not altogether obvious that this is correct. Provided that we take a dispositional view of opinions, and we individuate dispositions finely, it seems we might have an infinite range of opinions. The only requirement is that a single physical state must be capable of instantiating multiple dispositions. This certainly looks possible. Given van Fraassen's interests, though, we don't need this argument. For precision requires that our opinion include non-trivial judgements about every subject imaginable, and this is clearly false. The possibility I referred to is simply irrelevant because it isn't instantiated. So someone who is interested in how humans with their limited resources and interests should reason, or more generally an investigator into epistemological norms who places some weight on the dictum 'ought implies can', should take vagueness in credences seriously.

### Van Fraassen's Calculus

What follows is a brief summary of the proof in [-@vanfraassen1990a] that what van Fraassen calls figurehood is preserved under conditionalisation, and some comments on the potential importance of this proof. A random variable *g* is a function from a possibility space to reals. Given a probability function *Pr*, we can work out the expected value of *g* according to *Pr*, as Σ*Pr*(*w*)*g*(*w*), where *g* ranges across the possibility space. This will be notated as *E~Pr~*(*g*). When the possibility space is infinite we need to be more careful to ensure this sum can be calculated. van Fraassen makes all the necessary assumptions, but for this sketch I'll sacrifice a little precision for brevity. Readers who know the difficulty will also know how to make the corrections[^92]. We can add random variables to constants or to each others, and we can multiply them by constants (or indeed each other, though this is less important). In each case the addition or multiplication is simply done possibility by possibility.

[^92]: There is one aspect in which van Fraassen is considerably more precise than most of the literature. The basic possibilities include information about what we are believing. This necessitates quite a few complexities which are often ignored; it is more usual (but less accurate) to write as if each element of the possibility space is independent of our opinions.

Using expected values we can give a standard form for judgements. Say *E*(*g* ≥ *a*) is satisfied by *Pr* iff *E~Pr~*(*g*) ≥ *a*. Then a judgement is a Boolean combination of statements of the form *E*(*g* ≥ *a*). From this we can get *E*(*g* ≤ *a*), which means the same as *E*(-*g* ≥ -*a*), and using these two we can express many other judgements. We can interpret ordinary judgements (like '*A* is more probable than not') using indicator functions. The indicator function of *A* is a random variable *I~A~* which takes value 1 if *w* is in *A* and 0 otherwise. We get the following results:

> *Pr*(*A*) = *r* iff *E~Pr~*(*I~A~*) = *r*;    
> *Pr*(*A*): *Pr*(*B*) = *r* iff *E~Pr~*(*I~A~* - *rI~B~*) = 0    
> *Pr*(*A* \| *B*) = *r* iff *E~Pr~*(*I~A~* ~∩ *B*~ - *rI~B~*) = 0

Using these results van Fraassen shows how to express many probabilistic judgements. For example we have the following:

> '*A* is at least as likely as ¬*A*': *E*(*I~A~* - *I* ~¬*A*~ ≥ 0)    
> '*A* is *r* times as likely as *B*': *E*(*I~A~* - *rI~B~* = 0)    
> 'Given *C*, *A* seems twice as likely as not': *E*((*I~A~* ~∩ *C*~ - (2/3)*I~C~*) = 0)

There are, however, some judgements we can't express. For any function *g*, the set of probability functions satisfying *E*(*g* ≥ *a*) is convex. To prove this simply note that the value of *g* according to an equal mix between two functions is simply the average of its value according to those functions. And since the intersection of convex sets is itself convex, we can't express epistemic states represented by non-convex sets of probability functions. So for the reasons given in the previous section, this language looks like it can't be expressive enough to say all we want about partial beliefs. Nevertheless, it has interesting properties.

If we take the standard form judgements as basic, we can define an agent's epistemic state as the set of probability functions which satisfy these judgements. A *figure* is defined as either a finite set of judgements of the form *E*(*f* ≥ 0), or equivalently the set of probability functions which satisfy all these judgements[^93]. The complexity of a figure is the smallest number of judgements of which it is the intersection. That our mental states are finite is reflected in the claim that (coherent) human epistemic states are figures.

[^93]: Since *E*(*f* ≥ *a*) is the same as *E*(*f* - *a* ≥ 0) there is no loss of generality from this definition.

We now get a pair of interesting questions. Is the property of figurehood preserved under conditionalisation, and what effect does conditionalisation have on complexity? van Fraassen provides a nearly constructive proof that the complexity increases by at most two. First some notation. For any set *P* of probability functions, let P ~*B*~ be defined as {*Pr*( • \| *B*): *Pr* ∈ P and *Pr*(*B*) \> 0}. The second conjunct in this definition is a little restrictive, but this assumption is used in the proof. Let \|*B*\| be {*Pr*: *Pr*(*B*) = 1}. We will now construct P ~*B*~. Note that it follows immediately from the definition that (P  ∩ Q )~*B*~ = P ~*B*~ ∩ Q ~*B*~, so we can look at single half-spaces. That is, we'll assume P = *E*(*g* ≥ 0).

Every *Pr* has an orthogonal decomposition in terms of *B* for 0 \< *Pr*(*B*) \< 1:

*Pr* = *cP*^+^ + (1 - *c*)*P*^--^, where 0 \< *c* ≤ 1, *P*^+^ = *Pr*( • \| *B*), *P*^--^ = *Pr*( • \| ¬*B*).

It follows that *E~Pr~*(*g*) = *c* *E~P+~*(*g*) + (1 - *c*) *E~P~*~--~(*g*). If *P*^+^ exists then *Pr*(*B*) \> 0. So *q* ∈ P ~*B*~ iff *q* = *P*^+^ for some *Pr* ∈ P . And *Pr* ∈ P iff *E~Pr~*(*g*) ≥ 0, i.e. *c* *E~P+~*(*g*) + (1 - *c*) *E~P~*~--~(*g*) ≥ 0. This can be trivially rewritten as *c* · *E~P+~*(*g*) ≥ -- (1 - *c*) *E~P~*~--~(*g*).

Now there are two cases to consider. Case 1, there is a probability function *q* ∈ \|¬*B*\| such that *E~q~*(*g*) \> 0. Then for any *p* ∈ \|*B*\| there will be a value of *c* in (0, 1\] such that *c* *E~p~*(*g*) ≥ -- (1 - *c*) *E~q~*(*g*), since when *c* goes to 0 the LHS will go to 0 and the RHS to -*E~q~*(*g*). Let *c* be such a value. Hence *Pr* = *cp* + (1 ‑ *c*)*q* is an element of P since *p* and *q* are its decompositions. Since *Pr*( • \| *B*) = *p*, and *p* was an arbitrary member of \|*B*\| it follows that P ~*B*~⊆ \|*B*\|. But since every function in P ~*B*~ must satisfy *Pr*(*B*) = 1, P ~*B*~⊇ \|*B*\|, so P ~*B*~= \|*B*\|.

Case 2 then is when there is no *q* ∈ \|¬*B*\| such that *E~q~*(*g*) \> 0. In that case for *p*∈ \|*B*\| (and any *q* ∈ \|¬*B*\|), there will be a value of *c* in (0, 1\] such that *c* *E~p~*(*g*) ≥ -- (1 - *c*) *E~q~*(*g*) iff *E~p~*(*g*) \> 0. If we do have *E~p~*(*g*) \> 0 then again *Pr* = *cp* + (1 ‑ *c*)*q* is an element of P since *p* and *q* are its decompositions. However, if *E~p~*(*g*) \< 0, then for any *c*, *q*, if *Pr* = *cp* + (1 ‑ *c*)*q*, *E~Pr~*(*g*) = *c* *E~p~*(*g*) + (1 - *c*) *E~q~*(*g*) \< 0, so *Pr* ∉ P . This implies P ~*B*~ = \|*B*\| ∩ *E*(*g* ≥ 0).

So for any half space *E*(*g* ≥ 0), conditionalisation on *B* takes it either into \|*B*\| or its intersection with \|*B*\|. The former possibility will occur if there is an element *q* of \|¬*B*\| such that *E~q~*(*g*) \> 0, the latter otherwise. In the general case, when P is a set of half spaces, the same story holds. P ~*B*~ is the intersection of those judgements *E*(*g* ≥ 0) such that there is no element *q* of \|¬*B*\| such that *E~q~*(*g*) \> 0, with \|*B*\|. And we can write \|*B*\| as the intersection of *E*(*I~B~* - 1 ≥ 0) with *E*(1 - *I~B~* ≥ 0). Hence the complexity of P ~*B*~ is at most two more than the complexity of P .

What is most interesting about this proof is the possibility it opens up for computational purposes. If there is a simple test to determine which case applies for a given judgement, the one non-constructive part of van Fraassen's proof, then it will be easy to conditionalise vague epistemic states that are figures. Now we noted that the language of figures isn't quite as expressive as we might like, but given how easily they can be updated, this loss might not be excessive.

## 7.3 Jeffrey {#jeffrey}

In Jeffrey (1983a) there are two reasons given for allowing degrees of belief to be vague, or as Jeffrey puts it, for allowing reasonable epistemic states to be representable by "probasitions", which are just sets of probability functions. The first is technical; even if we assume an agent has a complete preference ordering over all possible states of affairs, this may be consistent with an infinite number of pairs of subjective probabilities and utilities. (See Jeffrey (1983b) for the details.) The second is more pragmatic; real people simply don't have enough preferences or dispositions to make it the case that their epistemic state is representable by a single probability function.

Part of the attraction for Jeffrey of this position is that it allows for more tools in the Bayesian toolkit than just conditionalisation[^94]. For example, let *A~i~*, for 1 ≤ *i* ≤ *n* + 1 mean the *i*th trial was a success, and ***A**~i~* be the indicator function of *A~i~*. We needn't bother what the trial is, it might be tossing a coin and seeing if it lands heads, or dropping a plate and seeing if it shatters. Assume that we know there have been *s* successes on the first *n* trials, and for whatever reason think the probability of success on a given trial is constant.[^95] Hence our epistemic state will be representable by the set {*Pr*: *Pr*(*A~i~*) = *Pr*(*A*~j~), 1 ≤ *i* ≤ *j* ≤ *n* + 1}. Since we know that ***A***~1~ + ... + ***A**~n~* = *s*, our expectation for ***A***~1~ + ... + ***A**~n~* ought be *s*. But that expectation is just *Pr*(*A*~1~) + ... + *Pr*(*A~n~*); as each term of that expression is equal and they must sum to *s*, each term must equal *s* / *n*. And since *Pr*(*A~n~*~+1~) = *Pr*(*A*~1~) for all *Pr* in our representor, *Pr*(*A~n~*~+1~) = *s* / *n*. This, thinks Jeffrey, is an important part of the explanation of the importance of frequency, and it's all been done without conditionalisation, and indeed without saying anything about the probability of success on one trial conditional on the success or failure of another.

[^94]: One of the motivations behind Jeffrey's paper is to respond to Clark Glymour's definition of Bayesians as those who believe all updating should take place by conditionalisation.

[^95]: In the kind of examples that Jeffrey seems to be considering, "textbook trials", this hardly seems like a reasonable step. To see why, consider what happens to the reasoning in the text when *s* equals *n* and is rather low.

The results Jeffrey gives in this area, as can be seen from the above, are hardly of stunning technical importance, and his justification is not particularly interesting from the point of view of this dissertation, as I'm interested in arguments to the conclusion that even ideally rational agents should have imprecise degrees of belief. However, Jeffrey's endorsement of vagueness is historically important for two (related) reasons. First, Jeffrey is arguably a paradigm case Bayesian; hence, it is inappropriate for supporters of imprecision in probability to say that the enemy are the Bayesians, as is occasionally done. Secondly, it seems Jeffrey's paper has been more influential in converting philosophers to the view that imprecision is rationally permissible than any other. Hence it is worthwhile noting here.

## 7.4 Kyburg {#kyburg}

In many ways the theory of probability developed by Henry Kyburg over a number of years is the most similar to the one defended here. In particular he argues that probability is an objective relation between a claim and the evidence for it and that the value of this relation is non-contingent, legislative for rational belief, and not necessarily numerical. However, the differences between his position and mine are also substantial. His is a logical, not merely a necessitarian, theory of probability. On Kyburg's story, probability, like provability, is a meta-linguistic relation which holds between sentences (not propositions) by virtue of their syntactic properties. As a consequence of this, probability relations are language dependent. Further, the interval-values that probability relations take are in a sense primitive or unstructured; there's nothing to say about the probability of *p*, beyond the fact that it is the interval \[*x*, *y*\]. In the theory defended here, it matters (particularly for comparatives) which probability functions generated which values in \[*x*, *y*\]. Most importantly, all knowledge of probabilities is grounded in knowledge of frequencies. This is not the same as analysing probability as frequency, but it does have rather similar consequences. I have been deliberately agnostic on the question of how we come to know probabilities; my main aim in this section will be to defend my agnosticism against Kyburg's particular religion.

This is Kyburg's summary of his position, which has remained unchanged on these questions for over 30 years.

> Roughly speaking, we shall say that a statement has the probability *p* (in general *p* will be an interval *i* of reals), relative to a body of knowledge, when (a) it is known in that body of knowledge that the statement is equivalent to (has the same truth value as) a statement of the form *a* ∈ *b*; (b) it is known in that body of knowledge that *a* belongs to *c*; (c) that body of knowledge contains the statistical knowledge that the proportion of objects in *c* that belong to *b* falls in the interval *i*; and (d) there is nothing in our body of knowledge that conflicts with this assignment of probability. (Kyburg 1974: 156‑7).

The technical construal of (d) has changed in response to some problems the early account generated, and I'll outline its evolution below. As should be clear from this quote, Kyburg's construal of evidence is much different to mine. For one thing, he takes evidence to be sentential, not propositional. For another, it includes 'statistical' sentences, such as sentences about the proportions of heads among the tosses of fair coins. Kyburg takes a fundamentally different position to the one (implicitly) adopted here on the nature of induction. I construe inductive arguments as concluding that something is probable. Kyburg takes them as having a categorical conclusion, but with reasoning that is defeasible, or perhaps merely probable. (The distinction between these two accounts of induction is given most clearly in Hempel (1965).) Hence Kyburg thinks we can include the conclusions of our inductions in our evidence set. These inductive conclusions generate statistical statements about frequencies, and these generate probability statements.

This leads Kyburg to make a more dramatic departure from conventional wisdom about rational belief sets. Kyburg thinks that we should take as beliefs anything that we believe to at least a certain, high, degree, say *n*. Now imagine a (fair) lottery with more than 1/(1 - *n*) tickets. For every ticket, we will believe to greater than degree *n* that it will lose, hence we believe it will lose. However, we also believe that some ticket will win. Hence we are committed to believing a set of inconsistent propositions. This is the lottery paradox, itself due to Kyburg (1961). Many writers have taken this as an argument against construing (full) belief as high degree of belief. Kyburg takes it to be an argument against requiring rational belief sets to be logically closed. Many of the technical complications in his theory arise from this. For example condition (a) above can't be construed as saying *p* ≡ (*a* ∈ *b*) is in our evidence set. Rather we construe it as saying there is some chain of biconditionals in our evidence set that starts with *p* and ends with *a* ∈ *b*. And in fact we have to say the same thing about *a* ∈ *b*. It might be that our evidence set merely contains *a* ∈ *d* and *d* ⊂ *b*; this has to be sufficient. The complications induced this way don't seem formally necessary to Kyburg's theory (when explaining some technical aspect of his theory he will occasionally assume logical closure for ease of exposition) but they are I think philosophically necessary. If we agree that we can have full belief in statistical statements (which he requires) then we are forced into his resolution of the lottery paradox and hence must deny closure.

As I said above, the main technical move in his theory has been the changes in condition (d), the randomness condition. Since Kyburg takes an epistemic construal of probability, he seems justified in interpreting randomness as ignorance. The question is just what type of ignorance it is. The probability of *a* ∈ *b* is the interval *i* iff it is known that [*a* is a random member of the class *c*]{.underline}, and *i* is the smallest interval such that the proportion of *c*'s in *b* is known to be in *i*. If all these conditions hold, we'll call *c* the reference class for the probability judgement. As a first approximation, we can read the underlined part of the clause as saying that we don't know anything special about *a* other than its membership of *c*. The problem is now to say what is 'special' knowledge. Again as an approximation, we know something special about *a* if we know *a* is a *d*, and we know the proportion of *d*'s in *b* is *j*, and *i* is not a sub-interval of *j*. This, Kyburg hoped in (1974), provided the best trade-off between using the smallest reference class available, and using the most precise knowledge we have.

To illustrate, assume we know of *a*~1~ that it is a *d*, and that *d* ⊂ *c*. We know the proportion of *d*'s in *b* is \[0.3, 0.8\], and the proportion of *c*'s in *b* is \[0.7, 0.9\]. Since neither interval is included in the other, the information we get about the probability of *a*~1~ being a *b* from examining the *d*'s and examining the *c*'s clashes, and in this case we use the smaller reference class. Hence the probability, on our evidence, of *a*~1~ being a *b* is \[0.3, 0.8\]. Compare this case with *a*~2~, which is known to be an *e*, and it is known that *e* ⊂ *c*. The proportion of *e*'s in *b* is known to be in \[0.6, 1\]. In this case the information we get about *a*~2~'s probability of being a *b* from looking at the *e*'s and looking at the *c*'s does not clash; one is merely more precise than the other. In this case we don't worry about the size of the reference classes, we just use the more precise knowledge. So the probability of *a*~2~ being a *b*, according to our evidence, is \[0.7, 0.9\].

There are more complications (dealing for example with *a*~3~ which we know is a *d* and an *e*), but that's the basis of the randomness criteria in (1974). However, it turns out to lead to rather odd results. With the benefit of hindsight, these oddities seem inevitable given the two-part rule adopted. Some cases are always going to fall through the cracks. Let *c* be the set of all draws made from an urn of counters, and *d* the set of all draws made yesterday. (Hence *d* ⊂ *c*.) Let *a* be a counter that was drawn yesterday. Our knowledge of the frequency of colours drawn amongst *c* and *d* is given in this table.

+---------------+:---------------+:---------------+:---------------+
|               | White          | Red            | Blue           |
+---------------+----------------+----------------+----------------+
| *c*           | \[0.5, 0.5\]   | \[0.25, 0.25\] | \[0.25, 0.25\] |
+---------------+----------------+----------------+----------------+
| *d*           | \[0.4, 0.4\]   | \[0, 0.6\]     | \[0, 0.6\]     |
+---------------+----------------+----------------+----------------+

When we are determining the probability that *a* is white, the information about *c* and *d* clashes, so we use the smaller reference class. Hence the probability that *a* is white is precisely 0.4. For red (and *mutatis mutandis* for blue) the information about *c* and *d* does not clash, so we use the reference class that gives us the more precise information. Hence the probability that *a* is red (blue) is 0.25. Kyburg is committed to saying that whenever probabilities are precise they should obey the probability calculus (these are just set-theoretic tautologies when interpreted as statements about frequencies), but here he is committed to a breach of that calculus. The solution (in Kyburg (1983)) is to change somewhat the definition of randomness. We now use a smaller reference class if it is known to generate clashing information about the predicate in which we are interested, or any other (known to be) disjoint predicate. Getting this formally right requires quite a lot of technical work -- in particular now we have to watch out that this rule isn't only obeyed for the partition of the possibility space we are using, but also for any refinement of it -- but this captures the philosophically important aspects of the amendment.

There has been quite a lot written about the technical aspects of Kyburg's work, and I don't particularly want to add to it here. Rather, I want to question the philosophical presumptions of his theory. In particular, I don't think the reliance on frequencies and the language-dependence of Kyburg's theory can be defended when we look at how probability is (and ought be) applied to the actual world.

When we are trying to discover the probability of interesting claims about the real world (as opposed to the colour of a marble in an urn) Kyburg notes that we may need to be creative in discovering the relevant frequencies (1961: 266ff). For example, what makes it true that 'It is probable (given our evidence) that Caeser crossed the Rubicon' is not the known frequency of Caeser-type leaders crossing Rubicon-type rivers. Rather it is, for us, the frequency that historical assertions assented to by the vast majority of historians turn out true. That frequency we (apparently) know to be high, and hence we can give a high probability to 'Caeser crossed the Rubicon'. For the historians themselves, who could hardly reason this way, they know there are records from the time which say the Caeser crossed the Rubicon, and they know the frequency of such records being correct, and this frequency underlies their probability judgement.

However, even with such ingenuity, I doubt it will be possible to discover enough frequencies to ground all the seemingly justified probability judgements that are made. For non-experts, the move of referring to the frequency of experts being correct can always be used[^96], so I'll look at how experts should reason. Consider one rather recent case. In early 1998 President Clinton was entangled in a scandal over an affair he'd apparently had with a young White House aide, Monica Lewinsky. This was of itself a minor scandal, but the real danger was that according to some allegations, he had lied about the affair under oath, and encouraged Ms. Lewinsky involved to do the same. If this were true, he'd have both committed and suborned perjury, and many felt this was grounds for impeachment. Given all this, Washington correspondents were frequently asked for their assessment of the chances of Clinton being impeached, or perhaps for his relative chances for survival on different strategies.[^97]

[^96]: Or at least attempted; in many fields I remain highly skeptical that experts are in general correct, particularly in prediction.

[^97]: At the time of writing, July 1998, the White House seems to have successfully swayed public opinion by focusing on procedural matters in the investigation. Hence there is little political will for impeachement outside the usual Republican suspects. But what I said in the last footnote holds for my predictions too, so don't hold me too tightly to that guess.

I claim that Kyburg cannot explain the use of probability in these reports from Washington, and that these reports are, if a little charitably interpreted, correct uses of 'probability'. This is a problem for a theory such as Kyburg's which does aim to capture uses of probability in natural language. More generally, I think the probabilities being discussed were legislative for rational belief, but were not based on frequencies, which if true would remove a foundation stone of Kyburg's theory.

As the crisis unfolded, one of the internet-based news magazines[^98] listed its daily assessment of the chances of Clinton being impeached. I interpret these as claims as to the probability of Clinton being impeached given all the evidence publically available[^99]. Their assessment of this probability, over the eleven main days of the crisis, was as follows.[^100]

[^98]: *The Slate*, at http://www.slate.com.

[^99]: They were explicitly estimates of the 'chance' of impeachment, which amounts to the same thing. They weren't particularly timely measures of chances because of the lag between something happening which affects the chance and it becoming publically known, but this doesn't affect the comments in the text.

[^100]: As background, between the 21st and 25th more rumours about evidence of the affair appeared, and erstwhile Clinton supporters publicly discussed impeachment and, occasionally, urged a resignation. From the 26th onwards Clinton was supported as more and more of the rumours turned out to be false, and polls showed that not only were most people against impeachment at present, a majority opposed impeachment even if the charges were proven, and at the end of the month Clinton's approval rating hit an all time high.

|                            |     |     |     |     |     |     |     |     |     |     |     |
|:---------------------------|:----|:----|:----|:----|:----|:----|:----|:----|:----|:----|:----|
| Day (of January)           | 21  | 22  | 23  | 24  | 25  | 26  | 27  | 28  | 29  | 30  | 31  |
| Probability of Impeachment | 25% | 35% | 40% | 45% | 45% | 40% | 38% | 35% | 34% | 30% | 21% |

Now the real numbers are obviously fuzzier than this, and the particular choice was hardly made seriously. In particular we should agree with Kyburg that the probability of impeachment on a given day should be an interval rather than a number, and perhaps quite a broad interval at that. What I do think should be taken seriously are the comparatives. Except perhaps for the 1% change from the 28th to the 29th, all the other day-to-day comparative statements look correct to me, given the evidence that was available at the time. I don't extend this to comparisons between the various probabilities quoted on the way up and the way down.[^101] So it was more probable on the 23rd he'd be impeached than the 22nd, equally (or perhaps incomparably) probable on the 25th that he'd be impeached as on the 24th, and just about every day after that it was getting less probable.

[^101]: It's easy with hindsight, but I don't know how Clinton's position could have been considered worse on the 27th than the 22nd. Once things started looking good for the President, the media, in order to protect a great story and justify their interest in it, overestimated the problems Clinton was in. Hence I'll only use day-to-day comparisons, because they seem the only justified ones.

I think a successful theory of probability should be able to explain these comparative statements, and be able to say something about the absolute probability statements. I doubt Kyburg's theory could explain the comparatives, but I must be a little cautious because Kyburg has not, as far as I can tell, written anything on comparative probability sentences. The strongest theory of comparatives he could plausibly endorse is the following. Say the probability of *p* is \[*x~p~*, *y~p~*\] and the probability of *q* is \[*x~q~*, *y~q~*\]. Then the probability of *p* is greater than the probability of *q* iff *x~p~* \> *x~q~* and *y~p~* \> *y~q~*. (Perhaps he could say that one of these is allowed to be an equality, but that doesn't sound overly plausible.) Given the way his theory is developed, it would seem more appropriate to say that the probability of *p* is greater than the probability of *q* iff *x~p~* \> *y~q~*, or *x~p~* = *y~q~* \> *x~q~*.

Since what I say is least effective the stronger the theory of comparatives Kyburg adopts, I'll assume he adopts the stronger of the two theories mentioned above. Now Kyburg has to find no fewer that nine different reference classes for judging the probability of 'Clinton will be impeached' over the eleven days of the crisis. If the reference class used on day *t* is the same as that used on day *t* + 1, then he can't say the probability of impeachment on day *t* is different (either greater than or less than) the probability of impeachment on day *t* + 1. I doubt that he could reasonably find one reference class; even if I'm wrong about this, I'd have to be wrong in a big way for him to find enough reference classes to make every comparative turn out true.

What reference classes could be used? The class of Presidents who are impeached and removed from office doesn't help; it's still empty. The class of events predicted by Washington correspondents might help for explaining the probability for some people, but not surely for the Washington correspondents we are looking at. If we knew that impeachment was an element of the class 'political actions desired by a majority' we might get somewhere, but that was never known, and as it turned out never true. (It was the realisation of this that sent the probability of impeachment diving at the end of January.) Perhaps instead of just looking at the frequency that Presidents are impeached, we could look at impeachment rates for elected officials (e.g. state governors). But it is hardly plausible to say the potential impeachment of a President is a random member of the class of potential impeachments of elected officials.

There is a more general difficulty for Kyburg here. Even if we had a long string of impeached Presidents, so we could judge the frequency of impeachment once a scandal got to such and such a stage, it's hard to see how that could be relevant here. One of the distinguishing features of this crisis was the massive involvement of the media, particularly internet-based media. And by internet-based media I include traditional newspapers who were concentrating on getting stories onto their web sites as quickly as possible, not just the best story for the morning newspaper. Some of the most absurd rumours of the scandal appeared through these sources. But these rumours not only affected everyone's perception of the probability of impeachment, there was every chance that, by affecting the way the crisis was viewed, they could have had a real effect. This affair was nothing like anything that had preceeded it, but that didn't prevent it being possible to make reasonable probability judgements. Indeed, anyone who had all the evidence but didn't form judgements about the probability of impeachment of similar form to that given above would be unreasonable. By similar form I just mean believing impeachment was more probable at the peak of the crisis, around the 25th, than it was a few days before and after, and believing that at these times impeachment was more probable than it was one month previously. But if Kyburg's correct, we oughtn't to be able to make probability judgements about such *sui generis* events.

The theoretical difficulty for Kyburg this raises is that it seems we may have a use, in social sciences at least, for 'pathological' predicates. For his theory to generate decent results, Kyburg needs to rule out pathological predicates like grue. This is related to both the language-dependence of his theory, and the need to generate known statistical statements as building blocks. However, in social sciences, and generally in making probability judgements about everyday life predicates that are broadly pathological may be needed. I don't mean we necessarily need predicates like 'grue' which change, in some sense, their meaning at the turn of a millennia.[^102] What we do need are predicates which, in some sense, shift a little when real factors in the outside world shift. From the political scientist's perspective, President Clinton's having an affair in this age of mass media is a quite kind of different occurrence from, say, President Kennedy's having an affair. Predicates which behave like 'green before an internet exists, blue otherwise' have some importance. Kyburg claims (1990: 126-130) that his theory can deliver the tools to develop the language of science, and this language will help us avoid the grue pitfalls. For the social scientist this seems both impossible and undesirable, and that conclusion seems to carry across to everyday life.

[^102]: Though we should never underestimate the importance of millennial symbolism in politics. Australia is currently debating whether it should change from a monarchy to republic on the first day of the new millennia. Perhaps we should say it's debating whether or not to stay as a monalic. Millennial symbolism is an important driving force here, and such symbolism is distorting election campaigns the world over.

